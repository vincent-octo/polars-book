{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PolarsBlazingly Fast DataFrame Library","text":"<p>Polars is a highly performant DataFrame library for manipulating structured data. The core is written in Rust, but the library is available in Python, Rust &amp; NodeJS. Its key features are:</p> <ul> <li>Fast: Polars is written from the ground up, designed close to the machine and without external dependencies. </li> <li>I/O: First class support for all common data storage layers: local, cloud storage &amp; databases. </li> <li>Easy to use: Write your queries the way they were intended. Polars, internally, will determine the most efficient way to execute using its query optimizer.</li> <li>Out of Core: Polars supports out of core data transformation with its streaming API. Allowing you to process your results without requiring all your data to be in memory at the same time</li> <li>Parallel: Polars fully utilises the power of your machine by dividing the workload among the available CPU cores without any additional configuration. </li> <li>Vectorized Query Engine: Polars uses Apache Arrow, a columnar data format, to process your queries in a vectorized manner. It uses SIMD to optimize CPU usage.</li> </ul>"},{"location":"#about-this-guide","title":"About this guide","text":"<p>The <code>Polars</code> user guide is intended to live alongside the API documentation. Its purpose is to explain (new) users how to use <code>Polars</code> and to provide meaningful examples. The guide is split into two parts:</p> <ul> <li>Getting Started: A 10 minute helicopter view of the library and its primary function.</li> <li>User Guide: A detailed explanation of how the library is setup and how to use it most effectively. </li> </ul> <p>If you are looking for details on a specific level / object, it is probably best to go the API documentation: Python | NodeJS | Rust.</p>"},{"location":"#performance","title":"Performance","text":"<p><code>Polars</code> is very fast, and in fact is one of the best performing solutions available. See the results in h2oai's db-benchmark, revived by the DuckDB project.</p> <p><code>Polars</code> TPCH Benchmark results are now available on the official website.</p>"},{"location":"#example","title":"Example","text":"Python Rust NodeJS <p> <code>scan_csv</code> \u00b7 <code>filter</code> \u00b7 <code>groupby</code> \u00b7 <code>collect</code> <pre><code>import polars as pl\n\nq = (\n    pl.scan_csv(\"docs/src/data/iris.csv\")\n    .filter(pl.col(\"sepal_length\") &gt; 5)\n    .groupby(\"species\")\n    .agg(pl.all().sum())\n)\n\ndf = q.collect()\n</code></pre></p> <p> <code>LazyCsvReader</code> \u00b7 <code>filter</code> \u00b7 <code>groupby</code> \u00b7 <code>collect</code> \u00b7  Available on feature csv \u00b7  Available on feature streaming <pre><code>use polars::prelude::*;\n\nlet q = LazyCsvReader::new(\"docs/src/data/iris.csv\")\n.has_header(true)\n.finish()?\n.filter(col(\"sepal_length\").gt(lit(5)))\n.groupby(vec![col(\"species\")])\n.agg([col(\"*\").sum()]);\n\nlet df = q.collect();\n</code></pre></p> <p> <code>scanCSV</code> \u00b7 <code>filter</code> \u00b7 <code>groupBy</code> \u00b7 <code>collect</code> <pre><code>const pl = require(\"nodejs-polars\");\n\nq = pl\n.scanCSV(\"docs/src/data/iris.csv\")\n.filter(pl.col(\"sepal_length\").gt(5))\n.groupBy(\"species\")\n.agg(pl.all().sum());\n\ndf = q.collect();\n</code></pre></p>"},{"location":"#sponsors","title":"Sponsors","text":""},{"location":"#community","title":"Community","text":"<p><code>Polars</code> has a very active community with frequent releases (approximately weekly). Below are some of the top contributors to the project: </p> <p> </p>"},{"location":"#contribute","title":"Contribute","text":"<p>Thanks for taking the time to contribute! We appreciate all contributions, from reporting bugs to implementing new features. If you're unclear on how to proceed read our contribution guide or contact us on discord.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the MIT license.</p>"},{"location":"people/","title":"People","text":""},{"location":"getting-started/expressions/","title":"Expressions","text":"<p><code>Expressions</code> are the core strength of <code>Polars</code>. The <code>expressions</code> offer a versatile structure that both solves easy queries and is easily extended to complex ones. Below we will cover the basic components that serve as building block (or in <code>Polars</code> terminology contexts) for all your queries:</p> <ul> <li><code>select</code></li> <li><code>filter</code></li> <li><code>with_columns</code></li> <li><code>groupby</code></li> </ul> <p>To learn more about expressions and the context in which they operate, see the User Guide sections: Contexts and Expressions.</p>"},{"location":"getting-started/expressions/#select-statement","title":"Select statement","text":"<p>To select a column we need to do two things. Define the <code>DataFrame</code> we want the data from. And second, select the data that we need. In the example below you see that we select <code>col('*')</code>. The asterisk stands for all columns.</p>  Python Rust NodeJS <p> <code>select</code> <pre><code>df.select(pl.col(\"*\"))\n</code></pre></p> <p> <code>select</code> <pre><code>let out = df.clone().lazy().select([col(\"*\")]).collect()?;\nprintln!(\"{}\",out);\n</code></pre></p> <p> <code>select</code> <pre><code>df.select(pl.col(\"*\"));\n</code></pre></p> <pre><code>shape: (8, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b        \u2506 c                   \u2506 d     \u2502\n\u2502 --- \u2506 ---      \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64 \u2506 f64      \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 0.449507 \u2506 2022-12-01 00:00:00 \u2506 1.0   \u2502\n\u2502 1   \u2506 0.064056 \u2506 2022-12-02 00:00:00 \u2506 2.0   \u2502\n\u2502 2   \u2506 0.012581 \u2506 2022-12-03 00:00:00 \u2506 NaN   \u2502\n\u2502 3   \u2506 0.802239 \u2506 2022-12-04 00:00:00 \u2506 NaN   \u2502\n\u2502 4   \u2506 0.946181 \u2506 2022-12-05 00:00:00 \u2506 0.0   \u2502\n\u2502 5   \u2506 0.157343 \u2506 2022-12-06 00:00:00 \u2506 -5.0  \u2502\n\u2502 6   \u2506 0.117897 \u2506 2022-12-07 00:00:00 \u2506 -42.0 \u2502\n\u2502 7   \u2506 0.765403 \u2506 2022-12-08 00:00:00 \u2506 null  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You can also specify the specific columns that you want to return. There are two ways to do this. The first option is to create a <code>list</code> of column names, as seen below.</p>  Python Rust NodeJS <p> <code>select</code> <pre><code>df.select(pl.col([\"a\", \"b\"]))\n</code></pre></p> <p> <code>select</code> <pre><code>let out = df.clone().lazy().select([col(\"a\"), col(\"b\")]).collect()?;\nprintln!(\"{}\",out);\n</code></pre></p> <p> <code>select</code> <pre><code>df.select(pl.col([\"a\", \"b\"]));\n</code></pre></p> <pre><code>shape: (8, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b        \u2502\n\u2502 --- \u2506 ---      \u2502\n\u2502 i64 \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 0.449507 \u2502\n\u2502 1   \u2506 0.064056 \u2502\n\u2502 2   \u2506 0.012581 \u2502\n\u2502 3   \u2506 0.802239 \u2502\n\u2502 4   \u2506 0.946181 \u2502\n\u2502 5   \u2506 0.157343 \u2502\n\u2502 6   \u2506 0.117897 \u2502\n\u2502 7   \u2506 0.765403 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The second option is to specify each column within a <code>list</code> in the <code>select</code> statement. This option is shown below.</p>  Python Rust NodeJS <p> <code>select</code> <pre><code>df.select([pl.col(\"a\"), pl.col(\"b\")]).limit(3)\n</code></pre></p> <p> <code>select</code> <pre><code>let out = df.clone().lazy().select([col(\"a\"), col(\"b\")]).limit(3).collect()?;\nprintln!(\"{}\",out);\n</code></pre></p> <p> <code>select</code> <pre><code>df.select([pl.col(\"a\"), pl.col(\"b\")]).limit(3);\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b        \u2502\n\u2502 --- \u2506 ---      \u2502\n\u2502 i64 \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 0.449507 \u2502\n\u2502 1   \u2506 0.064056 \u2502\n\u2502 2   \u2506 0.012581 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>If you want to exclude an entire column from your view, you can simply use <code>exclude</code> in your <code>select</code> statement.</p>  Python Rust NodeJS <p> <code>select</code> <pre><code>df.select([pl.exclude(\"a\")])\n</code></pre></p> <p> <code>select</code> <pre><code>let out = df.clone().lazy().select([col(\"*\").exclude([\"a\"])]).collect()?;\nprintln!(\"{}\",out);\n</code></pre></p> <p> <code>select</code> <pre><code>df.select([pl.exclude(\"a\")]);\n</code></pre></p> <pre><code>shape: (8, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 b        \u2506 c                   \u2506 d     \u2502\n\u2502 ---      \u2506 ---                 \u2506 ---   \u2502\n\u2502 f64      \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0.449507 \u2506 2022-12-01 00:00:00 \u2506 1.0   \u2502\n\u2502 0.064056 \u2506 2022-12-02 00:00:00 \u2506 2.0   \u2502\n\u2502 0.012581 \u2506 2022-12-03 00:00:00 \u2506 NaN   \u2502\n\u2502 0.802239 \u2506 2022-12-04 00:00:00 \u2506 NaN   \u2502\n\u2502 0.946181 \u2506 2022-12-05 00:00:00 \u2506 0.0   \u2502\n\u2502 0.157343 \u2506 2022-12-06 00:00:00 \u2506 -5.0  \u2502\n\u2502 0.117897 \u2506 2022-12-07 00:00:00 \u2506 -42.0 \u2502\n\u2502 0.765403 \u2506 2022-12-08 00:00:00 \u2506 null  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/expressions/#filter","title":"Filter","text":"<p>The <code>filter</code> option allows us to create a subset of the <code>DataFrame</code>. We use the same <code>DataFrame</code> as earlier and we filter between two specified dates.</p>  Python Rust NodeJS <p> <code>filter</code> <pre><code>df.filter(\n    pl.col(\"c\").is_between(datetime(2022, 12, 2), datetime(2022, 12, 8)),\n)\n</code></pre></p> <p> <code>filter</code> <pre><code>// TODO\n</code></pre></p> <p> <code>filter</code> <pre><code>df.filter(pl.col(\"c\").gt(new Date(2022, 12, 2)).lt(new Date(2022, 12, 8)));\n</code></pre></p> <pre><code>shape: (7, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b        \u2506 c                   \u2506 d     \u2502\n\u2502 --- \u2506 ---      \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64 \u2506 f64      \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 0.064056 \u2506 2022-12-02 00:00:00 \u2506 2.0   \u2502\n\u2502 2   \u2506 0.012581 \u2506 2022-12-03 00:00:00 \u2506 NaN   \u2502\n\u2502 3   \u2506 0.802239 \u2506 2022-12-04 00:00:00 \u2506 NaN   \u2502\n\u2502 4   \u2506 0.946181 \u2506 2022-12-05 00:00:00 \u2506 0.0   \u2502\n\u2502 5   \u2506 0.157343 \u2506 2022-12-06 00:00:00 \u2506 -5.0  \u2502\n\u2502 6   \u2506 0.117897 \u2506 2022-12-07 00:00:00 \u2506 -42.0 \u2502\n\u2502 7   \u2506 0.765403 \u2506 2022-12-08 00:00:00 \u2506 null  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>With <code>filter</code> you can also create more complex filters that include multiple columns.</p>  Python Rust NodeJS <p> <code>filter</code> <pre><code>df.filter((pl.col(\"a\") &lt;= 3) &amp; (pl.col(\"d\").is_not_nan()))\n</code></pre></p> <p> <code>filter</code> <pre><code>let out = df.clone().lazy().filter(col(\"a\").lt_eq(3).and(col(\"d\").is_not_null())).collect()?;\nprintln!(\"{}\",out);\n</code></pre></p> <p> <code>filter</code> <pre><code>df.filter(pl.col(\"a\").ltEq(3).and(pl.col(\"d\").isNotNull()));\n</code></pre></p> <pre><code>shape: (2, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b        \u2506 c                   \u2506 d   \u2502\n\u2502 --- \u2506 ---      \u2506 ---                 \u2506 --- \u2502\n\u2502 i64 \u2506 f64      \u2506 datetime[\u03bcs]        \u2506 f64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 0.449507 \u2506 2022-12-01 00:00:00 \u2506 1.0 \u2502\n\u2502 1   \u2506 0.064056 \u2506 2022-12-02 00:00:00 \u2506 2.0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/expressions/#with_columns","title":"With_columns","text":"<p><code>with_columns</code> allows you to create new columns for you analyses. We create two new columns <code>e</code> and <code>b+42</code>. First we sum all values from column <code>b</code> and store the results in column <code>e</code>. After that we add <code>42</code> to the values of <code>b</code>. Creating a new column <code>b+42</code> to store these results.</p>  Python Rust NodeJS <p> <code>with_columns</code> <pre><code>df.with_columns([pl.col(\"b\").sum().alias(\"e\"), (pl.col(\"b\") + 42).alias(\"b+42\")])\n</code></pre></p> <p> <code>with_columns</code> <pre><code>let out = df.clone().lazy().with_columns(\n[\ncol(\"b\").sum().alias(\"e\"),\n(col(\"b\") + lit(42)).alias(\"b+42\")\n]\n).collect()?;\nprintln!(\"{}\",out);\n</code></pre></p> <p> <code>withColumns</code> <pre><code>df.withColumns([\npl.col(\"b\").sum().alias(\"e\"),\npl.col(\"b\").plus(42).alias(\"b+42\"),\n]);\n</code></pre></p> <pre><code>shape: (8, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b        \u2506 c                   \u2506 d     \u2506 e        \u2506 b+42      \u2502\n\u2502 --- \u2506 ---      \u2506 ---                 \u2506 ---   \u2506 ---      \u2506 ---       \u2502\n\u2502 i64 \u2506 f64      \u2506 datetime[\u03bcs]        \u2506 f64   \u2506 f64      \u2506 f64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 0.449507 \u2506 2022-12-01 00:00:00 \u2506 1.0   \u2506 3.315207 \u2506 42.449507 \u2502\n\u2502 1   \u2506 0.064056 \u2506 2022-12-02 00:00:00 \u2506 2.0   \u2506 3.315207 \u2506 42.064056 \u2502\n\u2502 2   \u2506 0.012581 \u2506 2022-12-03 00:00:00 \u2506 NaN   \u2506 3.315207 \u2506 42.012581 \u2502\n\u2502 3   \u2506 0.802239 \u2506 2022-12-04 00:00:00 \u2506 NaN   \u2506 3.315207 \u2506 42.802239 \u2502\n\u2502 4   \u2506 0.946181 \u2506 2022-12-05 00:00:00 \u2506 0.0   \u2506 3.315207 \u2506 42.946181 \u2502\n\u2502 5   \u2506 0.157343 \u2506 2022-12-06 00:00:00 \u2506 -5.0  \u2506 3.315207 \u2506 42.157343 \u2502\n\u2502 6   \u2506 0.117897 \u2506 2022-12-07 00:00:00 \u2506 -42.0 \u2506 3.315207 \u2506 42.117897 \u2502\n\u2502 7   \u2506 0.765403 \u2506 2022-12-08 00:00:00 \u2506 null  \u2506 3.315207 \u2506 42.765403 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/expressions/#groupby","title":"Groupby","text":"<p>We will create a new <code>DataFrame</code> for the Groupby functionality. This new <code>DataFrame</code> will include several 'groups' that we want to groupby.</p>  Python Rust NodeJS <p> <code>DataFrame</code> <pre><code>df2 = pl.DataFrame(\n    {\n        \"x\": np.arange(0, 8),\n        \"y\": [\"A\", \"A\", \"A\", \"B\", \"B\", \"C\", \"X\", \"X\"],\n    }\n)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df2: DataFrame = df!(\"x\" =&gt; 0..8,\n\"y\"=&gt; &amp;[\"A\", \"A\", \"A\", \"B\", \"B\", \"C\", \"X\", \"X\"],\n).expect(\"should not fail\");\nprintln!(\"{}\",df2);\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>df2 = pl.DataFrame({\nx: [...Array(8).keys()],\ny: [\"A\", \"A\", \"A\", \"B\", \"B\", \"C\", \"X\", \"X\"],\n});\n</code></pre></p> <pre><code>shape: (8, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 x   \u2506 y   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 str \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 A   \u2502\n\u2502 1   \u2506 A   \u2502\n\u2502 2   \u2506 A   \u2502\n\u2502 3   \u2506 B   \u2502\n\u2502 4   \u2506 B   \u2502\n\u2502 5   \u2506 C   \u2502\n\u2502 6   \u2506 X   \u2502\n\u2502 7   \u2506 X   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>  Python Rust NodeJS <p> <code>groupby</code> <pre><code>df2.groupby(\"y\", maintain_order=True).count()\n</code></pre></p> <p> <code>groupby</code> <pre><code>let out = df2.clone().lazy().groupby([\"y\"]).agg([count()]).collect()?;\nprintln!(\"{}\",out);\n</code></pre></p> <p> <code>groupBy</code> <pre><code>df2.groupBy(\"y\").count();\nconsole.log(df2);\n</code></pre></p> <pre><code>shape: (4, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 y   \u2506 count \u2502\n\u2502 --- \u2506 ---   \u2502\n\u2502 str \u2506 u32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 A   \u2506 3     \u2502\n\u2502 B   \u2506 2     \u2502\n\u2502 C   \u2506 1     \u2502\n\u2502 X   \u2506 2     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>  Python Rust NodeJS <p> <code>groupby</code> <pre><code>df2.groupby(\"y\", maintain_order=True).agg(\n    [\n        pl.col(\"*\").count().alias(\"count\"),\n        pl.col(\"*\").sum().alias(\"sum\"),\n    ]\n)\n</code></pre></p> <p> <code>groupby</code> <pre><code>let out = df2.clone().lazy().groupby([\"y\"]).agg([\ncol(\"*\").count().alias(\"count\"),\ncol(\"*\").sum().alias(\"sum\"),\n]).collect()?;\nprintln!(\"{}\",out);\n</code></pre></p> <p> <code>groupBy</code> <pre><code>df2\n.groupBy(\"y\")\n.agg(pl.col(\"*\").sum().alias(\"count\"), pl.col(\"*\").sum().alias(\"sum\"));\n</code></pre></p> <pre><code>shape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 y   \u2506 count \u2506 sum \u2502\n\u2502 --- \u2506 ---   \u2506 --- \u2502\n\u2502 str \u2506 u32   \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 A   \u2506 3     \u2506 3   \u2502\n\u2502 B   \u2506 2     \u2506 7   \u2502\n\u2502 C   \u2506 1     \u2506 5   \u2502\n\u2502 X   \u2506 2     \u2506 13  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/expressions/#combining-operations","title":"Combining operations","text":"<p>Below are some examples on how to combine operations to create the <code>DataFrame</code> you require.</p>  Python Rust NodeJS <p> <code>select</code> \u00b7 <code>with_columns</code> <pre><code>df_x = df.with_columns((pl.col(\"a\") * pl.col(\"b\")).alias(\"a * b\")).select(\n    [pl.all().exclude([\"c\", \"d\"])]\n)\n\nprint(df_x)\n</code></pre></p> <p> <code>select</code> \u00b7 <code>with_columns</code> <pre><code>let out = df.clone().lazy().with_columns([\n(col(\"a\") * col(\"b\")).alias(\"a * b\")\n]).select([\ncol(\"*\").exclude([\"c\",\"d\"])\n]).collect()?;\nprintln!(\"{}\",out);\n</code></pre></p> <p> <code>select</code> \u00b7 <code>withColumns</code> <pre><code>df_x = df\n.withColumns(pl.col(\"a\").mul(pl.col(\"b\")).alias(\"a * b\"))\n.select([pl.all().exclude([\"c\", \"d\"])]);\n\nconsole.log(df_x);\n</code></pre></p> <pre><code>shape: (8, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b        \u2506 a * b    \u2502\n\u2502 --- \u2506 ---      \u2506 ---      \u2502\n\u2502 i64 \u2506 f64      \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 0.449507 \u2506 0.0      \u2502\n\u2502 1   \u2506 0.064056 \u2506 0.064056 \u2502\n\u2502 2   \u2506 0.012581 \u2506 0.025161 \u2502\n\u2502 3   \u2506 0.802239 \u2506 2.406717 \u2502\n\u2502 4   \u2506 0.946181 \u2506 3.784723 \u2502\n\u2502 5   \u2506 0.157343 \u2506 0.786714 \u2502\n\u2502 6   \u2506 0.117897 \u2506 0.707384 \u2502\n\u2502 7   \u2506 0.765403 \u2506 5.357824 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>  Python Rust NodeJS <p> <code>select</code> \u00b7 <code>with_columns</code> <pre><code>df_y = df.with_columns([(pl.col(\"a\") * pl.col(\"b\")).alias(\"a * b\")]).select(\n    [pl.all().exclude(\"d\")]\n)\n\nprint(df_y)\n</code></pre></p> <p> <code>select</code> \u00b7 <code>with_columns</code> <pre><code>let out = df.clone().lazy().with_columns([\n(col(\"a\") * col(\"b\")).alias(\"a * b\")\n]).select([\ncol(\"*\").exclude([\"d\"])\n]).collect()?;\nprintln!(\"{}\",out);\n</code></pre></p> <p> <code>select</code> \u00b7 <code>withColumns</code> <pre><code>df_y = df\n.withColumns([pl.col(\"a\").mul(pl.col(\"b\")).alias(\"a * b\")])\n.select([pl.all().exclude(\"d\")]);\nconsole.log(df_y);\n</code></pre></p> <pre><code>shape: (8, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b        \u2506 c                   \u2506 a * b    \u2502\n\u2502 --- \u2506 ---      \u2506 ---                 \u2506 ---      \u2502\n\u2502 i64 \u2506 f64      \u2506 datetime[\u03bcs]        \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 0.449507 \u2506 2022-12-01 00:00:00 \u2506 0.0      \u2502\n\u2502 1   \u2506 0.064056 \u2506 2022-12-02 00:00:00 \u2506 0.064056 \u2502\n\u2502 2   \u2506 0.012581 \u2506 2022-12-03 00:00:00 \u2506 0.025161 \u2502\n\u2502 3   \u2506 0.802239 \u2506 2022-12-04 00:00:00 \u2506 2.406717 \u2502\n\u2502 4   \u2506 0.946181 \u2506 2022-12-05 00:00:00 \u2506 3.784723 \u2502\n\u2502 5   \u2506 0.157343 \u2506 2022-12-06 00:00:00 \u2506 0.786714 \u2502\n\u2502 6   \u2506 0.117897 \u2506 2022-12-07 00:00:00 \u2506 0.707384 \u2502\n\u2502 7   \u2506 0.765403 \u2506 2022-12-08 00:00:00 \u2506 5.357824 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Polars is a library and installation is as simple as invoking the package manager of the corresponding programming language.</p>  Python Rust NodeJS <pre><code>pip install polars\n</code></pre> <pre><code>cargo add polars\n</code></pre> <pre><code>yarn add nodejs-polars\n</code></pre>"},{"location":"getting-started/installation/#importing","title":"Importing","text":"<p>To use the library import it into your project</p>  Python Rust NodeJS <pre><code>import polars as pl\n</code></pre> <pre><code>use polars::prelude::*;\n</code></pre> <pre><code>// esm\nimport pl from 'nodejs-polars';\n\n// require\nconst pl = require('nodejs-polars'); </code></pre>"},{"location":"getting-started/intro/","title":"Introduction","text":"<p>This getting started guide is written for new users of Polars. The goal is to provide a quick overview of the most common functionality. For a more detailed explanation, please go to the User Guide </p> <p> Rust Users Only</p> <p>Due to historical reasons the eager API in Rust is outdated. In the future we would like to redesign it as a small wrapper around the lazy API (as is the design in Python / NodeJS). In the  examples we will use the lazy API instead with <code>.lazy()</code> and <code>.collect()</code>. For now you can ignore these two functions. If you want to know more about the lazy and eager API go here. </p> <p>To enable the Lazy API ensure you have the feature flag <code>lazy</code> configured when installing Polars <pre><code># Cargo.toml\n[dependencies]\npolars = { version = \"x\", features = [\"lazy\", ...]}\n</code></pre></p> <p>Because of the ownership ruling in Rust we can not reuse the same <code>DataFrame</code> multiple times in the examples. For simplicity reasons we call <code>clone()</code> to overcome this issue. Note that this does not duplicate the data but just increments a pointer (<code>Arc</code>).</p>"},{"location":"getting-started/joins/","title":"Combining DataFrames","text":"<p>There are two ways <code>DataFrame</code>s can be combined depending on the use case: join and concat.</p>"},{"location":"getting-started/joins/#join","title":"Join","text":"<p>Polars supports all types of join (e.g. left, right, inner, outer). Let's have a closer look on how to <code>join</code> two <code>DataFrames</code> into a single <code>DataFrame</code>. Our two <code>DataFrames</code> both have an 'id'-like column: <code>a</code> and <code>x</code>. We can use those columns to <code>join</code> the <code>DataFrames</code> in this example.</p>  Python Rust NodeJS <p> <code>join</code> <pre><code>df = pl.DataFrame(\n    {\n        \"a\": np.arange(0, 8),\n        \"b\": np.random.rand(8),\n        \"d\": [1, 2.0, np.NaN, np.NaN, 0, -5, -42, None],\n    }\n)\n\ndf2 = pl.DataFrame(\n    {\n        \"x\": np.arange(0, 8),\n        \"y\": [\"A\", \"A\", \"A\", \"B\", \"B\", \"C\", \"X\", \"X\"],\n    }\n)\njoined = df.join(df2, left_on=\"a\", right_on=\"x\")\nprint(joined)\n</code></pre></p> <p> <code>join</code> <pre><code>use rand::Rng;\nlet mut rng = rand::thread_rng();\n\nlet df: DataFrame = df!(\"a\" =&gt; 0..8,\n\"b\"=&gt; (0..8).map(|_| rng.gen::&lt;f64&gt;()).collect::&lt;Vec&lt;f64&gt;&gt;(),\n\"d\"=&gt; [Some(1.0), Some(2.0), None, None, Some(0.0), Some(-5.0), Some(-42.), None]\n).expect(\"should not fail\");\nlet df2: DataFrame = df!(\"x\" =&gt; 0..8,\n\"y\"=&gt; &amp;[\"A\", \"A\", \"A\", \"B\", \"B\", \"C\", \"X\", \"X\"],\n).expect(\"should not fail\");\nlet joined = df.join(&amp;df2,[\"a\"],[\"x\"],JoinType::Left,None)?;\nprintln!(\"{}\",joined);\n</code></pre></p> <p> <code>join</code> <pre><code>df = pl.DataFrame({\na: [...Array(8).keys()],\nb: Array.from({ length: 8 }, () =&gt; Math.random()),\nd: [1, 2.0, null, null, 0, -5, -42, null],\n});\n\ndf2 = pl.DataFrame({\nx: [...Array(8).keys()],\ny: [\"A\", \"A\", \"A\", \"B\", \"B\", \"C\", \"X\", \"X\"],\n});\njoined = df.join(df2, { leftOn: \"a\", rightOn: \"x\" });\nconsole.log(joined);\n</code></pre></p> <pre><code>shape: (8, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b        \u2506 d     \u2506 y   \u2502\n\u2502 --- \u2506 ---      \u2506 ---   \u2506 --- \u2502\n\u2502 i64 \u2506 f64      \u2506 f64   \u2506 str \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 0.940413 \u2506 1.0   \u2506 A   \u2502\n\u2502 1   \u2506 0.791234 \u2506 2.0   \u2506 A   \u2502\n\u2502 2   \u2506 0.634507 \u2506 NaN   \u2506 A   \u2502\n\u2502 3   \u2506 0.879961 \u2506 NaN   \u2506 B   \u2502\n\u2502 4   \u2506 0.973144 \u2506 0.0   \u2506 B   \u2502\n\u2502 5   \u2506 0.205376 \u2506 -5.0  \u2506 C   \u2502\n\u2502 6   \u2506 0.131481 \u2506 -42.0 \u2506 X   \u2502\n\u2502 7   \u2506 0.385729 \u2506 null  \u2506 X   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To see more examples with other types of joins, go the User Guide.</p>"},{"location":"getting-started/joins/#concat","title":"Concat","text":"<p>We can also <code>concatenate</code> two <code>DataFrames</code>. Vertical concatenation will make the <code>DataFrame</code> longer. Horizontal concatenation will make the <code>DataFrame</code> wider. Below you can see the result of an horizontal concatenation of our two <code>DataFrames</code>.</p>  Python Rust NodeJS <p> <code>hstack</code> <pre><code>stacked = df.hstack(df2)\nprint(stacked)\n</code></pre></p> <p> <code>hstack</code> <pre><code>let stacked = df.hstack(df2.get_columns())?;\nprintln!(\"{}\",stacked);\n</code></pre></p> <p> <code>hstack</code> <pre><code>stacked = df.hstack(df2);\nconsole.log(stacked);\n</code></pre></p> <pre><code>shape: (8, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b        \u2506 d     \u2506 x   \u2506 y   \u2502\n\u2502 --- \u2506 ---      \u2506 ---   \u2506 --- \u2506 --- \u2502\n\u2502 i64 \u2506 f64      \u2506 f64   \u2506 i64 \u2506 str \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0   \u2506 0.940413 \u2506 1.0   \u2506 0   \u2506 A   \u2502\n\u2502 1   \u2506 0.791234 \u2506 2.0   \u2506 1   \u2506 A   \u2502\n\u2502 2   \u2506 0.634507 \u2506 NaN   \u2506 2   \u2506 A   \u2502\n\u2502 3   \u2506 0.879961 \u2506 NaN   \u2506 3   \u2506 B   \u2502\n\u2502 4   \u2506 0.973144 \u2506 0.0   \u2506 4   \u2506 B   \u2502\n\u2502 5   \u2506 0.205376 \u2506 -5.0  \u2506 5   \u2506 C   \u2502\n\u2502 6   \u2506 0.131481 \u2506 -42.0 \u2506 6   \u2506 X   \u2502\n\u2502 7   \u2506 0.385729 \u2506 null  \u2506 7   \u2506 X   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/reading-writing/","title":"Reading &amp; Writing","text":"<p>Polars supports reading &amp; writing to all common files (e.g. csv, json, parquet), cloud storage (S3, Azure Blob, BigQuery) and databases (e.g. postgres, mysql). In the following examples we will show how to operate on most common file formats. For the following dataframe</p>  Python Rust NodeJS <p> <code>DataFrame</code> <pre><code>import polars as pl\nfrom datetime import datetime\n\ndf = pl.DataFrame(\n    {\n        \"integer\": [1, 2, 3],\n        \"date\": [\n            datetime(2022, 1, 1),\n            datetime(2022, 1, 2),\n            datetime(2022, 1, 3),\n        ],\n        \"float\": [4.0, 5.0, 6.0],\n    }\n)\n\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>use std::fs::File;\nuse chrono::prelude::*;\n\nlet mut df: DataFrame = df!(\"integer\" =&gt; &amp;[1, 2, 3],\n\"date\" =&gt; &amp;[\nNaiveDate::from_ymd_opt(2022, 1, 1).unwrap().and_hms_opt(0, 0, 0).unwrap(),\nNaiveDate::from_ymd_opt(2022, 1, 2).unwrap().and_hms_opt(0, 0, 0).unwrap(),\nNaiveDate::from_ymd_opt(2022, 1, 3).unwrap().and_hms_opt(0, 0, 0).unwrap(),\n],\n\"float\" =&gt; &amp;[4.0, 5.0, 6.0]\n).expect(\"should not fail\");\nprintln!(\"{}\",df);\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df = pl.DataFrame({\ninteger: [1, 2, 3],\ndate: [\nnew Date(2022, 1, 1, 0, 0),\nnew Date(2022, 1, 2, 0, 0),\nnew Date(2022, 1, 3, 0, 0),\n],\nfloat: [4.0, 5.0, 6.0],\n});\nconsole.log(df);\n</code></pre></p> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integer \u2506 date                \u2506 float \u2502\n\u2502 ---     \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64     \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1       \u2506 2022-01-01 00:00:00 \u2506 4.0   \u2502\n\u2502 2       \u2506 2022-01-02 00:00:00 \u2506 5.0   \u2502\n\u2502 3       \u2506 2022-01-03 00:00:00 \u2506 6.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/reading-writing/#csv","title":"CSV","text":"<p>Polars has its own fast implementation for csv reading with many flexible configuration options. </p>  Python Rust NodeJS <p> <code>read_csv</code> \u00b7 <code>write_csv</code> <pre><code>df.write_csv(\"output.csv\")\ndf_csv = pl.read_csv(\"output.csv\")\nprint(df_csv)\n</code></pre></p> <p> <code>CsvReader</code> \u00b7 <code>CsvWriter</code> \u00b7  Available on feature csv <pre><code>let mut file = File::create(\"output.csv\").expect(\"could not create file\");\nCsvWriter::new(&amp;mut file).has_header(true).with_delimiter(b',').finish(&amp;mut df);\nlet df_csv = CsvReader::from_path(\"output.csv\")?.infer_schema(None).has_header(true).finish()?;\nprintln!(\"{}\",df_csv);\n</code></pre></p> <p> <code>readCSV</code> \u00b7 <code>writeCSV</code> <pre><code>df.writeCSV(\"output.csv\");\nvar df_csv = pl.readCSV(\"output.csv\");\nconsole.log(df_csv);\n</code></pre></p> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integer \u2506 date                       \u2506 float \u2502\n\u2502 ---     \u2506 ---                        \u2506 ---   \u2502\n\u2502 i64     \u2506 str                        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1       \u2506 2022-01-01T00:00:00.000000 \u2506 4.0   \u2502\n\u2502 2       \u2506 2022-01-02T00:00:00.000000 \u2506 5.0   \u2502\n\u2502 3       \u2506 2022-01-03T00:00:00.000000 \u2506 6.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>As we can see above, Polars made the datetimes a <code>string</code>. We can tell Polars to parse dates, when reading the csv, to ensure the date becomes a datetime. The example can be found below:</p>  Python Rust NodeJS <p> <code>read_csv</code> <pre><code>df_csv = pl.read_csv(\"output.csv\", try_parse_dates=True)\nprint(df_csv)\n</code></pre></p> <p> <code>CsvReader</code> \u00b7  Available on feature csv <pre><code>let mut file = File::create(\"output.csv\").expect(\"could not create file\");\nCsvWriter::new(&amp;mut file).has_header(true).with_delimiter(b',').finish(&amp;mut df);\nlet df_csv = CsvReader::from_path(\"output.csv\")?.infer_schema(None).has_header(true).with_parse_dates(true).finish()?;\nprintln!(\"{}\",df_csv);    </code></pre></p> <p> <code>readCSV</code> <pre><code>var df_csv = pl.readCSV(\"output.csv\", { parseDates: true });\nconsole.log(df_csv);\n</code></pre></p> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integer \u2506 date                \u2506 float \u2502\n\u2502 ---     \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64     \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1       \u2506 2022-01-01 00:00:00 \u2506 4.0   \u2502\n\u2502 2       \u2506 2022-01-02 00:00:00 \u2506 5.0   \u2502\n\u2502 3       \u2506 2022-01-03 00:00:00 \u2506 6.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/reading-writing/#json","title":"JSON","text":"Python Rust NodeJS <p> <code>read_json</code> \u00b7 <code>write_json</code> <pre><code>df.write_json(\"output.json\")\ndf_json = pl.read_json(\"output.json\")\nprint(df_json)\n</code></pre></p> <p> <code>JsonReader</code> \u00b7 <code>JsonWriter</code> \u00b7  Available on feature json <pre><code>let mut file = File::create(\"output.json\").expect(\"could not create file\");\nJsonWriter::new(&amp;mut file).finish(&amp;mut df);\nlet mut f = File::open(\"output.json\")?;\nlet df_json = JsonReader::new(f).with_json_format(JsonFormat::JsonLines).finish()?;\nprintln!(\"{}\",df_json);    </code></pre></p> <p> <code>readJSON</code> \u00b7 <code>writeJSON</code> <pre><code>df.writeJSON(\"output.json\");\nlet df_json = pl.readJSON(\"output.json\");\nconsole.log(df_json);\n</code></pre></p> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integer \u2506 date                \u2506 float \u2502\n\u2502 ---     \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64     \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1       \u2506 2022-01-01 00:00:00 \u2506 4.0   \u2502\n\u2502 2       \u2506 2022-01-02 00:00:00 \u2506 5.0   \u2502\n\u2502 3       \u2506 2022-01-03 00:00:00 \u2506 6.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/reading-writing/#parquet","title":"Parquet","text":"Python Rust NodeJS <p> <code>read_parquet</code> \u00b7 <code>write_parquet</code> <pre><code>df.write_parquet(\"output.parquet\")\ndf_parquet = pl.read_parquet(\"output.parquet\")\nprint(df_parquet)\n</code></pre></p> <p> <code>ParquetReader</code> \u00b7 <code>ParquetWriter</code> \u00b7  Available on feature parquet <pre><code>let mut file = File::create(\"output.parquet\").expect(\"could not create file\");\nParquetWriter::new(&amp;mut file).finish(&amp;mut df);\nlet mut f = File::open(\"output.parquet\")?;\nlet df_parquet = ParquetReader::new(f).finish()?;\nprintln!(\"{}\",df_parquet);\n</code></pre></p> <p> <code>readParquet</code> \u00b7 <code>writeParquet</code> <pre><code>df.writeParquet(\"output.parquet\");\nlet df_parquet = pl.readParquet(\"output.parquet\");\nconsole.log(df_parquet);\n</code></pre></p> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integer \u2506 date                \u2506 float \u2502\n\u2502 ---     \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64     \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1       \u2506 2022-01-01 00:00:00 \u2506 4.0   \u2502\n\u2502 2       \u2506 2022-01-02 00:00:00 \u2506 5.0   \u2502\n\u2502 3       \u2506 2022-01-03 00:00:00 \u2506 6.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To see more examples and other data formats go to the User Guide, section IO.</p>"},{"location":"getting-started/series-dataframes/","title":"Series &amp; DataFrames","text":"<p>The core base data structures provided by Polars are  <code>Series</code> and <code>DataFrames</code>. </p>"},{"location":"getting-started/series-dataframes/#series","title":"Series","text":"<p>Series are a 1-dimensional data structure. Within a series all elements have the same data type (e.g. int, string).  The snippet below shows how to create a simple named <code>Series</code> object. In a later section of this getting started guide we will learn how to read data from external sources (e.g. files, database), for now lets keep it simple.   </p>  Python Rust NodeJS <p> <code>Series</code> <pre><code>import polars as pl\n\ns = pl.Series(\"a\", [1, 2, 3, 4, 5])\nprint(s)\n</code></pre></p> <p> <code>Series</code> <pre><code>use chrono::prelude::*;\n\nlet s = Series::new(\"a\", [1, 2, 3, 4, 5]);\nprintln!(\"{}\",s);\n</code></pre></p> <p> <code>Series</code> <pre><code>const pl = require(\"nodejs-polars\");\n\nvar s = pl.Series(\"a\", [1, 2, 3, 4, 5]);\nconsole.log(s);\n</code></pre></p> <pre><code>shape: (5,)\nSeries: 'a' [i64]\n[\n    1\n    2\n    3\n    4\n    5\n]\n</code></pre>"},{"location":"getting-started/series-dataframes/#methods","title":"Methods","text":"<p>Although it is more common to work directly on a <code>DataFrame</code> object, <code>Series</code> implement a number of base methods which make it easy to perform transformations. Below are some examples of common operations you might want to perform. Note that these are for illustration purposes and only show a small subset of what is available.</p>"},{"location":"getting-started/series-dataframes/#aggregations","title":"Aggregations","text":"<p><code>Series</code> out of the box supports all basic aggregations (e.g. min, max, mean, mode, ...).</p>  Python Rust NodeJS <p> <code>min</code> \u00b7 <code>max</code> <pre><code>s = pl.Series(\"a\", [1, 2, 3, 4, 5])\nprint(s.min())\nprint(s.max())\n</code></pre></p> <p> <code>min</code> \u00b7 <code>max</code> <pre><code>let s = Series::new(\"a\", [1, 2, 3, 4, 5]);\n// The use of generics is necessary for the type system\nprintln!(\"{}\",s.min::&lt;u64&gt;().unwrap());\nprintln!(\"{}\",s.max::&lt;u64&gt;().unwrap());\n</code></pre></p> <p> <code>min</code> \u00b7 <code>max</code> <pre><code>var s = pl.Series(\"a\", [1, 2, 3, 4, 5]);\nconsole.log(s.min());\nconsole.log(s.max());\n</code></pre></p> <pre><code>1\n5\n</code></pre>"},{"location":"getting-started/series-dataframes/#string","title":"String","text":"<p>There are a number of methods related to string operations in the <code>StringNamespace</code>. These only work on <code>Series</code> with the Datatype <code>Utf8</code>.</p>  Python Rust NodeJS <p> <code>replace</code> <pre><code>s = pl.Series(\"a\", [\"polar\", \"bear\", \"arctic\", \"polar fox\", \"polar bear\"])\ns2 = s.str.replace(\"polar\", \"pola\")\nprint(s2)\n</code></pre></p> <pre><code>// This operation is not directly available on the Series object yet, only on the DataFrame\n</code></pre> <p> <code>replace</code> <pre><code>var s = pl.Series(\"a\", [\"polar\", \"bear\", \"arctic\", \"polar fox\", \"polar bear\"]);\nvar s2 = s.str.replace(\"polar\", \"pola\");\nconsole.log(s2);\n</code></pre></p> <pre><code>shape: (5,)\nSeries: 'a' [str]\n[\n    \"pola\"\n    \"bear\"\n    \"arctic\"\n    \"pola fox\"\n    \"pola bear\"\n]\n</code></pre>"},{"location":"getting-started/series-dataframes/#datetime","title":"Datetime","text":"<p>Similar to strings, there is a seperate namespace for datetime related operations in the <code>DateLikeNameSpace</code>. These only work on <code>Series</code>with DataTypes related to dates.</p>  Python Rust NodeJS <p> <code>day</code> <pre><code>from datetime import datetime\n\nstart = datetime(2001, 1, 1)\nstop = datetime(2001, 1, 9)\ns = pl.date_range(start, stop, interval=\"2d\", eager=True)\ns.dt.day()\nprint(s)\n</code></pre></p> <pre><code>// This operation is not directly available on the Series object yet, only on the DataFrame\n</code></pre> <p> <code>day</code> <pre><code>var s = pl.Series(\"a\", [\nnew Date(2001, 1, 1),\nnew Date(2001, 1, 3),\nnew Date(2001, 1, 5),\nnew Date(2001, 1, 7),\nnew Date(2001, 1, 9),\n]);\nvar s2 = s.date.day();\nconsole.log(s2);\n</code></pre></p> <pre><code>shape: (5,)\nSeries: '' [datetime[\u03bcs]]\n[\n    2001-01-01 00:00:00\n    2001-01-03 00:00:00\n    2001-01-05 00:00:00\n    2001-01-07 00:00:00\n    2001-01-09 00:00:00\n]\n</code></pre>"},{"location":"getting-started/series-dataframes/#dataframe","title":"DataFrame","text":"<p>A <code>DataFrame</code> is a 2-dimensional data structure that is backed by a <code>Series</code>, and it could be seen as an abstraction of on collection (e.g. list) of <code>Series</code>. Operations that can be executed on <code>DataFrame</code> are very similar to what is done in a <code>SQL</code> like query. You can <code>GROUP BY</code>, <code>JOIN</code>, <code>PIVOT</code>, but also define custom functions. In the next pages we will cover how to perform these transformations.</p>  Python Rust NodeJS <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"integer\": [1, 2, 3, 4, 5],\n        \"date\": [\n            datetime(2022, 1, 1),\n            datetime(2022, 1, 2),\n            datetime(2022, 1, 3),\n            datetime(2022, 1, 4),\n            datetime(2022, 1, 5),\n        ],\n        \"float\": [4.0, 5.0, 6.0, 7.0, 8.0],\n    }\n)\n\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df: DataFrame = df!(\"integer\" =&gt; &amp;[1, 2, 3, 4, 5],\n\"date\" =&gt; &amp;[\nNaiveDate::from_ymd_opt(2022, 1, 1).unwrap().and_hms_opt(0, 0, 0).unwrap(),\nNaiveDate::from_ymd_opt(2022, 1, 2).unwrap().and_hms_opt(0, 0, 0).unwrap(),\nNaiveDate::from_ymd_opt(2022, 1, 3).unwrap().and_hms_opt(0, 0, 0).unwrap(),\nNaiveDate::from_ymd_opt(2022, 1, 4).unwrap().and_hms_opt(0, 0, 0).unwrap(),\nNaiveDate::from_ymd_opt(2022, 1, 5).unwrap().and_hms_opt(0, 0, 0).unwrap()\n],\n\"float\" =&gt; &amp;[4.0, 5.0, 6.0, 7.0, 8.0]\n).expect(\"should not fail\");\nprintln!(\"{}\",df);\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df = pl.DataFrame({\ninteger: [1, 2, 3, 4, 5],\ndate: [\nnew Date(2022, 1, 1, 0, 0),\nnew Date(2022, 1, 2, 0, 0),\nnew Date(2022, 1, 3, 0, 0),\nnew Date(2022, 1, 4, 0, 0),\nnew Date(2022, 1, 5, 0, 0),\n],\nfloat: [4.0, 5.0, 6.0, 7.0, 8.0],\n});\nconsole.log(df);\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integer \u2506 date                \u2506 float \u2502\n\u2502 ---     \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64     \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1       \u2506 2022-01-01 00:00:00 \u2506 4.0   \u2502\n\u2502 2       \u2506 2022-01-02 00:00:00 \u2506 5.0   \u2502\n\u2502 3       \u2506 2022-01-03 00:00:00 \u2506 6.0   \u2502\n\u2502 4       \u2506 2022-01-04 00:00:00 \u2506 7.0   \u2502\n\u2502 5       \u2506 2022-01-05 00:00:00 \u2506 8.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/series-dataframes/#viewing-data","title":"Viewing data","text":"<p>This part focuses on viewing data in a <code>DataFrame</code>. We will use the <code>DataFrame</code> from the previous example as a starting point.</p>"},{"location":"getting-started/series-dataframes/#head","title":"Head","text":"<p>The <code>head</code> function shows by default the first 5 rows of a <code>DataFrame</code>. You can specify the number of rows you want to see (e.g. <code>df.head(10)</code>).</p>  Python Rust NodeJS <p> <code>head</code> <pre><code>print(df.head(3))\n</code></pre></p> <p> <code>head</code> <pre><code>println!(\"{}\",df.head(Some(3)));\n</code></pre></p> <p> <code>head</code> <pre><code>console.log(df.head(3));\n</code></pre></p> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integer \u2506 date                \u2506 float \u2502\n\u2502 ---     \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64     \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1       \u2506 2022-01-01 00:00:00 \u2506 4.0   \u2502\n\u2502 2       \u2506 2022-01-02 00:00:00 \u2506 5.0   \u2502\n\u2502 3       \u2506 2022-01-03 00:00:00 \u2506 6.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/series-dataframes/#tail","title":"Tail","text":"<p>The <code>tail</code> function shows the last 5 rows of a <code>DataFrame</code>. You can also specify the number of rows you want to see, similar to <code>head</code>.</p>  Python Rust NodeJS <p> <code>tail</code> <pre><code>print(df.tail(3))\n</code></pre></p> <p> <code>tail</code> <pre><code>println!(\"{}\",df.tail(Some(3)));\n</code></pre></p> <p> <code>tail</code> <pre><code>console.log(df.tail(3));\n</code></pre></p> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integer \u2506 date                \u2506 float \u2502\n\u2502 ---     \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64     \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3       \u2506 2022-01-03 00:00:00 \u2506 6.0   \u2502\n\u2502 4       \u2506 2022-01-04 00:00:00 \u2506 7.0   \u2502\n\u2502 5       \u2506 2022-01-05 00:00:00 \u2506 8.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/series-dataframes/#sample","title":"Sample","text":"<p>If you want to get an impression of the data of your <code>DataFrame</code>, you can also use <code>sample</code>. With <code>sample</code> you get an n number of random rows from the <code>DataFrame</code>.</p>  Python Rust NodeJS <p> <code>sample</code> <pre><code>print(df.sample(2))\n</code></pre></p> <p> <code>sample_n</code> <pre><code>println!(\"{}\",df.sample_n(2, false, true, None)?);\n</code></pre></p> <p> <code>sample</code> <pre><code>console.log(df.sample(2));\n</code></pre></p> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integer \u2506 date                \u2506 float \u2502\n\u2502 ---     \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64     \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1       \u2506 2022-01-01 00:00:00 \u2506 4.0   \u2502\n\u2502 4       \u2506 2022-01-04 00:00:00 \u2506 7.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/series-dataframes/#describe","title":"Describe","text":"<p><code>Describe</code> returns summary statistics of your <code>DataFrame</code>. It will provide several quick statistics if possible.</p>  Python Rust NodeJS <p> <code>describe</code> <pre><code>print(df.describe())\n</code></pre></p> <p> <code>describe</code> \u00b7  Available on feature describe <pre><code>println!(\"{}\",df.describe(None));\n</code></pre></p> <p> <code>describe</code> <pre><code>console.log(df.describe());\n</code></pre></p> <pre><code>shape: (9, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 describe   \u2506 integer  \u2506 date                \u2506 float    \u2502\n\u2502 ---        \u2506 ---      \u2506 ---                 \u2506 ---      \u2502\n\u2502 str        \u2506 f64      \u2506 str                 \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 count      \u2506 5.0      \u2506 5                   \u2506 5.0      \u2502\n\u2502 null_count \u2506 0.0      \u2506 0                   \u2506 0.0      \u2502\n\u2502 mean       \u2506 3.0      \u2506 null                \u2506 6.0      \u2502\n\u2502 std        \u2506 1.581139 \u2506 null                \u2506 1.581139 \u2502\n\u2502 min        \u2506 1.0      \u2506 2022-01-01 00:00:00 \u2506 4.0      \u2502\n\u2502 max        \u2506 5.0      \u2506 2022-01-05 00:00:00 \u2506 8.0      \u2502\n\u2502 median     \u2506 3.0      \u2506 null                \u2506 6.0      \u2502\n\u2502 25%        \u2506 2.0      \u2506 null                \u2506 5.0      \u2502\n\u2502 75%        \u2506 4.0      \u2506 null                \u2506 7.0      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"snippets/rust_lazy_clone_warning/","title":"Rust lazy clone warning","text":"<p> Rust Users Only</p> <p>Due to historical reasons the eager API in Rust is outdated. In the future we would like to redesign it as a small wrapper around the lazy API (as is the design in Python / NodeJS). In the  examples we will use the lazy API instead with <code>.lazy()</code> and <code>.collect()</code>. For now you can ignore these two functions. If you want to know more about the lazy and eager API go here. </p> <p>To enable the Lazy API ensure you have the feature flag <code>lazy</code> configured when installing Polars <pre><code># Cargo.toml\n[dependencies]\npolars = { version = \"x\", features = [\"lazy\", ...]}\n</code></pre></p> <p>Because of the ownership ruling in Rust we can not reuse the same <code>DataFrame</code> multiple times in the examples. For simplicity reasons we call <code>clone()</code> to overcome this issue. Note that this does not duplicate the data but just increments a pointer (<code>Arc</code>).</p>"},{"location":"snippets/under_construction/","title":"Under construction","text":"<p> Under Construction  </p> <p>This section is still under development. Want to help out? Consider contributing and making a pull request to our repository. Please read our Contribution Guidelines on how to proceed.</p>"},{"location":"user-guide/","title":"Introduction","text":"<p>This User Guide is an introduction to the <code>Polars</code> DataFrame library. Its goal is to introduce you to <code>Polars</code> by going through examples and comparing it to other solutions. Some design choices are introduced here. The guide will also introduce you to optimal usage of <code>Polars</code>.</p> <p>Even though <code>Polars</code> is completely written in <code>Rust</code> (no runtime overhead!) and uses <code>Arrow</code> -- the native arrow2 <code>Rust</code> implementation -- as its foundation, the examples presented in this guide will be mostly using its higher-level language bindings. Higher-level bindings only serve as a thin wrapper for functionality implemented in the core library.</p> <p>For <code>Pandas</code> users, our Python package will offer the easiest way to get started with <code>Polars</code>.</p>"},{"location":"user-guide/#philosophy","title":"Philosophy","text":"<p>The goal of <code>Polars</code> is to provide a lightning fast <code>DataFrame</code> library that:</p> <ul> <li>Utilizes all available cores on your machine.</li> <li>Optimizes queries to reduce unneeded work/memory allocations. </li> <li>Handles datasets much larger than your available RAM.</li> <li>Has an API that is consistent and predictable.</li> <li>Has a strict schema (data-types should be known before running the query).</li> </ul> <p>Polars is written in Rust which gives it C/C++ performance and allows it to fully control performance critical parts in a query engine.</p> <p>As such <code>Polars</code> goes to great lengths to:</p> <ul> <li>Reduce redundant copies.</li> <li>Traverse memory cache efficiently.</li> <li>Minimize contention in parallelism.</li> <li>Process data in chunks.</li> <li>Reuse memory allocations.</li> </ul>"},{"location":"user-guide/installation/","title":"Installation","text":"<p>Polars is a library and installation is as simple as invoking the package manager of the corresponding programming language.</p>  Python Rust NodeJS <pre><code>pip install polars\n</code></pre> <pre><code>cargo add polars -F lazy\n\n# Or Cargo.toml\n[dependencies]\npolars = { version = \"x\", features = [\"lazy\", ...]}\n</code></pre> <pre><code>yarn add nodejs-polars\n</code></pre>"},{"location":"user-guide/installation/#importing","title":"Importing","text":"<p>To use the library import it into your project</p>  Python Rust NodeJS <pre><code>import polars as pl\n</code></pre> <pre><code>use polars::prelude::*;\n</code></pre> <pre><code>// esm\nimport pl from 'nodejs-polars';\n\n// require\nconst pl = require('nodejs-polars'); </code></pre>"},{"location":"user-guide/installation/#feature-flags","title":"Feature Flags","text":"<p>By using the above command you install the core of <code>Polars</code> onto your system. However depending on your use case you might want to install the optional dependencies as well. These are made optional to minimize the footprint. The flags are different depending on the programming language. Throughout the user guide we will mention when a functionality is used that requires an additional dependency.</p>"},{"location":"user-guide/installation/#python","title":"Python","text":"<pre><code># For example\npip install polars[numpy, fsspec]\n</code></pre> Tag Description all Install all optional dependencies (all of the following) pandas Install with Pandas for converting data to and from Pandas Dataframes/Series numpy Install with numpy for converting data to and from numpy arrays pyarrow Reading data formats using PyArrow fsspec Support for reading from remote file systems connectorx Support for reading from SQL databases xlsx2csv Support for reading from Excel files deltalake Support for reading from Delta Lake Tables timezone Timezone support, only needed if 1. you are on Python &lt; 3.9 and/or 2. you are on Windows, otherwise no dependencies will be installed"},{"location":"user-guide/installation/#rust","title":"Rust","text":"<p><pre><code># Cargo.toml\n[dependencies]\npolars = { version = \"0.26.1\", features = [\"lazy\",\"temporal\",\"describe\",\"json\",\"parquet\",\"dtype-datetime\"]}\n</code></pre>  The opt-in features are:</p> <ul> <li>Additional data types:<ul> <li><code>dtype-date</code></li> <li><code>dtype-datetime</code></li> <li><code>dtype-time</code></li> <li><code>dtype-duration</code></li> <li><code>dtype-i8</code></li> <li><code>dtype-i16</code></li> <li><code>dtype-u8</code></li> <li><code>dtype-u16</code></li> <li><code>dtype-categorical</code></li> <li><code>dtype-struct</code></li> </ul> </li> <li><code>performant</code> - Longer compile times more fast paths.</li> <li><code>lazy</code> - Lazy API<ul> <li><code>lazy_regex</code> - Use regexes in column selection</li> <li><code>dot_diagram</code> - Create dot diagrams from lazy logical plans.</li> </ul> </li> <li><code>sql</code> - Pass SQL queries to polars.</li> <li><code>streaming</code> - Be able to process datasets that are larger than RAM.</li> <li><code>random</code> - Generate arrays with randomly sampled values</li> <li><code>ndarray</code>- Convert from <code>DataFrame</code> to <code>ndarray</code></li> <li><code>temporal</code> - Conversions between Chrono and Polars for temporal data types</li> <li><code>timezones</code> - Activate timezone support.</li> <li><code>strings</code> - Extra string utilities for <code>Utf8Chunked</code><ul> <li><code>string_justify</code> - <code>zfill</code>, <code>ljust</code>, <code>rjust</code></li> <li><code>string_from_radix</code> - <code>parse_int</code></li> </ul> </li> <li><code>object</code> - Support for generic ChunkedArrays called <code>ObjectChunked&lt;T&gt;</code> (generic over <code>T</code>).               These are downcastable from Series through the Any trait.</li> <li>Performance related:<ul> <li><code>nightly</code> - Several nightly only features such as SIMD and specialization.</li> <li><code>performant</code> - more fast paths, slower compile times.</li> <li><code>bigidx</code> - Activate this feature if you expect &gt;&gt; 2^32 rows. This has not been needed by anyone.               This allows polars to scale up way beyond that by using <code>u64</code> as an index.               Polars will be a bit slower with this feature activated as many data structures               are less cache efficient.</li> <li><code>cse</code> - Activate common subplan elimination optimization</li> </ul> </li> <li> <p>IO related:     </p> <ul> <li><code>serde</code> - Support for serde serialization and deserialization.              Can be used for JSON and more serde supported serialization formats.</li> <li><code>serde-lazy</code> - Support for serde serialization and deserialization.              Can be used for JSON and more serde supported serialization formats. </li> <li><code>parquet</code> - Read Apache Parquet format</li> <li><code>json</code> - JSON serialization</li> <li><code>ipc</code> - Arrow's IPC format serialization</li> <li><code>decompress</code> - Automatically infer compression of csvs and decompress them.                   Supported compressions:                      - zip                      - gzip</li> </ul> </li> <li> <p><code>DataFrame</code> operations:</p> <ul> <li><code>dynamic_groupby</code> - Groupby based on a time window instead of predefined keys.                        Also activates rolling window group by operations.</li> <li><code>sort_multiple</code> - Allow sorting a <code>DataFrame</code> on multiple columns</li> <li><code>rows</code> - Create <code>DataFrame</code> from rows and extract rows from <code>DataFrames</code>.             And activates <code>pivot</code> and <code>transpose</code> operations</li> <li><code>join_asof</code> - Join ASOF, to join on nearest keys instead of exact equality match.</li> <li><code>cross_join</code> - Create the cartesian product of two DataFrames.</li> <li><code>semi_anti_join</code> - SEMI and ANTI joins.</li> <li><code>groupby_list</code> - Allow groupby operation on keys of type List.</li> <li><code>row_hash</code> - Utility to hash DataFrame rows to UInt64Chunked</li> <li><code>diagonal_concat</code> - Concat diagonally thereby combining different schemas.</li> <li><code>horizontal_concat</code> - Concat horizontally and extend with null values if lengths don't match</li> <li><code>dataframe_arithmetic</code> - Arithmetic on (Dataframe and DataFrames) and (DataFrame on Series)</li> <li><code>partition_by</code> - Split into multiple DataFrames partitioned by groups.</li> </ul> </li> <li><code>Series</code>/<code>Expression</code> operations:<ul> <li><code>is_in</code> - Check for membership in <code>Series</code></li> <li><code>zip_with</code> - Zip two Series/ ChunkedArrays</li> <li><code>round_series</code> - round underlying float types of <code>Series</code>.</li> <li><code>repeat_by</code> - [Repeat element in an Array N times, where N is given by another array.</li> <li><code>is_first</code> - Check if element is first unique value.</li> <li><code>is_last</code> - Check if element is last unique value.</li> <li><code>checked_arithmetic</code> - checked arithmetic/ returning <code>None</code> on invalid operations.</li> <li><code>dot_product</code> - Dot/inner product on Series and Expressions.</li> <li><code>concat_str</code> - Concat string data in linear time.</li> <li><code>reinterpret</code> - Utility to reinterpret bits to signed/unsigned</li> <li><code>take_opt_iter</code> - Take from a Series with <code>Iterator&lt;Item=Option&lt;usize&gt;&gt;</code></li> <li><code>mode</code> - Return the most occurring value(s)</li> <li><code>cum_agg</code> - cumsum, cummin, cummax aggregation.</li> <li><code>rolling_window</code> - rolling window functions, like rolling_mean</li> <li><code>interpolate</code> interpolate None values</li> <li><code>extract_jsonpath</code> - Run jsonpath queries on Utf8Chunked</li> <li><code>list</code> - List utils.<ul> <li><code>list_take</code> take sublist by multiple indices</li> </ul> </li> <li><code>rank</code> - Ranking algorithms.</li> <li><code>moment</code> - kurtosis and skew statistics</li> <li><code>ewma</code> - Exponential moving average windows</li> <li><code>abs</code> - Get absolute values of Series</li> <li><code>arange</code> - Range operation on Series</li> <li><code>product</code> - Compute the product of a Series.</li> <li><code>diff</code> - <code>diff</code> operation.</li> <li><code>pct_change</code> - Compute change percentages.</li> <li><code>unique_counts</code> - Count unique values in expressions.</li> <li><code>log</code> - Logarithms for <code>Series</code>.</li> <li><code>list_to_struct</code> - Convert <code>List</code> to <code>Struct</code> dtypes.</li> <li><code>list_count</code> - Count elements in lists.</li> <li><code>list_eval</code> - Apply expressions over list elements.</li> <li><code>cumulative_eval</code> - Apply expressions over cumulatively increasing windows.</li> <li><code>arg_where</code> - Get indices where condition holds.</li> <li><code>search_sorted</code> - Find indices where elements should be inserted to maintain order.</li> <li><code>date_offset</code> Add an offset to dates that take months and leap years into account.</li> <li><code>trigonometry</code> Trigonometric functions.</li> <li><code>sign</code> Compute the element-wise sign of a Series.</li> <li><code>propagate_nans</code> NaN propagating min/max aggregations.</li> </ul> </li> <li><code>DataFrame</code> pretty printing<ul> <li><code>fmt</code> - Activate DataFrame formatting</li> </ul> </li> </ul>"},{"location":"user-guide/concepts/contexts/","title":"Contexts","text":"<p>Polars has developed its own Domain Specific Language (DSL) for transforming data. The language is very easy to use and allows for complex queries that remain human readable. The two core components of the language are Contexts and Expressions, the latter we will cover in the next section. </p> <p>A context, as implied by the name, refers to the context in which an expression needs to be evaluated. There are three main contexts 1: </p> <ol> <li>Selection: <code>df.select([..])</code>, <code>df.with_columns([..])</code></li> <li>Filtering: <code>df.filter()</code></li> <li>Groupy / Aggregation: <code>df.groupby(..).agg([..])</code></li> </ol> <p>The examples below are performed on the following <code>DataFrame</code>:</p>  Python Rust NodeJS <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"nrs\": [1, 2, 3, None, 5],\n        \"names\": [\"foo\", \"ham\", \"spam\", \"egg\", None],\n        \"random\": np.random.rand(5),\n        \"groups\": [\"A\", \"A\", \"B\", \"C\", \"B\"],\n    }\n)\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>use rand::{thread_rng, Rng};\n\nlet mut arr = [0f64; 5];\nthread_rng().fill(&amp;mut arr);\n\nlet df = df! (\n\"nrs\" =&gt; &amp;[Some(1), Some(2), Some(3), None, Some(5)],\n\"names\" =&gt; &amp;[Some(\"foo\"), Some(\"ham\"), Some(\"spam\"), Some(\"eggs\"), None],\n\"random\" =&gt; &amp;arr,\n\"groups\" =&gt; &amp;[\"A\", \"A\", \"B\", \"C\", \"B\"],\n)?;\n\nprintln!(\"{}\", &amp;df);\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>const arr = Array.from({ length: 5 }).map((_) =&gt;\nchance.floating({ min: 0, max: 1 }),\n);\n\nlet df = pl.DataFrame({\nnrs: [1, 2, 3, null, 5],\nnames: [\"foo\", \"ham\", \"spam\", \"egg\", null],\nrandom: arr,\ngroups: [\"A\", \"A\", \"B\", \"C\", \"B\"],\n});\nconsole.log(df);\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs  \u2506 names \u2506 random   \u2506 groups \u2502\n\u2502 ---  \u2506 ---   \u2506 ---      \u2506 ---    \u2502\n\u2502 i64  \u2506 str   \u2506 f64      \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 foo   \u2506 0.154163 \u2506 A      \u2502\n\u2502 2    \u2506 ham   \u2506 0.74005  \u2506 A      \u2502\n\u2502 3    \u2506 spam  \u2506 0.263315 \u2506 B      \u2502\n\u2502 null \u2506 egg   \u2506 0.533739 \u2506 C      \u2502\n\u2502 5    \u2506 null  \u2506 0.014575 \u2506 B      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/contexts/#select","title":"Select","text":"<p>In the <code>select</code> context the selection applies expressions over columns. The expressions in this context must produce <code>Series</code> that are all the same length or have a length of 1.</p> <p>A <code>Series</code> of a length of 1 will be broadcasted to match the height of the <code>DataFrame</code>. Note that a select may produce new columns that are aggregations, combinations of expressions, or literals.</p>  Python Rust NodeJS <p> <code>select</code> <pre><code>out = df.select(\n    [\n        pl.sum(\"nrs\"),\n        pl.col(\"names\").sort(),\n        pl.col(\"names\").first().alias(\"first name\"),\n        (pl.mean(\"nrs\") * 10).alias(\"10xnrs\"),\n    ]\n)\nprint(out)\n</code></pre></p> <p> <code>select</code> <pre><code>let out = df\n.clone()\n.lazy()\n.select([\nsum(\"nrs\"),\ncol(\"names\").sort(false),\ncol(\"names\").first().alias(\"first name\"),\n(mean(\"nrs\") * lit(10)).alias(\"10xnrs\"),\n])\n.collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <p> <code>select</code> <pre><code>let out = df.select(\npl.col(\"nrs\").sum(),\npl.col(\"names\").sort(),\npl.col(\"names\").first().alias(\"first name\"),\npl.mean(\"nrs\").multiplyBy(10).alias(\"10xnrs\"),\n);\nconsole.log(out);\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs \u2506 names \u2506 first name \u2506 10xnrs \u2502\n\u2502 --- \u2506 ---   \u2506 ---        \u2506 ---    \u2502\n\u2502 i64 \u2506 str   \u2506 str        \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 11  \u2506 null  \u2506 foo        \u2506 27.5   \u2502\n\u2502 11  \u2506 egg   \u2506 foo        \u2506 27.5   \u2502\n\u2502 11  \u2506 foo   \u2506 foo        \u2506 27.5   \u2502\n\u2502 11  \u2506 ham   \u2506 foo        \u2506 27.5   \u2502\n\u2502 11  \u2506 spam  \u2506 foo        \u2506 27.5   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>As you can see from the query the <code>select</code> context is very powerful and allows you to perform arbitrary expressions independent (and in parallel) of each other. </p> <p>Similarly to the <code>select</code> statement there is the <code>with_columns</code> statement which also is an entrance to the selection context. The main difference is that <code>with_columns</code> retains the original columns and adds new ones while <code>select</code> drops the original columns.</p>  Python Rust NodeJS <p> <code>with_columns</code> <pre><code>df = df.with_columns(\n    [\n        pl.sum(\"nrs\").alias(\"nrs_sum\"),\n        pl.col(\"random\").count().alias(\"count\"),\n    ]\n)\nprint(df)\n</code></pre></p> <p> <code>with_columns</code> <pre><code>let out = df\n.clone()\n.lazy()\n.with_columns([\nsum(\"nrs\").alias(\"nrs_sum\"),\ncol(\"random\").count().alias(\"count\"),\n])\n.collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <p> <code>withColumns</code> <pre><code>df = df.withColumns(\npl.col(\"nrs\").sum().alias(\"nrs_sum\"),\npl.col(\"random\").count().alias(\"count\"),\n);\n\nconsole.log(df);\n</code></pre></p> <pre><code>shape: (5, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs  \u2506 names \u2506 random   \u2506 groups \u2506 nrs_sum \u2506 count \u2502\n\u2502 ---  \u2506 ---   \u2506 ---      \u2506 ---    \u2506 ---     \u2506 ---   \u2502\n\u2502 i64  \u2506 str   \u2506 f64      \u2506 str    \u2506 i64     \u2506 u32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 foo   \u2506 0.154163 \u2506 A      \u2506 11      \u2506 5     \u2502\n\u2502 2    \u2506 ham   \u2506 0.74005  \u2506 A      \u2506 11      \u2506 5     \u2502\n\u2502 3    \u2506 spam  \u2506 0.263315 \u2506 B      \u2506 11      \u2506 5     \u2502\n\u2502 null \u2506 egg   \u2506 0.533739 \u2506 C      \u2506 11      \u2506 5     \u2502\n\u2502 5    \u2506 null  \u2506 0.014575 \u2506 B      \u2506 11      \u2506 5     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/contexts/#filter","title":"Filter","text":"<p>In the <code>filter</code> context you filter the existing dataframe based on arbritary expression which evaluates to the <code>Boolean</code> data type. </p>  Python Rust NodeJS <p> <code>filter</code> <pre><code>out = df.filter(pl.col(\"nrs\") &gt; 2)\nprint(out)\n</code></pre></p> <p> <code>filter</code> <pre><code>let out = df.clone().lazy().filter(col(\"nrs\").gt(lit(2))).collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <p> <code>filter</code> <pre><code>out = df.filter(pl.col(\"nrs\").gt(2));\nconsole.log(out);\n</code></pre></p> <pre><code>shape: (2, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs \u2506 names \u2506 random   \u2506 groups \u2506 nrs_sum \u2506 count \u2502\n\u2502 --- \u2506 ---   \u2506 ---      \u2506 ---    \u2506 ---     \u2506 ---   \u2502\n\u2502 i64 \u2506 str   \u2506 f64      \u2506 str    \u2506 i64     \u2506 u32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3   \u2506 spam  \u2506 0.263315 \u2506 B      \u2506 11      \u2506 5     \u2502\n\u2502 5   \u2506 null  \u2506 0.014575 \u2506 B      \u2506 11      \u2506 5     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/contexts/#groupby-aggregation","title":"Groupby / Aggregation","text":"<p>In the <code>groupby</code> context expressions work on groups and thus may yield results of any length (a group may have many members).</p>  Python Rust NodeJS <p> <code>groupby</code> <pre><code>out = df.groupby(\"groups\").agg(\n    [\n        pl.sum(\"nrs\"),  # sum nrs by groups\n        pl.col(\"random\").count().alias(\"count\"),  # count group members\n        # sum random where name != null\n        pl.col(\"random\").filter(pl.col(\"names\").is_not_null()).sum().suffix(\"_sum\"),\n        pl.col(\"names\").reverse().alias((\"reversed names\")),\n    ]\n)\nprint(out)\n</code></pre></p> <p> <code>groupby</code> <pre><code>let out = df\n.lazy()\n.groupby([col(\"groups\")])\n.agg([\nsum(\"nrs\"),                           // sum nrs by groups\ncol(\"random\").count().alias(\"count\"), // count group members\n// sum random where name != null\ncol(\"random\")\n.filter(col(\"names\").is_not_null())\n.sum()\n.suffix(\"_sum\"),\ncol(\"names\").reverse().alias(\"reversed names\"),\n])\n.collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <p> <code>groupBy</code> <pre><code>out = df.groupBy(\"groups\").agg(\npl\n.col(\"nrs\")\n.sum(), // sum nrs by groups\npl\n.col(\"random\")\n.count()\n.alias(\"count\"), // count group members\n// sum random where name != null\npl\n.col(\"random\")\n.filter(pl.col(\"names\").isNotNull())\n.sum()\n.suffix(\"_sum\"),\npl.col(\"names\").reverse().alias(\"reversed names\"),\n);\nconsole.log(out);\n</code></pre></p> <pre><code>shape: (3, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 groups \u2506 nrs  \u2506 count \u2506 random_sum \u2506 reversed names \u2502\n\u2502 ---    \u2506 ---  \u2506 ---   \u2506 ---        \u2506 ---            \u2502\n\u2502 str    \u2506 i64  \u2506 u32   \u2506 f64        \u2506 list[str]      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 B      \u2506 8    \u2506 2     \u2506 0.263315   \u2506 [null, \"spam\"] \u2502\n\u2502 C      \u2506 null \u2506 1     \u2506 0.533739   \u2506 [\"egg\"]        \u2502\n\u2502 A      \u2506 3    \u2506 2     \u2506 0.894213   \u2506 [\"ham\", \"foo\"] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>As you can see from the result all expressions are applied to the group defined by the <code>groupby</code> context. Besides the standard <code>groupby</code>, <code>groupby_dynamic</code>, and <code>groupby_rolling</code> are also entrances to the groupby context.</p> <ol> <li> <p>There are additional List and SQL contexts which are covered later in this guide. But for simplicity, we leave them out of scope for now.\u00a0\u21a9</p> </li> </ol>"},{"location":"user-guide/concepts/data-structures/","title":"Data Structures","text":"<p>The core base data structures provided by Polars are  <code>Series</code> and <code>DataFrames</code>. </p>"},{"location":"user-guide/concepts/data-structures/#series","title":"Series","text":"<p>Series are a 1-dimensional data structure. Within a series all elements have the same Data Type .  The snippet below shows how to create a simple named <code>Series</code> object. </p>  Python Rust NodeJS <p> <code>Series</code> <pre><code>import polars as pl\n\ns = pl.Series(\"a\", [1, 2, 3, 4, 5])\nprint(s)\n</code></pre></p> <p> <code>Series</code> <pre><code>use chrono::prelude::*;\n\nlet s = Series::new(\"a\", [1, 2, 3, 4, 5]);\nprintln!(\"{}\",s);\n</code></pre></p> <p> <code>Series</code> <pre><code>const pl = require(\"nodejs-polars\");\n\nvar s = pl.Series(\"a\", [1, 2, 3, 4, 5]);\nconsole.log(s);\n</code></pre></p> <pre><code>shape: (5,)\nSeries: 'a' [i64]\n[\n    1\n    2\n    3\n    4\n    5\n]\n</code></pre>"},{"location":"user-guide/concepts/data-structures/#dataframe","title":"DataFrame","text":"<p>A <code>DataFrame</code> is a 2-dimensional data structure that is backed by a <code>Series</code>, and it can be seen as an abstraction of a collection (e.g. list) of <code>Series</code>. Operations that can be executed on a <code>DataFrame</code> are very similar to what is done in a <code>SQL</code> like query. You can <code>GROUP BY</code>, <code>JOIN</code>, <code>PIVOT</code>, but also define custom functions.</p>  Python Rust NodeJS <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"integer\": [1, 2, 3, 4, 5],\n        \"date\": [\n            datetime(2022, 1, 1),\n            datetime(2022, 1, 2),\n            datetime(2022, 1, 3),\n            datetime(2022, 1, 4),\n            datetime(2022, 1, 5),\n        ],\n        \"float\": [4.0, 5.0, 6.0, 7.0, 8.0],\n    }\n)\n\nprint(df)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df: DataFrame = df!(\"integer\" =&gt; &amp;[1, 2, 3, 4, 5],\n\"date\" =&gt; &amp;[\nNaiveDate::from_ymd_opt(2022, 1, 1).unwrap().and_hms_opt(0, 0, 0).unwrap(),\nNaiveDate::from_ymd_opt(2022, 1, 2).unwrap().and_hms_opt(0, 0, 0).unwrap(),\nNaiveDate::from_ymd_opt(2022, 1, 3).unwrap().and_hms_opt(0, 0, 0).unwrap(),\nNaiveDate::from_ymd_opt(2022, 1, 4).unwrap().and_hms_opt(0, 0, 0).unwrap(),\nNaiveDate::from_ymd_opt(2022, 1, 5).unwrap().and_hms_opt(0, 0, 0).unwrap()\n],\n\"float\" =&gt; &amp;[4.0, 5.0, 6.0, 7.0, 8.0]\n).expect(\"should not fail\");\nprintln!(\"{}\",df);\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let df = pl.DataFrame({\ninteger: [1, 2, 3, 4, 5],\ndate: [\nnew Date(2022, 1, 1, 0, 0),\nnew Date(2022, 1, 2, 0, 0),\nnew Date(2022, 1, 3, 0, 0),\nnew Date(2022, 1, 4, 0, 0),\nnew Date(2022, 1, 5, 0, 0),\n],\nfloat: [4.0, 5.0, 6.0, 7.0, 8.0],\n});\nconsole.log(df);\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integer \u2506 date                \u2506 float \u2502\n\u2502 ---     \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64     \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1       \u2506 2022-01-01 00:00:00 \u2506 4.0   \u2502\n\u2502 2       \u2506 2022-01-02 00:00:00 \u2506 5.0   \u2502\n\u2502 3       \u2506 2022-01-03 00:00:00 \u2506 6.0   \u2502\n\u2502 4       \u2506 2022-01-04 00:00:00 \u2506 7.0   \u2502\n\u2502 5       \u2506 2022-01-05 00:00:00 \u2506 8.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/data-structures/#viewing-data","title":"Viewing data","text":"<p>This part focuses on viewing data in a <code>DataFrame</code>. We will use the <code>DataFrame</code> from the previous example as a starting point.</p>"},{"location":"user-guide/concepts/data-structures/#head","title":"Head","text":"<p>The <code>head</code> function shows by default the first 5 rows of a <code>DataFrame</code>. You can specify the number of rows you want to see (e.g. <code>df.head(10)</code>).</p>  Python Rust NodeJS <p> <code>head</code> <pre><code>print(df.head(3))\n</code></pre></p> <p> <code>head</code> <pre><code>println!(\"{}\",df.head(Some(3)));\n</code></pre></p> <p> <code>head</code> <pre><code>console.log(df.head(3));\n</code></pre></p> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integer \u2506 date                \u2506 float \u2502\n\u2502 ---     \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64     \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1       \u2506 2022-01-01 00:00:00 \u2506 4.0   \u2502\n\u2502 2       \u2506 2022-01-02 00:00:00 \u2506 5.0   \u2502\n\u2502 3       \u2506 2022-01-03 00:00:00 \u2506 6.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/data-structures/#tail","title":"Tail","text":"<p>The <code>tail</code> function shows the last 5 rows of a <code>DataFrame</code>. You can also specify the number of rows you want to see, similar to <code>head</code>.</p>  Python Rust NodeJS <p> <code>tail</code> <pre><code>print(df.tail(3))\n</code></pre></p> <p> <code>tail</code> <pre><code>println!(\"{}\",df.tail(Some(3)));\n</code></pre></p> <p> <code>tail</code> <pre><code>console.log(df.tail(3));\n</code></pre></p> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integer \u2506 date                \u2506 float \u2502\n\u2502 ---     \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64     \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3       \u2506 2022-01-03 00:00:00 \u2506 6.0   \u2502\n\u2502 4       \u2506 2022-01-04 00:00:00 \u2506 7.0   \u2502\n\u2502 5       \u2506 2022-01-05 00:00:00 \u2506 8.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/data-structures/#sample","title":"Sample","text":"<p>If you want to get an impression of the data of your <code>DataFrame</code>, you can also use <code>sample</code>. With <code>sample</code> you get an n number of random rows from the <code>DataFrame</code>.</p>  Python Rust NodeJS <p> <code>sample</code> <pre><code>print(df.sample(2))\n</code></pre></p> <p> <code>sample_n</code> <pre><code>println!(\"{}\",df.sample_n(2, false, true, None)?);\n</code></pre></p> <p> <code>sample</code> <pre><code>console.log(df.sample(2));\n</code></pre></p> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integer \u2506 date                \u2506 float \u2502\n\u2502 ---     \u2506 ---                 \u2506 ---   \u2502\n\u2502 i64     \u2506 datetime[\u03bcs]        \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1       \u2506 2022-01-01 00:00:00 \u2506 4.0   \u2502\n\u2502 5       \u2506 2022-01-05 00:00:00 \u2506 8.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/data-structures/#describe","title":"Describe","text":"<p><code>Describe</code> returns summary statistics of your <code>DataFrame</code>. It will provide several quick statistics if possible.</p>  Python Rust NodeJS <p> <code>describe</code> <pre><code>print(df.describe())\n</code></pre></p> <p> <code>describe</code> \u00b7  Available on feature describe <pre><code>println!(\"{}\",df.describe(None));\n</code></pre></p> <p> <code>describe</code> <pre><code>console.log(df.describe());\n</code></pre></p> <pre><code>shape: (9, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 describe   \u2506 integer  \u2506 date                \u2506 float    \u2502\n\u2502 ---        \u2506 ---      \u2506 ---                 \u2506 ---      \u2502\n\u2502 str        \u2506 f64      \u2506 str                 \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 count      \u2506 5.0      \u2506 5                   \u2506 5.0      \u2502\n\u2502 null_count \u2506 0.0      \u2506 0                   \u2506 0.0      \u2502\n\u2502 mean       \u2506 3.0      \u2506 null                \u2506 6.0      \u2502\n\u2502 std        \u2506 1.581139 \u2506 null                \u2506 1.581139 \u2502\n\u2502 min        \u2506 1.0      \u2506 2022-01-01 00:00:00 \u2506 4.0      \u2502\n\u2502 max        \u2506 5.0      \u2506 2022-01-05 00:00:00 \u2506 8.0      \u2502\n\u2502 median     \u2506 3.0      \u2506 null                \u2506 6.0      \u2502\n\u2502 25%        \u2506 2.0      \u2506 null                \u2506 5.0      \u2502\n\u2502 75%        \u2506 4.0      \u2506 null                \u2506 7.0      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/data-types/","title":"Data types","text":"<p><code>Polars</code> is entirely based on <code>Arrow</code> data types and backed by <code>Arrow</code> memory arrays. This makes data processing cache-efficient and well-supported for Inter Process Communication. Most data types follow the exact implementation from <code>Arrow</code>, with exception of <code>Utf8</code> (this is actually <code>LargeUtf8</code>), <code>Categorical</code>, and <code>Object</code> (support is limited). The data types are:</p> Group Type Details Numeric <code>Int8</code> 8-bit signed integer. <code>Int16</code> 16-bit signed integer. <code>Int32</code> 32-bit signed integer. <code>Int64</code> 64-bit signed integer. <code>UInt8</code> 8-bit unsigned integer. <code>UInt16</code> 16-bit unsigned integer. <code>UInt32</code> 32-bit unsigned integer. <code>UInt64</code> 64-bit unsigned integer. <code>Float32</code> 32-bit floating point. <code>Float64</code> 64-bit floating point. Nested <code>Struct</code> A struct array is represented as <code>Vec&lt;Series&gt;</code> and is useful to pack multiple/heterogenous values in a single column. <code>List</code> A list array contains a child array containing the list values and an offset array. (this is actually <code>Arrow</code> <code>LargeList</code> internally). Temporal <code>Date</code> Date representation, internally represented as days since UNIX epoch encoded by a 32-bit signed integer. <code>Datetime</code> Datetime representation, internally represented as microseconds since UNIX epoch encoded by a 64-bit signed integer. <code>Duration</code> A timedelta type, internally represented as microseconds. Created when subtracting <code>Date/Datetime</code>. <code>Time</code> Time representation, internally represented as nanoseconds since midnight. Other <code>Boolean</code> Boolean type effectively bit packed. <code>Utf8</code> String data (this is actually <code>Arrow</code> <code>LargeUtf8</code> internally). <code>Binary</code> Store data as bytes. <code>Object</code> A limited supported data type that can be any value. <code>Categorical</code> A categorical encoding of a set of strings. <p>To learn more about the internal representation of these data types, check the <code>Arrow</code> columnar format.</p>"},{"location":"user-guide/concepts/expressions/","title":"Expressions","text":"<p><code>Polars</code> has a powerful concept called expressions that is central to its very fast performance.</p> <p>Expressions are at the core of many data science operations:</p> <ul> <li>taking a sample of rows from a column</li> <li>multiplying values in a column</li> <li>extracting a column of years from dates</li> <li>convert a column of strings to lowercase</li> <li>and so on!</li> </ul> <p>However, expressions are also used within other operations:</p> <ul> <li>taking the mean of a group in a <code>groupby</code> operation</li> <li>calculating the size of groups in a <code>groupby</code> operation</li> <li>taking the sum horizontally across columns</li> </ul> <p><code>Polars</code> performs these core data transformations very quickly by:</p> <ul> <li>automatic query optimization on each expression</li> <li>automatic parallelization of expressions on many columns</li> </ul> <p>Polars expressions are a mapping from a series to a series (or mathematically <code>Fn(Series) -&gt; Series</code>). As expressions have a <code>Series</code> as an input and a <code>Series</code> as an output then it is straightforward to do a sequence of expressions (similar to method chaining in <code>Pandas</code>).</p>"},{"location":"user-guide/concepts/expressions/#examples","title":"Examples","text":"<p>The following is an expression:</p>  Python Rust NodeJS <p> <code>col</code> \u00b7 <code>sort</code> \u00b7 <code>head</code> <pre><code>pl.col(\"foo\").sort().head(2)\n</code></pre></p> <p> <code>col</code> \u00b7 <code>sort</code> \u00b7 <code>head</code> <pre><code>df.column(\"foo\")?.sort(false).head(Some(2));\n</code></pre></p> <p> <code>head</code> <pre><code>pl.col(\"foo\").sort().head(2);\n</code></pre></p> <p>The snippet above says:</p> <ol> <li>Select column \"foo\"</li> <li>Then sort the column (not in reversed order)</li> <li>Then take the first two values of the sorted output</li> </ol> <p>The power of expressions is that every expression produces a new expression, and that they can be piped together. You can run an expression by passing them to one of <code>Polars</code> execution contexts.</p> <p>Here we run two expressions by running <code>df.select</code>:</p>  Python Rust NodeJS <p> <code>select</code> <pre><code>df.select(\n    [\n        pl.col(\"foo\").sort().head(2),\n        pl.col(\"bar\").filter(pl.col(\"foo\") == 1).sum(),\n    ]\n)\n</code></pre></p> <p> <code>select</code> <pre><code>df.clone().lazy().select([\ncol(\"foo\").sort(Default::default()).head(Some(2)),\ncol(\"bar\").filter(col(\"foo\").eq(lit(1))).sum(),\n]).collect()?;\n</code></pre></p> <p> <code>select</code> <pre><code>df.select(\npl.col(\"foo\").sort().head(2),\npl.col(\"bar\").filter(pl.col(\"foo\").eq(1)).sum(),\n);\n</code></pre></p> <p>All expressions are run in parallel, meaning that separate <code>Polars</code> expressions are embarrassingly parallel. Note that within an expression there may be more parallelization going on.</p>"},{"location":"user-guide/concepts/expressions/#conclusion","title":"Conclusion","text":"<p>This is the tip of the iceberg in terms of possible expressions. There are a ton more, and they can be combined in a variety ways. This page is intended to get you familiar with the concept of expressions, in the section on expressions we will dive deeper.</p>"},{"location":"user-guide/concepts/lazy-vs-eager/","title":"Lazy / Eager API","text":"<p><code>Polars</code> supports two modes of operation: lazy and eager. In the eager API the query is executed immediately while in the lazy API the query is only evaluated once it is 'needed'. Deferring the execution to the last minute can have significant performance advantages that is why the Lazy API is preferred in most cases. Let us demonstrate this with an example:</p>  Python Rust NodeJS <p> <code>read_csv</code> <pre><code>df = pl.read_csv(\"docs/src/data/iris.csv\")\ndf_small = df.filter(pl.col(\"sepal_length\") &gt; 5)\ndf_agg = df_small.groupby(\"species\").agg(pl.col(\"sepal_width\").mean())\nprint(df_agg)\n</code></pre></p> <p> <code>CsvReader</code> \u00b7  Available on feature csv <pre><code>let df = CsvReader::from_path(\"docs/src/data/iris.csv\").unwrap().finish().unwrap();\nlet mask = df.column(\"sepal_width\")?.f64()?.gt(5.0);\nlet df_small = df.filter(&amp;mask)?;\nlet df_agg = df_small.groupby([\"species\"])?.select([\"sepal_width\"]).mean()?;\nprintln!(\"{}\", df_agg);\n</code></pre></p> <p> <code>readCSV</code> <pre><code>df = pl.readCSV(\"docs/src/data/iris.csv\");\ndf_small = df.filter(pl.col(\"sepal_length\").gt(5));\ndf_agg = df_small.groupBy(\"species\").agg(pl.col(\"sepal_width\").mean());\nconsole.log(df_agg);\n</code></pre></p> <p>In this example we use the eager API to:</p> <ol> <li>Read the iris dataset. </li> <li>Filter the dataset based on sepal length</li> <li>Calculate the mean of the sepal width per species</li> </ol> <p>Every step is executed immediately returning the intermediate results. This can be very wastefull as we might do work or load extra data that is not being used. If we instead used the lazy API and waited on execution untill all the steps are defined then the query planner could perform various optimizations. In this case:</p> <ul> <li>Predicate pushdown: Apply filters as early as possible while reading the dataset, thus only reading rows with sepal length greater than 5.</li> <li>Projection pushdown: Select only the columns that are needed while reading the dataset, thus removing the need to load additional columns (e.g. petal length &amp; petal width)</li> </ul>  Python Rust NodeJS <p> <code>scan_csv</code> <pre><code>q = (\n    pl.scan_csv(\"docs/src/data/iris.csv\")\n    .filter(pl.col(\"sepal_length\") &gt; 5)\n    .groupby(\"species\")\n    .agg(pl.col(\"sepal_width\").mean())\n)\n\ndf = q.collect()\n</code></pre></p> <p> <code>LazyCsvReader</code> \u00b7  Available on feature csv <pre><code>let q = LazyCsvReader::new(\"docs/src/data/iris.csv\")\n.has_header(true)\n.finish()?\n.filter(col(\"sepal_length\").gt(lit(5)))\n.groupby(vec![col(\"species\")])\n.agg([col(\"sepal_width\").mean()]);\nlet df = q.collect()?;\nprintln!(\"{}\", df);\n</code></pre></p> <p> <code>scanCSV</code> <pre><code>q = pl\n.scanCSV(\"docs/src/data/iris.csv\")\n.filter(pl.col(\"sepal_length\").gt(5))\n.groupBy(\"species\")\n.agg(pl.col(\"sepal_width\").mean());\n\ndf = q.collect();\n</code></pre></p> <p>These will signficantly lower the load on memory &amp; CPU thus allowing you to fit bigger datasets in memory and process faster. Once the query is defined you call <code>collect</code> to inform <code>Polars</code> that you want to execute it. In the section on Lazy API we will go into more details on its implementation.</p> <p>Eager API</p> <p>In many cases the eager API is actually calling the lazy API under the hood and immediately collecting the result. This has the benefit that within the query itself optimization(s) made by the query planner can still take place. </p>"},{"location":"user-guide/concepts/lazy-vs-eager/#when-to-use-which","title":"When to use which","text":"<p>In general the lazy API should be preferred unless you are either interested in the intermediate results or are doing exploratory work and don't know yet how your query is going to look like. </p>"},{"location":"user-guide/concepts/streaming/","title":"Streaming API","text":"<p>One additional benefit of the lazy API is that it allows queries to be executed in a streaming manner. Instead of processing the data all-at-once <code>Polars</code> can execute the query in batches allowing you to process datasets that are larger-than-memory. </p> <p>To tell Polars we want to execute a query in streaming mode we pass the <code>streaming=True</code> argument to <code>collect</code></p>  Python Rust <p> <code>collect</code> <pre><code>q = (\n    pl.scan_csv(\"docs/src/data/iris.csv\")\n    .filter(pl.col(\"sepal_length\") &gt; 5)\n    .groupby(\"species\")\n    .agg(pl.col(\"sepal_width\").mean())\n)\n\ndf = q.collect(streaming=True)\n</code></pre></p> <p> <code>collect</code> \u00b7  Available on feature streaming <pre><code>let q = LazyCsvReader::new(\"docs/src/data/iris.csv\")\n.has_header(true)\n.finish()?\n.filter(col(\"sepal_length\").gt(lit(5)))\n.groupby(vec![col(\"species\")])\n.agg([col(\"sepal_width\").mean()]);\n\nlet df = q.with_streaming(true).collect()?;\nprintln!(\"{}\", df);\n</code></pre></p>"},{"location":"user-guide/concepts/streaming/#when-is-streaming-available","title":"When is streaming available?","text":"<p>Streaming is still in development. We can ask Polars to execute any lazy query in streaming mode. However, not all lazy operations support streaming. If there is an operation for which streaming is not supported Polars will run the query in non-streaming mode.</p> <p>Streaming is supported for many operations including:</p> <ul> <li><code>filter</code>,<code>slice</code>,<code>head</code>,<code>tail</code></li> <li><code>with_columns</code>,<code>select</code></li> <li><code>groupby</code></li> <li><code>join</code></li> <li><code>sort</code></li> <li><code>explode</code>,<code>melt</code></li> <li><code>scan_csv</code>,<code>scan_parquet</code>,<code>scan_ipc</code></li> </ul>"},{"location":"user-guide/expressions/aggregation/","title":"Aggregation","text":"<p><code>Polars</code> implements a powerful syntax defined not only in its lazy API, but also in its eager API. Let's take a look at what that means.</p> <p>We can start with the simple US congress <code>dataset</code>.</p>  Python Rust <p> <code>DataFrame</code> \u00b7 <code>Categorical</code> <pre><code>url = \"https://theunitedstates.io/congress-legislators/legislators-historical.csv\"\n\ndtypes = {\n    \"first_name\": pl.Categorical,\n    \"gender\": pl.Categorical,\n    \"type\": pl.Categorical,\n    \"state\": pl.Categorical,\n    \"party\": pl.Categorical,\n}\n\ndataset = pl.read_csv(url, dtypes=dtypes).with_columns(\n    pl.col(\"birthday\").str.strptime(pl.Date, strict=False)\n)\n</code></pre></p> <p> <code>DataFrame</code> \u00b7 <code>Categorical</code> \u00b7  Available on feature dtype-categorical <pre><code>use std::io::Cursor;\nuse reqwest::blocking::Client;\n\nlet url = \"https://theunitedstates.io/congress-legislators/legislators-historical.csv\";\n\nlet mut schema = Schema::new();\nschema.with_column(\"first_name\".to_string(), DataType::Categorical(None));\nschema.with_column(\"gender\".to_string(), DataType::Categorical(None));\nschema.with_column(\"type\".to_string(), DataType::Categorical(None));\nschema.with_column(\"state\".to_string(), DataType::Categorical(None));\nschema.with_column(\"party\".to_string(), DataType::Categorical(None));\nschema.with_column(\"birthday\".to_string(), DataType::Date);\n\nlet data: Vec&lt;u8&gt; = Client::new().get(url).send()?.text()?.bytes().collect();\n\nlet dataset = CsvReader::new(Cursor::new(data))\n.has_header(true)\n.with_dtypes(Some(&amp;schema))\n.with_parse_dates(true)\n.finish()?;\n\nprintln!(\"{}\", &amp;dataset);\n</code></pre></p>"},{"location":"user-guide/expressions/aggregation/#basic-aggregations","title":"Basic aggregations","text":"<p>You can easily combine different aggregations by adding multiple expressions in a <code>list</code>. There is no upper bound on the number of aggregations you can do, and you can make any combination you want. In the snippet below we do the following aggregations:</p> <p>Per GROUP <code>\"first_name\"</code> we</p> <ul> <li>count the number of rows in the group:</li> <li>short form: <code>pl.count(\"party\")</code></li> <li>full form: <code>pl.col(\"party\").count()</code></li> <li>aggregate the gender values groups:</li> <li>full form: <code>pl.col(\"gender\")</code></li> <li>get the first value of column <code>\"last_name\"</code> in the group:</li> <li>short form: <code>pl.first(\"last_name\")</code> (not available in Rust)</li> <li>full form: <code>pl.col(\"last_name\").first()</code></li> </ul> <p>Besides the aggregation, we immediately sort the result and limit to the top <code>5</code> so that we have a nice summary overview.</p>  Python Rust <p> <code>groupby</code> <pre><code>q = (\n    dataset.lazy()\n    .groupby(\"first_name\")\n    .agg(\n        [\n            pl.count(),\n            pl.col(\"gender\"),\n            pl.first(\"last_name\"),\n        ]\n    )\n    .sort(\"count\", descending=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>groupby</code> <pre><code>let df = dataset\n.clone()\n.lazy()\n.groupby([\"first_name\"])\n.agg([count(), col(\"gender\").list(), col(\"last_name\").first()])\n.sort(\n\"count\",\nSortOptions {\ndescending: true,\nnulls_last: true,\n},\n)\n.limit(5)\n.collect()?;\n\nprintln!(\"{}\", df);\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 first_name \u2506 count \u2506 gender            \u2506 last_name \u2502\n\u2502 ---        \u2506 ---   \u2506 ---               \u2506 ---       \u2502\n\u2502 cat        \u2506 u32   \u2506 list[cat]         \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 John       \u2506 1256  \u2506 [\"M\", \"M\", \u2026 \"M\"] \u2506 Walker    \u2502\n\u2502 William    \u2506 1022  \u2506 [\"M\", \"M\", \u2026 \"M\"] \u2506 Few       \u2502\n\u2502 James      \u2506 714   \u2506 [\"M\", \"M\", \u2026 \"M\"] \u2506 Armstrong \u2502\n\u2502 Thomas     \u2506 454   \u2506 [\"M\", \"M\", \u2026 \"M\"] \u2506 Tucker    \u2502\n\u2502 Charles    \u2506 439   \u2506 [\"M\", \"M\", \u2026 \"M\"] \u2506 Carroll   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/aggregation/#conditionals","title":"Conditionals","text":"<p>It's that easy! Let's turn it up a notch. Let's say we want to know how many delegates of a \"state\" are \"Pro\" or \"Anti\" administration. We could directly query that in the aggregation without the need of <code>lambda</code> or grooming the <code>DataFrame</code>.</p>  Python Rust <p> <code>groupby</code> <pre><code>q = (\n    dataset.lazy()\n    .groupby(\"state\")\n    .agg(\n        [\n            (pl.col(\"party\") == \"Anti-Administration\").sum().alias(\"anti\"),\n            (pl.col(\"party\") == \"Pro-Administration\").sum().alias(\"pro\"),\n        ]\n    )\n    .sort(\"pro\", descending=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>groupby</code> <pre><code>let df = dataset\n.clone()\n.lazy()\n.groupby([\"state\"])\n.agg([\n(col(\"party\").eq(lit(\"Anti-Administration\")))\n.sum()\n.alias(\"anti\"),\n(col(\"party\").eq(lit(\"Pro-Administration\")))\n.sum()\n.alias(\"pro\"),\n])\n.sort(\n\"pro\",\nSortOptions {\ndescending: true,\nnulls_last: false,\n},\n)\n.limit(5)\n.collect()?;\n\nprintln!(\"{}\", df);\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 anti \u2506 pro \u2502\n\u2502 ---   \u2506 ---  \u2506 --- \u2502\n\u2502 cat   \u2506 u32  \u2506 u32 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 NJ    \u2506 0    \u2506 3   \u2502\n\u2502 CT    \u2506 0    \u2506 3   \u2502\n\u2502 NC    \u2506 1    \u2506 2   \u2502\n\u2502 SC    \u2506 0    \u2506 1   \u2502\n\u2502 MA    \u2506 0    \u2506 1   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Similarly,  this could also be done with a nested GROUPBY, but that doesn't help show off some of these nice features. \ud83d\ude09</p>  Python Rust <p> <code>groupby</code> <pre><code>q = (\n    dataset.lazy()\n    .groupby([\"state\", \"party\"])\n    .agg([pl.count(\"party\").alias(\"count\")])\n    .filter(\n        (pl.col(\"party\") == \"Anti-Administration\")\n        | (pl.col(\"party\") == \"Pro-Administration\")\n    )\n    .sort(\"count\", descending=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>groupby</code> <pre><code>let df = dataset\n.clone()\n.lazy()\n.groupby([\"state\", \"party\"])\n.agg([col(\"party\").count().alias(\"count\")])\n.filter(\ncol(\"party\")\n.eq(lit(\"Anti-Administration\"))\n.or(col(\"party\").eq(lit(\"Pro-Administration\"))),\n)\n.sort(\n\"count\",\nSortOptions {\ndescending: true,\nnulls_last: true,\n},\n)\n.limit(5)\n.collect()?;\n\nprintln!(\"{}\", df);\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 party               \u2506 count \u2502\n\u2502 ---   \u2506 ---                 \u2506 ---   \u2502\n\u2502 cat   \u2506 cat                 \u2506 u32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 NJ    \u2506 Pro-Administration  \u2506 3     \u2502\n\u2502 VA    \u2506 Anti-Administration \u2506 3     \u2502\n\u2502 CT    \u2506 Pro-Administration  \u2506 3     \u2502\n\u2502 NC    \u2506 Pro-Administration  \u2506 2     \u2502\n\u2502 PA    \u2506 Pro-Administration  \u2506 1     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/aggregation/#filtering","title":"Filtering","text":"<p>We can also filter the groups. Let's say we want to compute a mean per group, but we don't want to include all values from that group, and we also don't want to filter the rows from the <code>DataFrame</code> (because we need those rows for another aggregation).</p> <p>In the example below we show how that can be done.</p> <p>Note</p> <p>Note that we can make <code>Python</code> functions for clarity. These functions don't cost us anything. That is because we only create <code>Polars</code> expressions, we don't apply a custom function over a <code>Series</code> during runtime of the query.  Of course, you can make functions that return expressions in Rust, too.</p>  Python Rust <p> <code>groupby</code> <pre><code>def compute_age() -&gt; pl.Expr:\n    return date(2021, 1, 1).year - pl.col(\"birthday\").dt.year()\n\n\ndef avg_birthday(gender: str) -&gt; pl.Expr:\n    return (\n        compute_age()\n        .filter(pl.col(\"gender\") == gender)\n        .mean()\n        .alias(f\"avg {gender} birthday\")\n    )\n\n\nq = (\n    dataset.lazy()\n    .groupby([\"state\"])\n    .agg(\n        [\n            avg_birthday(\"M\"),\n            avg_birthday(\"F\"),\n            (pl.col(\"gender\") == \"M\").sum().alias(\"# male\"),\n            (pl.col(\"gender\") == \"F\").sum().alias(\"# female\"),\n        ]\n    )\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>groupby</code> <pre><code>fn compute_age() -&gt; Expr {\nlit(2022) - col(\"birthday\").dt().year()\n}\n\nfn avg_birthday(gender: &amp;str) -&gt; Expr {\ncompute_age()\n.filter(col(\"gender\").eq(lit(gender)))\n.mean()\n.alias(&amp;format!(\"avg {} birthday\", gender))\n}\n\nlet df = dataset\n.clone()\n.lazy()\n.groupby([\"state\"])\n.agg([\navg_birthday(\"M\"),\navg_birthday(\"F\"),\n(col(\"gender\").eq(lit(\"M\"))).sum().alias(\"# male\"),\n(col(\"gender\").eq(lit(\"F\"))).sum().alias(\"# female\"),\n])\n.limit(5)\n.collect()?;\n\nprintln!(\"{}\", df);\n</code></pre></p> <pre><code>shape: (5, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 avg M birthday \u2506 avg F birthday \u2506 # male \u2506 # female \u2502\n\u2502 ---   \u2506 ---            \u2506 ---            \u2506 ---    \u2506 ---      \u2502\n\u2502 cat   \u2506 f64            \u2506 f64            \u2506 u32    \u2506 u32      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 ID    \u2506 137.719298     \u2506 99.0           \u2506 57     \u2506 2        \u2502\n\u2502 WA    \u2506 131.43956      \u2506 84.166667      \u2506 91     \u2506 6        \u2502\n\u2502 TX    \u2506 131.637405     \u2506 78.833333      \u2506 263    \u2506 6        \u2502\n\u2502 VA    \u2506 191.542781     \u2506 65.2           \u2506 430    \u2506 5        \u2502\n\u2502 VI    \u2506 91.0           \u2506 76.0           \u2506 3      \u2506 1        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/aggregation/#sorting","title":"Sorting","text":"<p>It's common to see a <code>DataFrame</code> being sorted for the sole purpose of managing the ordering during a GROUPBY operation. Let's say that we want to get the names of the oldest and youngest politicians per state. We could SORT and GROUPBY.</p>  Python Rust <p> <code>groupby</code> <pre><code>def get_person() -&gt; pl.Expr:\n    return pl.col(\"first_name\") + pl.lit(\" \") + pl.col(\"last_name\")\n\n\nq = (\n    dataset.lazy()\n    .sort(\"birthday\", descending=True)\n    .groupby([\"state\"])\n    .agg(\n        [\n            get_person().first().alias(\"youngest\"),\n            get_person().last().alias(\"oldest\"),\n        ]\n    )\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>groupby</code> <pre><code>fn get_person() -&gt; Expr {\ncol(\"first_name\") + lit(\" \") + col(\"last_name\")\n}\n\nlet df = dataset\n.clone()\n.lazy()\n.sort(\n\"birthday\",\nSortOptions {\ndescending: true,\nnulls_last: true,\n},\n)\n.groupby([\"state\"])\n.agg([\nget_person().first().alias(\"youngest\"),\nget_person().last().alias(\"oldest\"),\n])\n.limit(5)\n.collect()?;\n\nprintln!(\"{}\", df);\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 youngest           \u2506 oldest              \u2502\n\u2502 ---   \u2506 ---                \u2506 ---                 \u2502\n\u2502 cat   \u2506 str                \u2506 str                 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 PI    \u2506 Carlos Romulo      \u2506 Pablo Ocampo        \u2502\n\u2502 MD    \u2506 Frank Kratovil     \u2506 Benjamin Contee     \u2502\n\u2502 IN    \u2506 Trey Hollingsworth \u2506 Waller Taylor       \u2502\n\u2502 IL    \u2506 Aaron Schock       \u2506 Benjamin Stephenson \u2502\n\u2502 ID    \u2506 Ra\u00fal Labrador      \u2506 William Wallace     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>However, if we also want to sort the names alphabetically, this breaks. Luckily we can sort in a <code>groupby</code> context separate from the <code>DataFrame</code>.</p>  Python Rust <p> <code>groupby</code> <pre><code>def get_person() -&gt; pl.Expr:\n    return pl.col(\"first_name\") + pl.lit(\" \") + pl.col(\"last_name\")\n\n\nq = (\n    dataset.lazy()\n    .sort(\"birthday\", descending=True)\n    .groupby([\"state\"])\n    .agg(\n        [\n            get_person().first().alias(\"youngest\"),\n            get_person().last().alias(\"oldest\"),\n            get_person().sort().first().alias(\"alphabetical_first\"),\n        ]\n    )\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>groupby</code> <pre><code>let df = dataset\n.clone()\n.lazy()\n.sort(\n\"birthday\",\nSortOptions {\ndescending: true,\nnulls_last: true,\n},\n)\n.groupby([\"state\"])\n.agg([\nget_person().first().alias(\"youngest\"),\nget_person().last().alias(\"oldest\"),\nget_person().sort(false).first().alias(\"alphabetical_first\"),\n])\n.limit(5)\n.collect()?;\n\nprintln!(\"{}\", df);\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 youngest         \u2506 oldest           \u2506 alphabetical_first \u2502\n\u2502 ---   \u2506 ---              \u2506 ---              \u2506 ---                \u2502\n\u2502 cat   \u2506 str              \u2506 str              \u2506 str                \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 WY    \u2506 Liz Cheney       \u2506 Stephen Nuckolls \u2506 Alan Simpson       \u2502\n\u2502 NC    \u2506 Madison Cawthorn \u2506 John Ashe        \u2506 Abraham Rencher    \u2502\n\u2502 WI    \u2506 Sean Duffy       \u2506 Henry Dodge      \u2506 Adolphus Nelson    \u2502\n\u2502 ID    \u2506 Ra\u00fal Labrador    \u2506 William Wallace  \u2506 Abe Goff           \u2502\n\u2502 NH    \u2506 Frank Guinta     \u2506 John Sherburne   \u2506 Aaron Cragin       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>We can even sort by another column in the <code>groupby</code> context. If we want to know if the alphabetically sorted name is male or female we could add: <code>pl.col(\"gender\").sort_by(\"first_name\").first().alias(\"gender\")</code></p>  Python Rust <p> <code>groupby</code> <pre><code>def get_person() -&gt; pl.Expr:\n    return pl.col(\"first_name\") + pl.lit(\" \") + pl.col(\"last_name\")\n\n\nq = (\n    dataset.lazy()\n    .sort(\"birthday\", descending=True)\n    .groupby([\"state\"])\n    .agg(\n        [\n            get_person().first().alias(\"youngest\"),\n            get_person().last().alias(\"oldest\"),\n            get_person().sort().first().alias(\"alphabetical_first\"),\n            pl.col(\"gender\").sort_by(\"first_name\").first().alias(\"gender\"),\n        ]\n    )\n    .sort(\"state\")\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n</code></pre></p> <p> <code>groupby</code> <pre><code>let df = dataset\n.clone()\n.lazy()\n.sort(\n\"birthday\",\nSortOptions {\ndescending: true,\nnulls_last: true,\n},\n)\n.groupby([\"state\"])\n.agg([\nget_person().first().alias(\"youngest\"),\nget_person().last().alias(\"oldest\"),\nget_person().sort(false).first().alias(\"alphabetical_first\"),\ncol(\"gender\")\n.sort_by([\"first_name\"], [false])\n.first()\n.alias(\"gender\"),\n])\n.sort(\"state\", SortOptions::default())\n.limit(5)\n.collect()?;\n\nprintln!(\"{}\", df);\n</code></pre></p> <pre><code>shape: (5, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 youngest         \u2506 oldest            \u2506 alphabetical_first \u2506 gender \u2502\n\u2502 ---   \u2506 ---              \u2506 ---               \u2506 ---                \u2506 ---    \u2502\n\u2502 cat   \u2506 str              \u2506 str               \u2506 str                \u2506 cat    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 PA    \u2506 Conor Lamb       \u2506 Thomas Fitzsimons \u2506 Aaron Kreider      \u2506 M      \u2502\n\u2502 KY    \u2506 Ben Chandler     \u2506 John Edwards      \u2506 Aaron Harding      \u2506 M      \u2502\n\u2502 MD    \u2506 Frank Kratovil   \u2506 Benjamin Contee   \u2506 Albert Blakeney    \u2506 M      \u2502\n\u2502 OH    \u2506 Anthony Gonzalez \u2506 John Smith        \u2506 Aaron Harlan       \u2506 M      \u2502\n\u2502 VA    \u2506 Scott Taylor     \u2506 William Grayson   \u2506 A. McEachin        \u2506 M      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/aggregation/#do-not-kill-parallelization","title":"Do not kill parallelization","text":"<p>Python Users Only</p> <p>The following section is specific to <code>Python</code>, and doesn't apply to <code>Rust</code>. Within <code>Rust</code>, blocks and closures (lambdas) can, and will, be executed concurrently.</p> <p>We have all heard that <code>Python</code> is slow, and does \"not scale.\" Besides the overhead of running \"slow\" bytecode, <code>Python</code> has to remain within the constraints of the Global Interpreter Lock (GIL). This means that if you were to use a <code>lambda</code> or a custom <code>Python</code> function to apply during a parallelized phase, <code>Polars</code> speed is capped running <code>Python</code> code preventing any multiple threads from executing the function.</p> <p>This all feels terribly limiting, especially because we often need those <code>lambda</code> functions in a <code>.groupby()</code> step, for example. This approach is still supported by <code>Polars</code>, but keeping in mind bytecode and the GIL costs have to be paid. It is recommended to try to solve your queries using the expression syntax before moving to <code>lambdas</code>. If you want to learn more about using <code>lambdas</code>, go to the user defined functions section. </p>"},{"location":"user-guide/expressions/aggregation/#conclusion","title":"Conclusion","text":"<p>In the examples above we've seen that we can do a lot by combining expressions. By doing so we delay the use of custom <code>Python</code> functions that slow down the queries (by the slow nature of Python AND the GIL).</p> <p>If we are missing a type expression let us know by opening a feature request!</p>"},{"location":"user-guide/expressions/casting/","title":"Casting","text":"<p>Casting converts the underlying <code>DataType</code> of a column to a new one. Polars uses Arrow to manage the data in memory and relies on the compute kernels in the rust implementation to do the conversion. Casting is available with the <code>cast()</code> method. </p> <p>The <code>cast</code> method includes a <code>strict</code> parameter that determines how Polars behaves when it encounters a value that can't be converted from the source <code>DataType</code> to the target <code>DataType</code>. By default, <code>strict=True</code>, which means that Polars will throw an error to notify the user of the failed conversion and provide details on the values that couldn't be cast. On the other hand, if <code>strict=False</code>, any values that can't be converted to the target <code>DataType</code> will be quietly converted to <code>null</code>.</p>"},{"location":"user-guide/expressions/casting/#numerics","title":"Numerics","text":"<p>Let's take a look at the following <code>DataFrame</code> which contains both integers and floating numbers.</p>  Python <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"integers\": [1, 2, 3, 4, 5],\n        \"big_integers\": [1, 10000002, 3, 10000004, 10000005],\n        \"floats\": [4.0, 5.0, 6.0, 7.0, 8.0],\n        \"floats_with_decimal\": [4.532, 5.5, 6.5, 7.5, 8.5],\n    }\n)\n\nprint(df)\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integers \u2506 big_integers \u2506 floats \u2506 floats_with_decimal \u2502\n\u2502 ---      \u2506 ---          \u2506 ---    \u2506 ---                 \u2502\n\u2502 i64      \u2506 i64          \u2506 f64    \u2506 f64                 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1        \u2506 1            \u2506 4.0    \u2506 4.532               \u2502\n\u2502 2        \u2506 10000002     \u2506 5.0    \u2506 5.5                 \u2502\n\u2502 3        \u2506 3            \u2506 6.0    \u2506 6.5                 \u2502\n\u2502 4        \u2506 10000004     \u2506 7.0    \u2506 7.5                 \u2502\n\u2502 5        \u2506 10000005     \u2506 8.0    \u2506 8.5                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To perform casting operations between floats and integers, or vice versa, we can invoke the <code>cast()</code> function.</p>  Python <p> <code>cast</code> <pre><code>out = df.select(\n    [\n        pl.col(\"integers\").cast(pl.Float32).alias(\"integers_as_floats\"),\n        pl.col(\"floats\").cast(pl.Int32).alias(\"floats_as_integers\"),\n        pl.col(\"floats_with_decimal\")\n        .cast(pl.Int32)\n        .alias(\"floats_with_decimal_as_integers\"),\n    ]\n)\nprint(out)\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integers_as_floats \u2506 floats_as_integers \u2506 floats_with_decimal_as_integers \u2502\n\u2502 ---                \u2506 ---                \u2506 ---                             \u2502\n\u2502 f32                \u2506 i32                \u2506 i32                             \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1.0                \u2506 4                  \u2506 4                               \u2502\n\u2502 2.0                \u2506 5                  \u2506 5                               \u2502\n\u2502 3.0                \u2506 6                  \u2506 6                               \u2502\n\u2502 4.0                \u2506 7                  \u2506 7                               \u2502\n\u2502 5.0                \u2506 8                  \u2506 8                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note that in the case of decimal values these are rounded downwards when casting to an integer.</p>"},{"location":"user-guide/expressions/casting/#downcast","title":"Downcast","text":"<p>Reducing the memory footprint is also achievable by modifying the number of bits allocated to an element. As an illustration, the code below demonstrates how casting from <code>Int64</code> to <code>Int16</code> and from <code>Float64</code> to <code>Float32</code> can be used to lower memory usage.</p>  Python <p> <code>cast</code> <pre><code>out = df.select(\n    [\n        pl.col(\"integers\").cast(pl.Int16).alias(\"integers_smallfootprint\"),\n        pl.col(\"floats\").cast(pl.Float32).alias(\"floats_smallfootprint\"),\n    ]\n)\nprint(out)\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integers_smallfootprint \u2506 floats_smallfootprint \u2502\n\u2502 ---                     \u2506 ---                   \u2502\n\u2502 i16                     \u2506 f32                   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1                       \u2506 4.0                   \u2502\n\u2502 2                       \u2506 5.0                   \u2502\n\u2502 3                       \u2506 6.0                   \u2502\n\u2502 4                       \u2506 7.0                   \u2502\n\u2502 5                       \u2506 8.0                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/casting/#overflow","title":"Overflow","text":"<p>When performing downcasting, it is crucial to ensure that the chosen number of bits (such as 64, 32, or 16) is sufficient to accommodate the largest and smallest numbers in the column. For example, using a 32-bit signed integer (<code>Int32</code>) allows handling integers within the range of -2147483648 to +2147483647, while using <code>Int8</code> covers integers between -128 to 127. Attempting to cast to a <code>DataType</code> that is too small will result in a <code>ComputeError</code> thrown by Polars, as the operation is not supported.</p>  Python <p> <code>cast</code> <pre><code>try:\n    out = df.select([pl.col(\"big_integers\").cast(pl.Int8)])\n    print(out)\nexcept Exception as e:\n    print(e)\n</code></pre></p> <pre><code>strict conversion from `i64` to `i8` failed for value(s) [10000002, 10000004, 10000005]; if you were trying to cast Utf8 to temporal dtypes, consider using `strptime`\n</code></pre> <p>You can set the <code>strict</code> parameter to <code>False</code>, this converts values that are overflowing to null values.</p>  Python <p> <code>cast</code> <pre><code>out = df.select([pl.col(\"big_integers\").cast(pl.Int8, strict=False)])\nprint(out)\n</code></pre></p> <pre><code>shape: (5, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 big_integers \u2502\n\u2502 ---          \u2502\n\u2502 i8           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1            \u2502\n\u2502 null         \u2502\n\u2502 3            \u2502\n\u2502 null         \u2502\n\u2502 null         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/casting/#strings","title":"Strings","text":"<p>Strings can be casted to numerical data types and vice versa:</p>  Python <p> <code>cast</code> <pre><code>df = pl.DataFrame(\n    {\n        \"integers\": [1, 2, 3, 4, 5],\n        \"float\": [4.0, 5.03, 6.0, 7.0, 8.0],\n        \"floats_as_string\": [\"4.0\", \"5.0\", \"6.0\", \"7.0\", \"8.0\"],\n    }\n)\n\nout = df.select(\n    [\n        pl.col(\"integers\").cast(pl.Utf8),\n        pl.col(\"float\").cast(pl.Utf8),\n        pl.col(\"floats_as_string\").cast(pl.Float64),\n    ]\n)\nprint(out)\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integers \u2506 float \u2506 floats_as_string \u2502\n\u2502 ---      \u2506 ---   \u2506 ---              \u2502\n\u2502 str      \u2506 str   \u2506 f64              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1        \u2506 4.0   \u2506 4.0              \u2502\n\u2502 2        \u2506 5.03  \u2506 5.0              \u2502\n\u2502 3        \u2506 6.0   \u2506 6.0              \u2502\n\u2502 4        \u2506 7.0   \u2506 7.0              \u2502\n\u2502 5        \u2506 8.0   \u2506 8.0              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In case the column contains a non-numerical value, Polars will throw a <code>ComputeError</code> detailing the conversion error. Setting <code>strict=False</code> will convert the non float value to <code>null</code>.</p>  Python <p> <code>cast</code> <pre><code>df = pl.DataFrame(\n    {\n        \"strings_not_float\": [\"4.0\", \"not_a_number\", \"6.0\", \"7.0\", \"8.0\"],\n    }\n)\ntry:\n    out = df.select([pl.col(\"strings_not_float\").cast(pl.Float64)])\n    print(out)\nexcept Exception as e:\n    print(e)\n</code></pre></p> <pre><code>strict conversion from `str` to `f64` failed for value(s) [\"not_a_number\"]; if you were trying to cast Utf8 to temporal dtypes, consider using `strptime`\n</code></pre>"},{"location":"user-guide/expressions/casting/#booleans","title":"Booleans","text":"<p>Booleans can be expressed as either 1 (<code>True</code>) or 0 (<code>False</code>). It's possible to perform casting operations between a numerical <code>DataType</code> and a boolean, and vice versa. However, keep in mind that casting from a string (<code>Utf8</code>) to a boolean is not permitted.</p>  Python <p> <code>cast</code> <pre><code>df = pl.DataFrame(\n    {\n        \"integers\": [-1, 0, 2, 3, 4],\n        \"floats\": [0.0, 1.0, 2.0, 3.0, 4.0],\n        \"bools\": [True, False, True, False, True],\n    }\n)\n\nout = df.select(\n    [\n        pl.col(\"integers\").cast(pl.Boolean),\n        pl.col(\"floats\").cast(pl.Boolean),\n    ]\n)\nprint(out)\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 integers \u2506 floats \u2502\n\u2502 ---      \u2506 ---    \u2502\n\u2502 bool     \u2506 bool   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 true     \u2506 false  \u2502\n\u2502 false    \u2506 true   \u2502\n\u2502 true     \u2506 true   \u2502\n\u2502 true     \u2506 true   \u2502\n\u2502 true     \u2506 true   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/casting/#dates","title":"Dates","text":"<p>Temporal data types such as <code>Date</code> or <code>Datetime</code> are represented as the number of days (<code>Date</code>) and microseconds (<code>Datetime</code>) since epoch. Therefore, casting between the numerical types and the temporal data types is allowed.</p>  Python <p> <code>cast</code> <pre><code>from datetime import date, datetime\n\ndf = pl.DataFrame(\n    {\n        \"date\": pl.date_range(date(2022, 1, 1), date(2022, 1, 5), eager=True),\n        \"datetime\": pl.date_range(\n            datetime(2022, 1, 1), datetime(2022, 1, 5), eager=True\n        ),\n    }\n)\n\nout = df.select([pl.col(\"date\").cast(pl.Int64), pl.col(\"datetime\").cast(pl.Int64)])\nprint(out)\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date  \u2506 datetime         \u2502\n\u2502 ---   \u2506 ---              \u2502\n\u2502 i64   \u2506 i64              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 18993 \u2506 1640995200000000 \u2502\n\u2502 18994 \u2506 1641081600000000 \u2502\n\u2502 18995 \u2506 1641168000000000 \u2502\n\u2502 18996 \u2506 1641254400000000 \u2502\n\u2502 18997 \u2506 1641340800000000 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To perform casting operations between strings and <code>Dates</code>/<code>Datetimes</code>, <code>strftime</code> and <code>strptime</code> are utilized. Polars adopts the chrono format syntax for when formatting. It's worth noting that <code>strptime</code> features additional options that support timezone functionality. Refer to the API documentation for further information.</p>  Python <p> <code>strftime</code> \u00b7 <code>strptime</code> <pre><code>df = pl.DataFrame(\n    {\n        \"date\": pl.date_range(date(2022, 1, 1), date(2022, 1, 5), eager=True),\n        \"string\": [\n            \"2022-01-01\",\n            \"2022-01-02\",\n            \"2022-01-03\",\n            \"2022-01-04\",\n            \"2022-01-05\",\n        ],\n    }\n)\n\nout = df.select(\n    [\n        pl.col(\"date\").dt.strftime(\"%Y-%m-%d\"),\n        pl.col(\"string\").str.strptime(pl.Datetime, \"%Y-%m-%d\"),\n    ]\n)\nprint(out)\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 date       \u2506 string              \u2502\n\u2502 ---        \u2506 ---                 \u2502\n\u2502 str        \u2506 datetime[\u03bcs]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2022-01-01 \u2506 2022-01-01 00:00:00 \u2502\n\u2502 2022-01-02 \u2506 2022-01-02 00:00:00 \u2502\n\u2502 2022-01-03 \u2506 2022-01-03 00:00:00 \u2502\n\u2502 2022-01-04 \u2506 2022-01-04 00:00:00 \u2502\n\u2502 2022-01-05 \u2506 2022-01-05 00:00:00 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/folds/","title":"Folds","text":"<p><code>Polars</code> provides expressions/methods for horizontal aggregations like <code>sum</code>,<code>min</code>, <code>mean</code>, etc. by setting the argument <code>axis=1</code>. However, when you need a more complex aggregation the default methods <code>Polars</code> may not be sufficient. That's when <code>folds</code> come in handy.</p> <p>The <code>fold</code> expression operates on columns for maximum speed. It utilizes the data layout very efficiently and often has vectorized execution.</p>"},{"location":"user-guide/expressions/folds/#manual-sum","title":"Manual Sum","text":"<p>Let's start with an example by implementing the <code>sum</code> operation ourselves, with a <code>fold</code>.</p>  Python Rust <p> <code>fold</code> <pre><code>df = pl.DataFrame(\n    {\n        \"a\": [1, 2, 3],\n        \"b\": [10, 20, 30],\n    }\n)\n\nout = df.select(\n    pl.fold(acc=pl.lit(0), function=lambda acc, x: acc + x, exprs=pl.all()).alias(\n        \"sum\"\n    ),\n)\nprint(out)\n</code></pre></p> <p> <code>fold_exprs</code> <pre><code>let df = df!(\n\"a\" =&gt; &amp;[1, 2, 3],\n\"b\" =&gt; &amp;[10, 20, 30],\n)?;\n\nlet out = df\n.lazy()\n.select([fold_exprs(lit(0), |acc, x| Ok(Some(acc + x)), [col(\"*\")]).alias(\"sum\")])\n.collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sum \u2502\n\u2502 --- \u2502\n\u2502 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 11  \u2502\n\u2502 22  \u2502\n\u2502 33  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The snippet above recursively applies the function <code>f(acc, x) -&gt; acc</code> to an accumulator <code>acc</code> and a new column <code>x</code>. The function operates on columns individually and can take advantage of cache efficiency and vectorization.</p>"},{"location":"user-guide/expressions/folds/#conditional","title":"Conditional","text":"<p>In the case where you'd want to apply a condition/predicate on all columns in a <code>DataFrame</code> a <code>fold</code> operation can be a very concise way to express this.</p>  Python Rust <p> <code>fold</code> <pre><code>df = pl.DataFrame(\n    {\n        \"a\": [1, 2, 3],\n        \"b\": [0, 1, 2],\n    }\n)\n\nout = df.filter(\n    pl.fold(\n        acc=pl.lit(True),\n        function=lambda acc, x: acc &amp; x,\n        exprs=pl.col(\"*\") &gt; 1,\n    )\n)\nprint(out)\n</code></pre></p> <p> <code>fold_exprs</code> <pre><code>let df = df!(\n\"a\" =&gt; &amp;[1, 2, 3],\n\"b\" =&gt; &amp;[0, 1, 2],\n)?;\n\nlet out = df\n.lazy()\n.filter(fold_exprs(\nlit(true),\n|acc, x| Some(acc.bitand(&amp;x)),\n[col(\"*\").gt(1)],\n))\n.collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3   \u2506 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In the snippet we filter all rows where each column value is <code>&gt; 1</code>.</p>"},{"location":"user-guide/expressions/folds/#folds-and-string-data","title":"Folds and string data","text":"<p>Folds could be used to concatenate string data. However, due to the materialization of intermediate columns, this operation will have squared complexity.</p> <p>Therefore, we recommend using the <code>concat_str</code> expression for this.</p>  Python Rust <p> <code>concat_str</code> <pre><code>df = pl.DataFrame(\n    {\n        \"a\": [\"a\", \"b\", \"c\"],\n        \"b\": [1, 2, 3],\n    }\n)\n\nout = df.select(\n    [\n        pl.concat_str([\"a\", \"b\"]),\n    ]\n)\nprint(out)\n</code></pre></p> <p> <code>concat_str</code> \u00b7  Available on feature concat_str <pre><code>let df = df!(\n\"a\" =&gt; &amp;[\"a\", \"b\", \"c\"],\n\"b\" =&gt; &amp;[1, 2, 3],\n)?;\n\nlet out = df\n.lazy()\n.select([concat_str([col(\"a\"), col(\"b\")], \"\")])\n.collect()?;\nprintln!(\"{:?}\", out);\n</code></pre></p> <pre><code>shape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2502\n\u2502 --- \u2502\n\u2502 str \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a1  \u2502\n\u2502 b2  \u2502\n\u2502 c3  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/functions/","title":"Functions","text":"<p><code>Polars</code> expressions have a large number of build in functions. These allow you to create complex queries without the need for user defined functions. There are too many to go through here, but we will cover some of the more popular use cases. If you want to view all the functions go to the API Reference for your programming language.</p> <p>In the examples below we will use the following <code>DataFrame</code>:</p>  Python <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"nrs\": [1, 2, 3, None, 5],\n        \"names\": [\"foo\", \"ham\", \"spam\", \"egg\", \"spam\"],\n        \"random\": np.random.rand(5),\n        \"groups\": [\"A\", \"A\", \"B\", \"C\", \"B\"],\n    }\n)\nprint(df)\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs  \u2506 names \u2506 random   \u2506 groups \u2502\n\u2502 ---  \u2506 ---   \u2506 ---      \u2506 ---    \u2502\n\u2502 i64  \u2506 str   \u2506 f64      \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 foo   \u2506 0.154163 \u2506 A      \u2502\n\u2502 2    \u2506 ham   \u2506 0.74005  \u2506 A      \u2502\n\u2502 3    \u2506 spam  \u2506 0.263315 \u2506 B      \u2502\n\u2502 null \u2506 egg   \u2506 0.533739 \u2506 C      \u2502\n\u2502 5    \u2506 spam  \u2506 0.014575 \u2506 B      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/functions/#column-selection","title":"Column Selection","text":"<p>There are various convenience methods to select multiple or all columns. </p>"},{"location":"user-guide/expressions/functions/#select-all-columns","title":"Select All Columns","text":"Python <p> <code>all</code> <pre><code>df_all = df.select([pl.col(\"*\")])\n\n# Is equivalent to\ndf_all = df.select([pl.all()])\nprint(df_all)\n</code></pre></p>"},{"location":"user-guide/expressions/functions/#select-all-columns-except","title":"Select All Columns Except","text":"Python <p> <code>exclude</code> <pre><code>df_exclude = df.select([pl.exclude(\"groups\")])\nprint(df_exclude)\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs  \u2506 names \u2506 random   \u2502\n\u2502 ---  \u2506 ---   \u2506 ---      \u2502\n\u2502 i64  \u2506 str   \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 foo   \u2506 0.154163 \u2502\n\u2502 2    \u2506 ham   \u2506 0.74005  \u2502\n\u2502 3    \u2506 spam  \u2506 0.263315 \u2502\n\u2502 null \u2506 egg   \u2506 0.533739 \u2502\n\u2502 5    \u2506 spam  \u2506 0.014575 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/functions/#column-naming","title":"Column Naming","text":"<p>By default if you perform a expression it will keep the same name as the original column. In the example below we perform an expression on the <code>nrs</code> column. Note that the output <code>DataFrame</code> still has the same name.</p>  Python Python <pre><code>df_samename = df.select([pl.col(\"nrs\") + 5])\nprint(df_samename)\n</code></pre> <pre><code>df_samename = df.select([pl.col(\"nrs\") + 5])\nprint(df_samename)\n</code></pre> <pre><code>shape: (5, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs  \u2502\n\u2502 ---  \u2502\n\u2502 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6    \u2502\n\u2502 7    \u2502\n\u2502 8    \u2502\n\u2502 null \u2502\n\u2502 10   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This might get problematic in case you use the same column muliple times in your expression as the output columns will get duplicated. For example the following query will fail.</p>  Python <pre><code>try:\n    df_samename2 = df.select([pl.col(\"nrs\") + 5, pl.col(\"nrs\") - 5])\n    print(df_samename2)\nexcept Exception as e:\n    print(e)\n</code></pre> <pre><code>column with name 'nrs' has more than one occurrences\n</code></pre> <p>You can change the output name of an expression by using the <code>alias</code> function </p>  Python <p> <code>alias</code> <pre><code>df_alias = df.select(\n    [\n        (pl.col(\"nrs\") + 5).alias(\"nrs + 5\"),\n        (pl.col(\"nrs\") - 5).alias(\"nrs - 5\"),\n    ]\n)\nprint(df_alias)\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs + 5 \u2506 nrs - 5 \u2502\n\u2502 ---     \u2506 ---     \u2502\n\u2502 i64     \u2506 i64     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6       \u2506 -4      \u2502\n\u2502 7       \u2506 -3      \u2502\n\u2502 8       \u2506 -2      \u2502\n\u2502 null    \u2506 null    \u2502\n\u2502 10      \u2506 0       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In case of multiple columns for example when using <code>all()</code> or <code>col(*)</code> you can apply a mapping function <code>map_alias</code>  to change the original column name into something else. In case you want to add a suffix (<code>suffix()</code>) or prefix (<code>prefix()</code>) these are also build in. </p>  Python <p> <code>prefix</code> <code>suffix</code> <code>map_alias</code></p>"},{"location":"user-guide/expressions/functions/#count-unique-values","title":"Count Unique Values","text":"<p>There are two ways two count unique values in <code>Polars</code> one is an exact methodology and the other one is an approximantion. The approximation uses the HyperLogLog++ algorithm to approximate the cardinality and is especially usefull for very large datasets where an approximation is good enough.</p>  Python <p> <code>n_unique</code> \u00b7 <code>approx_unique</code> <pre><code>df_alias = df.select(\n    [\n        pl.col(\"names\").n_unique().alias(\"unique\"),\n        pl.approx_unique(\"names\").alias(\"unique_approx\"),\n    ]\n)\nprint(df_alias)\n</code></pre></p> <pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 unique \u2506 unique_approx \u2502\n\u2502 ---    \u2506 ---           \u2502\n\u2502 u32    \u2506 u32           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 4      \u2506 4             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/functions/#conditionals","title":"Conditionals","text":"<p><code>Polars</code> supports if-like conditions in expression with the <code>when</code>, <code>then</code>, <code>otherwise</code> syntax. The predicate is placed in the <code>when</code> clause and when this evaluates to <code>true</code> the <code>then</code> expression is applied otherwise the <code>otherwise</code> expression is applied (row-wise).</p>  Python <p> <code>when</code> <pre><code>df_conditional = df.select(\n    [\n        pl.col(\"nrs\"),\n        pl.when(pl.col(\"nrs\") &gt; 2)\n        .then(pl.lit(True))\n        .otherwise(pl.lit(False))\n        .alias(\"conditional\"),\n    ]\n)\nprint(df_conditional)\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs  \u2506 conditional \u2502\n\u2502 ---  \u2506 ---         \u2502\n\u2502 i64  \u2506 bool        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 false       \u2502\n\u2502 2    \u2506 false       \u2502\n\u2502 3    \u2506 true        \u2502\n\u2502 null \u2506 false       \u2502\n\u2502 5    \u2506 true        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/lists/","title":"Lists","text":"<p>An expression context we haven't discussed yet is the <code>List</code> context. This means simply we can apply any expression on the elements of a <code>List</code>.</p>"},{"location":"user-guide/expressions/lists/#row-wise-computations","title":"Row wise computations","text":"<p>This context is ideal for computing things in row orientation.</p> <p>Polars expressions work on columns that have the guarantee that they consist of homogeneous data. Columns have this guarantee, rows in a <code>DataFrame</code> not so much. Luckily we have a data type that has the guarantee that the rows are homogeneous: <code>pl.List</code> data type.</p> <p>Let's say we have the following data:</p>  Python Rust <p> <code>DataFrame</code> <pre><code>grades = pl.DataFrame(\n    {\n        \"student\": [\"bas\", \"laura\", \"tim\", \"jenny\"],\n        \"arithmetic\": [10, 5, 6, 8],\n        \"biology\": [4, 6, 2, 7],\n        \"geography\": [8, 4, 9, 7],\n    }\n)\nprint(grades)\n</code></pre></p> <p> <code>DataFrame</code> <pre><code>let grades = df!(\n\"student\" =&gt; &amp;[\"bas\", \"laura\", \"tim\", \"jenny\"],\n\"arithmetic\" =&gt; &amp;[10, 5, 6, 8],\n\"biology\" =&gt; &amp;[4, 6, 2, 7],\n\"geography\" =&gt; &amp;[8, 4, 9, 7],\n)?;\nprintln!(\"{}\", grades);\n</code></pre></p> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 student \u2506 arithmetic \u2506 biology \u2506 geography \u2502\n\u2502 ---     \u2506 ---        \u2506 ---     \u2506 ---       \u2502\n\u2502 str     \u2506 i64        \u2506 i64     \u2506 i64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 bas     \u2506 10         \u2506 4       \u2506 8         \u2502\n\u2502 laura   \u2506 5          \u2506 6       \u2506 4         \u2502\n\u2502 tim     \u2506 6          \u2506 2       \u2506 9         \u2502\n\u2502 jenny   \u2506 8          \u2506 7       \u2506 7         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>If we want to compute the <code>rank</code> of all the columns except for <code>\"student\"</code>, we can collect those into a <code>list</code> data type:</p> <p>This would give:</p>  Python Rust <p> <code>concat_list</code> <pre><code>out = grades.select([pl.concat_list(pl.all().exclude(\"student\")).alias(\"all_grades\")])\nprint(out)\n</code></pre></p> <p> <code>concat_lst</code> <pre><code>let out = grades\n.clone()\n.lazy()\n.select([concat_lst([all().exclude([\"student\"])]).alias(\"all_grades\")])\n.collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 all_grades \u2502\n\u2502 ---        \u2502\n\u2502 list[i64]  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 [10, 4, 8] \u2502\n\u2502 [5, 6, 4]  \u2502\n\u2502 [6, 2, 9]  \u2502\n\u2502 [8, 7, 7]  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/lists/#running-polars-expression-on-list-elements","title":"Running polars expression on list elements","text":"<p>We can run any polars expression on the elements of a list with the <code>arr.eval</code> (<code>arr().eval</code> in Rust) expression! These expressions run entirely on polars' query engine and can run in parallel so will be super fast.</p> <p>Let's expand the example from above with something a little more interesting. Pandas allows you to compute the percentages of the <code>rank</code> values. Polars doesn't provide such a keyword argument. But because expressions are so versatile we can create our own percentage rank expression. Let's try that!</p> <p>Note that we must <code>select</code> the list's element from the context. When we apply expressions over list elements, we use <code>pl.element()</code> to select the element of a list.</p>  Python Rust <p> <code>arr.eval</code> <pre><code># the percentage rank expression\nrank_pct = pl.element().rank(descending=True) / pl.col(\"*\").count()\n\nout = grades.with_columns(\n    # create the list of homogeneous data\n    pl.concat_list(pl.all().exclude(\"student\")).alias(\"all_grades\")\n).select(\n    [\n        # select all columns except the intermediate list\n        pl.all().exclude(\"all_grades\"),\n        # compute the rank by calling `arr.eval`\n        pl.col(\"all_grades\").arr.eval(rank_pct, parallel=True).alias(\"grades_rank\"),\n    ]\n)\nprint(out)\n</code></pre></p> <p> <code>arr</code> \u00b7  Available on feature rank \u00b7  Available on feature list_eval <pre><code>// the percentage rank expression\nlet rank_opts = RankOptions {\nmethod: RankMethod::Average,\ndescending: true,\n};\nlet rank_pct = col(\"\").rank(rank_opts) / col(\"\").count().cast(DataType::Float32);\n\nlet grades = grades\n.clone()\n.lazy()\n.with_columns(\n// create the list of homogeneous data\n[concat_lst([all().exclude([\"student\"])]).alias(\"all_grades\")],\n)\n.select([\n// select all columns except the intermediate list\nall().exclude([\"all_grades\"]),\n// compute the rank by calling `arr.eval`\ncol(\"all_grades\")\n.arr()\n.eval(rank_pct, true)\n.alias(\"grades_rank\"),\n])\n.collect()?;\nprintln!(\"{}\", grades);\n</code></pre></p> <pre><code>shape: (4, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 student \u2506 arithmetic \u2506 biology \u2506 geography \u2506 grades_rank                    \u2502\n\u2502 ---     \u2506 ---        \u2506 ---     \u2506 ---       \u2506 ---                            \u2502\n\u2502 str     \u2506 i64        \u2506 i64     \u2506 i64       \u2506 list[f64]                      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 bas     \u2506 10         \u2506 4       \u2506 8         \u2506 [0.333333, 1.0, 0.666667]      \u2502\n\u2502 laura   \u2506 5          \u2506 6       \u2506 4         \u2506 [0.666667, 0.333333, 1.0]      \u2502\n\u2502 tim     \u2506 6          \u2506 2       \u2506 9         \u2506 [0.666667, 1.0, 0.333333]      \u2502\n\u2502 jenny   \u2506 8          \u2506 7       \u2506 7         \u2506 [0.333333, 0.833333, 0.833333] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note that this solution works for any expressions/operation you want to do row wise.</p>"},{"location":"user-guide/expressions/null/","title":"Missing data","text":"<p>This page sets out how missing data is represented in <code>Polars</code> and how missing data can be filled.</p>"},{"location":"user-guide/expressions/null/#null-and-nan-values","title":"<code>null</code> and <code>NaN</code> values","text":"<p>Each column in a <code>DataFrame</code> (or equivalently a <code>Series</code>) is an Arrow array or a collection of Arrow arrays based on the Apache Arrow format. Missing data is represented in Arrow and <code>Polars</code> with a <code>null</code> value. This <code>null</code> missing value applies for all data types including numerical values.</p> <p><code>Polars</code> also allows <code>NotaNumber</code> or <code>NaN</code> values for float columns. These <code>NaN</code> values are considered to be a type of floating point data rather than missing data. We discuss <code>NaN</code> values separately below.</p> <p>You can manually define a missing value with the python <code>None</code> value:</p>  Python <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"value\": [1, None],\n    },\n)\nprint(df)\n</code></pre></p> <pre><code>shape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 value \u2502\n\u2502 ---   \u2502\n\u2502 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2502\n\u2502 null  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Info</p> <p>In <code>Pandas</code> the value for missing data depends on the dtype of the column. In <code>Polars</code> missing data is always represented as a <code>null</code> value.</p>"},{"location":"user-guide/expressions/null/#missing-data-metadata","title":"Missing data metadata","text":"<p>Each Arrow array used by <code>Polars</code> stores two kinds of metadata related to missing data. This metadata allows <code>Polars</code> to quickly show how many missing values there are and which values are missing.</p> <p>The first piece of metadata is the <code>null_count</code> - this is the number of rows with <code>null</code> values in the column:</p>  Python <p> <code>null_count</code> <pre><code>null_count_df = df.null_count()\nprint(null_count_df)\n</code></pre></p> <pre><code>shape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 value \u2502\n\u2502 ---   \u2502\n\u2502 u32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The <code>null_count</code> method can be called on a <code>DataFrame</code>, a column from a <code>DataFrame</code> or a <code>Series</code>. The <code>null_count</code>method is a cheap operation as <code>null_count</code> is already calculated for the underlying Arrow array.</p> <p>The second piece of metadata is an array called a validity bitmap that indicates whether each data value is valid or missing. The validity bitmap is memory efficient as it is bit encoded - each value is either a 0 or a 1. This bit encoding means the memory overhead per array is only (array length / 8) bytes. The validity bitmap is used by the <code>is_null</code> method in <code>Polars</code>.</p> <p>You can return a <code>Series</code> based on the validity bitmap for a column in a <code>DataFrame</code> or a <code>Series</code> with the <code>is_null</code> method:</p>  Python <p> <code>is_null</code> <pre><code>is_null_series = df.select(\n    pl.col(\"value\").is_null(),\n)\nprint(is_null_series)\n</code></pre></p> <pre><code>shape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 value \u2502\n\u2502 ---   \u2502\n\u2502 bool  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 false \u2502\n\u2502 true  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The <code>is_null</code> method is a cheap operation that does not require scanning the full column for <code>null</code> values. This is because the validity bitmap already exists and can be returned as a Boolean array.</p>"},{"location":"user-guide/expressions/null/#filling-missing-data","title":"Filling missing data","text":"<p>Missing data in a <code>Series</code> can be filled with the <code>fill_null</code> method. You have to specify how you want the <code>fill_null</code> method to fill the missing data. The main ways to do this are filling with:</p> <ul> <li>a literal such as 0 or \"0\"</li> <li>a strategy such as filling forwards</li> <li>an expression such as replacing with values from another column</li> <li>interpolation</li> </ul> <p>We illustrate each way to fill nulls by defining a simple <code>DataFrame</code> with a missing value in <code>col2</code>:</p>  Python <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"col1\": [1, 2, 3],\n        \"col2\": [1, None, 3],\n    },\n)\nprint(df)\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col1 \u2506 col2 \u2502\n\u2502 ---  \u2506 ---  \u2502\n\u2502 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 1    \u2502\n\u2502 2    \u2506 null \u2502\n\u2502 3    \u2506 3    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/null/#fill-with-specified-literal-value","title":"Fill with specified literal value","text":"<p>We can fill the missing data with a specified literal value with <code>pl.lit</code>:</p>  Python <p> <code>fill_null</code> <pre><code>fill_literal_df = (\n    df.with_columns(\n        pl.col(\"col2\").fill_null(\n            pl.lit(2),\n        ),\n    ),\n)\nprint(fill_literal_df)\n</code></pre></p> <pre><code>(shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col1 \u2506 col2 \u2502\n\u2502 ---  \u2506 ---  \u2502\n\u2502 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 1    \u2502\n\u2502 2    \u2506 2    \u2502\n\u2502 3    \u2506 3    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518,)\n</code></pre>"},{"location":"user-guide/expressions/null/#fill-with-a-strategy","title":"Fill with a strategy","text":"<p>We can fill the missing data with a strategy such as filling forward:</p>  Python <p> <code>fill_null</code> <pre><code>fill_forward_df = df.with_columns(\n    pl.col(\"col2\").fill_null(strategy=\"forward\"),\n)\nprint(fill_forward_df)\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col1 \u2506 col2 \u2502\n\u2502 ---  \u2506 ---  \u2502\n\u2502 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 1    \u2502\n\u2502 2    \u2506 1    \u2502\n\u2502 3    \u2506 3    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You can find other fill strategies in the API docs.</p>"},{"location":"user-guide/expressions/null/#fill-with-an-expression","title":"Fill with an expression","text":"<p>For more flexibility we can fill the missing data with an expression. For example, to fill nulls with the median value from that column:</p>  Python <p> <code>fill_null</code> <pre><code>fill_median_df = df.with_columns(\n    pl.col(\"col2\").fill_null(pl.median(\"col2\")),\n)\nprint(fill_median_df)\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col1 \u2506 col2 \u2502\n\u2502 ---  \u2506 ---  \u2502\n\u2502 i64  \u2506 f64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 1.0  \u2502\n\u2502 2    \u2506 2.0  \u2502\n\u2502 3    \u2506 3.0  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In this case the column is cast from integer to float because the median is a float statistic.</p>"},{"location":"user-guide/expressions/null/#fill-with-interpolation","title":"Fill with interpolation","text":"<p>In addition, we can fill nulls with interpolation (without using the <code>fill_null</code> function):</p>  Python <p> <code>interpolate</code> <pre><code>fill_interpolation_df = df.with_columns(\n    pl.col(\"col2\").interpolate(),\n)\nprint(fill_interpolation_df)\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 col1 \u2506 col2 \u2502\n\u2502 ---  \u2506 ---  \u2502\n\u2502 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 1    \u2502\n\u2502 2    \u2506 2    \u2502\n\u2502 3    \u2506 3    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/null/#notanumber-or-nan-values","title":"<code>NotaNumber</code> or <code>NaN</code> values","text":"<p>Missing data in a <code>Series</code> has a <code>null</code> value. However, you can use <code>NotaNumber</code> or <code>NaN</code> values in columns with float datatypes. These <code>NaN</code> values can be created from Numpy's <code>np.nan</code> or the native python <code>float('nan')</code>:</p>  Python <p> <code>DataFrame</code> <pre><code>nan_df = pl.DataFrame(\n    {\n        \"value\": [1.0, np.NaN, float(\"nan\"), 3.0],\n    },\n)\nprint(nan_df)\n</code></pre></p> <pre><code>shape: (4, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 value \u2502\n\u2502 ---   \u2502\n\u2502 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1.0   \u2502\n\u2502 NaN   \u2502\n\u2502 NaN   \u2502\n\u2502 3.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Info</p> <p>In <code>Pandas</code> by default a <code>NaN</code> value in an integer column causes the column to be cast to float. This does not happen in <code>Polars</code> - instead an exception is raised.</p> <p><code>NaN</code> values are considered to be a type of floating point data and are not considered to be missing data in <code>Polars</code>. This means:</p> <ul> <li><code>NaN</code> values are not counted with the <code>null_count</code> method</li> <li><code>NaN</code> values are filled when you use <code>fill_nan</code> method but are not filled with the <code>fill_null</code> method</li> </ul> <p><code>Polars</code> has <code>is_nan</code> and <code>fill_nan</code> methods which work in a similar way to the <code>is_null</code> and <code>fill_null</code> methods. The underlying Arrow arrays do not have a pre-computed validity bitmask for <code>NaN</code> values so this has to be computed for the <code>is_nan</code> method.</p> <p>One further difference between <code>null</code> and <code>NaN</code> values is that taking the <code>mean</code> of a column with <code>null</code> values excludes the <code>null</code> values from the calculation but with <code>NaN</code> values taking the mean results in a <code>NaN</code>. This behaviour can be avoided by replacing the <code>NaN</code> values with <code>null</code> values;</p>  Python <p> <code>fill_nan</code> <pre><code>mean_nan_df = nan_df.with_columns(\n    pl.col(\"value\").fill_nan(None).alias(\"value\"),\n).mean()\nprint(mean_nan_df)\n</code></pre></p> <pre><code>shape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 value \u2502\n\u2502 ---   \u2502\n\u2502 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/numpy/","title":"Numpy","text":"<p><code>Polars</code> expressions support <code>NumPy</code> ufuncs. See here for a list on all supported numpy functions.</p> <p>This means that if a function is not provided by <code>Polars</code>, we can use <code>NumPy</code> and we still have fast columnar operation through the <code>NumPy</code> API.</p>"},{"location":"user-guide/expressions/numpy/#example","title":"Example","text":"Python <p> <code>DataFrame</code> \u00b7 <code>log</code> \u00b7  Available on feature numpy <pre><code>import polars as pl\nimport numpy as np\n\ndf = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n\nout = df.select(\n    [\n        np.log(pl.all()).suffix(\"_log\"),\n    ]\n)\nprint(out)\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a_log    \u2506 b_log    \u2502\n\u2502 ---      \u2506 ---      \u2502\n\u2502 f64      \u2506 f64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0.0      \u2506 1.386294 \u2502\n\u2502 0.693147 \u2506 1.609438 \u2502\n\u2502 1.098612 \u2506 1.791759 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/numpy/#interoperability","title":"Interoperability","text":"<p>Polars <code>Series</code> have support for NumPy universal functions (ufuncs). Element-wise functions such as <code>np.exp()</code>, <code>np.cos()</code>, <code>np.div()</code>, etc. all work with almost zero overhead.</p> <p>However, as a Polars-specific remark: missing values are a separate bitmask and are not visible by NumPy. This can lead to a window function or a <code>np.convolve()</code> giving flawed or incomplete results.</p> <p>Convert a Polars <code>Series</code> to a NumPy array with the <code>.to_numpy()</code> method. Missing values will be replaced by <code>np.nan</code> during the conversion. If the <code>Series</code> does not include missing values, or those values are not desired anymore, the <code>.view()</code> method can be used instead, providing a zero-copy NumPy array of the data.</p>"},{"location":"user-guide/expressions/operators/","title":"Basic Operators","text":"<p>This section describes how to use basic operators (e.g. addition, substraction) in conjunction with Expressions. We will provide various examples using different themes in the context of the following dataframe.</p> <p>Note</p> <p>In Rust and Python it is possible to use the operators directly (as in <code>+ - * / &lt; &gt;</code>) as the language allows operator overloading. For instance, the operator <code>+</code> translates to the <code>.add()</code> method. In NodeJS this is not possible and you must use the methods themselves, in python and rust you can choose which one you prefer.</p>  Python <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"nrs\": [1, 2, 3, None, 5],\n        \"names\": [\"foo\", \"ham\", \"spam\", \"egg\", None],\n        \"random\": np.random.rand(5),\n        \"groups\": [\"A\", \"A\", \"B\", \"C\", \"B\"],\n    }\n)\nprint(df)\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs  \u2506 names \u2506 random   \u2506 groups \u2502\n\u2502 ---  \u2506 ---   \u2506 ---      \u2506 ---    \u2502\n\u2502 i64  \u2506 str   \u2506 f64      \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1    \u2506 foo   \u2506 0.154163 \u2506 A      \u2502\n\u2502 2    \u2506 ham   \u2506 0.74005  \u2506 A      \u2502\n\u2502 3    \u2506 spam  \u2506 0.263315 \u2506 B      \u2502\n\u2502 null \u2506 egg   \u2506 0.533739 \u2506 C      \u2502\n\u2502 5    \u2506 null  \u2506 0.014575 \u2506 B      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/operators/#numerical","title":"Numerical","text":"Python <p> <code>operators</code> <pre><code>df_numerical = df.select(\n    [\n        (pl.col(\"nrs\") + 5).alias(\"nrs + 5\"),\n        (pl.col(\"nrs\") - 5).alias(\"nrs - 5\"),\n        (pl.col(\"nrs\") * pl.col(\"random\")).alias(\"nrs * random\"),\n        (pl.col(\"nrs\") / pl.col(\"random\")).alias(\"nrs / random\"),\n    ]\n)\nprint(df_numerical)\n</code></pre></p> <pre><code>shape: (5, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs + 5 \u2506 nrs - 5 \u2506 nrs * random \u2506 nrs / random \u2502\n\u2502 ---     \u2506 ---     \u2506 ---          \u2506 ---          \u2502\n\u2502 i64     \u2506 i64     \u2506 f64          \u2506 f64          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6       \u2506 -4      \u2506 0.154163     \u2506 6.486647     \u2502\n\u2502 7       \u2506 -3      \u2506 1.480099     \u2506 2.702521     \u2502\n\u2502 8       \u2506 -2      \u2506 0.789945     \u2506 11.393198    \u2502\n\u2502 null    \u2506 null    \u2506 null         \u2506 null         \u2502\n\u2502 10      \u2506 0       \u2506 0.072875     \u2506 343.054056   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/operators/#logical","title":"Logical","text":"Python <p> <code>operators</code> <pre><code>df_logical = df.select(\n    [\n        (pl.col(\"nrs\") &gt; 1).alias(\"nrs &gt; 1\"),\n        (pl.col(\"random\") &lt;= 0.5).alias(\"random &lt; .5\"),\n        (pl.col(\"nrs\") != 1).alias(\"nrs != 1\"),\n        (pl.col(\"nrs\") == 1).alias(\"nrs == 1\"),\n        ((pl.col(\"random\") &lt;= 0.5) &amp; (pl.col(\"nrs\") &gt; 1)).alias(\"and_expr\"),  # and\n        ((pl.col(\"random\") &lt;= 0.5) | (pl.col(\"nrs\") &gt; 1)).alias(\"or_expr\"),  # or\n    ]\n)\nprint(df_logical)\n</code></pre></p> <pre><code>shape: (5, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nrs &gt; 1 \u2506 random &lt; .5 \u2506 nrs != 1 \u2506 nrs == 1 \u2506 and_expr \u2506 or_expr \u2502\n\u2502 ---     \u2506 ---         \u2506 ---      \u2506 ---      \u2506 ---      \u2506 ---     \u2502\n\u2502 bool    \u2506 bool        \u2506 bool     \u2506 bool     \u2506 bool     \u2506 bool    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 false   \u2506 true        \u2506 false    \u2506 true     \u2506 false    \u2506 true    \u2502\n\u2502 true    \u2506 false       \u2506 true     \u2506 false    \u2506 false    \u2506 true    \u2502\n\u2502 true    \u2506 true        \u2506 true     \u2506 false    \u2506 true     \u2506 true    \u2502\n\u2502 null    \u2506 false       \u2506 true     \u2506 false    \u2506 false    \u2506 null    \u2502\n\u2502 true    \u2506 true        \u2506 true     \u2506 false    \u2506 true     \u2506 true    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/strings/","title":"Strings","text":"<p>The following section discusses operations performed on <code>Utf8</code> strings, which are a frequently used <code>DataType</code> when working with <code>DataFrames</code>. However, processing strings can often be inefficient due to their unpredictable memory size, causing the CPU to access many random memory locations. To address this issue, Polars utilizes <code>Arrow</code> as its backend, which stores all strings in a contiguous block of memory. As a result, string traversal is cache-optimal and predictable for the CPU.</p> <p>String processing functions are available in the <code>str</code> namespace.</p>"},{"location":"user-guide/expressions/strings/#accessing-the-string-namespace","title":"Accessing the string namespace","text":"<p>The <code>str</code> namespace can be accessed through the <code>.str</code> attribute of a column with <code>Utf8</code> data type. In the following example, we create a column named <code>animal</code> and compute the length of each element in the column in terms of the number of bytes and the number of characters. If you are working with ASCII text, then the results of these two computations will be the same, and using <code>lengths</code> is recommended since it is faster.</p>  Python <p> <code>lengths</code> \u00b7 <code>n_chars</code> <pre><code>df = pl.DataFrame({\"animal\": [\"Crab\", \"cat and dog\", \"rab$bit\", None]})\n\nout = df.select(\n    [\n        pl.col(\"animal\").str.lengths().alias(\"byte_count\"),\n        pl.col(\"animal\").str.n_chars().alias(\"letter_count\"),\n    ]\n)\nprint(out)\n</code></pre></p> <pre><code>shape: (4, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 byte_count \u2506 letter_count \u2502\n\u2502 ---        \u2506 ---          \u2502\n\u2502 u32        \u2506 u32          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 4          \u2506 4            \u2502\n\u2502 11         \u2506 11           \u2502\n\u2502 7          \u2506 7            \u2502\n\u2502 null       \u2506 null         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/strings/#string-parsing","title":"String Parsing","text":"<p><code>Polars</code> offers multiple methods for checking and parsing elements of a string. Firstly, we can use the <code>contains</code> method to check whether a given pattern exists within a substring. Subsequently, we can extract these patterns and replace them using other methods, which will be demonstrated in upcoming examples.</p>"},{"location":"user-guide/expressions/strings/#check-for-existence-of-a-pattern","title":"Check for existence of a pattern","text":"<p>To check for the presence of a pattern within a string, we can use the contains method. The <code>contains</code> method accepts either a regular substring or a regex pattern, depending on the value of the <code>literal</code> parameter. If the pattern we're searching for is a simple substring located either at the beginning or end of the string, we can alternatively use the <code>starts_with</code> and <code>ends_with</code> functions.</p>  Python <p> <code>contains</code> \u00b7 <code>starts_with</code> \u00b7 <code>ends_with</code> <pre><code>out = df.select(\n    [\n        pl.col(\"animal\"),\n        pl.col(\"animal\").str.contains(\"cat|bit\").alias(\"regex\"),\n        pl.col(\"animal\").str.contains(\"rab$\", literal=True).alias(\"literal\"),\n        pl.col(\"animal\").str.starts_with(\"rab\").alias(\"starts_with\"),\n        pl.col(\"animal\").str.ends_with(\"dog\").alias(\"ends_with\"),\n    ]\n)\nprint(out)\n</code></pre></p> <pre><code>shape: (4, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 animal      \u2506 regex \u2506 literal \u2506 starts_with \u2506 ends_with \u2502\n\u2502 ---         \u2506 ---   \u2506 ---     \u2506 ---         \u2506 ---       \u2502\n\u2502 str         \u2506 bool  \u2506 bool    \u2506 bool        \u2506 bool      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Crab        \u2506 false \u2506 false   \u2506 false       \u2506 false     \u2502\n\u2502 cat and dog \u2506 true  \u2506 false   \u2506 false       \u2506 true      \u2502\n\u2502 rab$bit     \u2506 true  \u2506 true    \u2506 true        \u2506 false     \u2502\n\u2502 null        \u2506 null  \u2506 null    \u2506 null        \u2506 null      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/strings/#extract-a-pattern","title":"Extract a pattern","text":"<p>The <code>extract</code> method allows us to extract a pattern from a specified string. This method takes a regex pattern containing one or more capture groups, which are defined by parentheses <code>()</code> in the pattern. The group index indicates which capture group to output.</p>  Python <p> <code>extract</code> <pre><code>df = pl.DataFrame(\n    {\n        \"a\": [\n            \"http://vote.com/ballon_dor?candidate=messi&amp;ref=polars\",\n            \"http://vote.com/ballon_dor?candidat=jorginho&amp;ref=polars\",\n            \"http://vote.com/ballon_dor?candidate=ronaldo&amp;ref=polars\",\n        ]\n    }\n)\nout = df.select(\n    [\n        pl.col(\"a\").str.extract(r\"candidate=(\\w+)\", group_index=1),\n    ]\n)\nprint(out)\n</code></pre></p> <pre><code>shape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a       \u2502\n\u2502 ---     \u2502\n\u2502 str     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 messi   \u2502\n\u2502 null    \u2502\n\u2502 ronaldo \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To extract all occurrences of a pattern within a string, we can use the <code>extract_all</code> method. In the example below, we extract all numbers from a string using the regex pattern <code>(\\d+)</code>, which matches one or more digits. The resulting output of the <code>extract_all</code> method is a list containing all instances of the matched pattern within the string.</p>  Python <p> <code>extract_all</code> <pre><code>df = pl.DataFrame({\"foo\": [\"123 bla 45 asd\", \"xyz 678 910t\"]})\nout = df.select(\n    [\n        pl.col(\"foo\").str.extract_all(r\"(\\d+)\").alias(\"extracted_nrs\"),\n    ]\n)\nprint(out)\n</code></pre></p> <pre><code>shape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 extracted_nrs  \u2502\n\u2502 ---            \u2502\n\u2502 list[str]      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 [\"123\", \"45\"]  \u2502\n\u2502 [\"678\", \"910\"] \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/strings/#replace-a-pattern","title":"Replace a pattern","text":"<p>We have discussed two methods for pattern matching and extraction thus far, and now we will explore how to replace a pattern within a string. Similar to <code>extract</code> and <code>extract_all</code>, Polars provides the <code>replace</code> and <code>replace_all</code> methods for this purpose. In the example below we replace one match of <code>abc</code> at the end of a word (<code>\\b</code>) by <code>ABC</code> and we replace all occurrence of <code>a</code> with <code>-</code>.</p>  Python <p> <code>replace</code> \u00b7 <code>replace_all</code> <pre><code>df = pl.DataFrame({\"id\": [1, 2], \"text\": [\"123abc\", \"abc456\"]})\nout = df.with_columns(\n    pl.col(\"text\").str.replace(r\"abc\\b\", \"ABC\"),\n    pl.col(\"text\").str.replace_all(\"a\", \"-\", literal=True).alias(\"text_replace_all\"),\n)\nprint(out)\n</code></pre></p> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 text   \u2506 text_replace_all \u2502\n\u2502 --- \u2506 ---    \u2506 ---              \u2502\n\u2502 i64 \u2506 str    \u2506 str              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 123ABC \u2506 123-bc           \u2502\n\u2502 2   \u2506 abc456 \u2506 -bc456           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/strings/#api-documentation","title":"API Documentation","text":"<p>In addition to the examples covered above, Polars offers various other string manipulation methods for tasks such as formatting, stripping, splitting, and more. To explore these additional methods, you can go to the API documentation of your chosen programming language for Polars.</p>"},{"location":"user-guide/expressions/user-defined-functions/","title":"User Defined functions","text":"<p>You should be convinced by now that polar expressions are so powerful and flexible that the need for custom python functions is much less needed than you might need in other libraries.</p> <p>Still, you need to have the power to be able to pass an expression's state to a third party library or apply your black box function over data in polars.</p> <p>For this we provide the following expressions:</p> <ul> <li><code>map</code></li> <li><code>apply</code></li> </ul>"},{"location":"user-guide/expressions/user-defined-functions/#to-map-or-to-apply","title":"To <code>map</code> or to <code>apply</code>.","text":"<p>These functions have an important distinction in how they operate and consequently what data they will pass to the user.</p> <p>A <code>map</code> passes the <code>Series</code> backed by the <code>expression</code> as is.</p> <p><code>map</code> follows the same rules in both the <code>select</code> and the <code>groupby</code> context, this will mean that the <code>Series</code> represents a column in a <code>DataFrame</code>. Note that in the <code>groupby</code> context, that column is not yet aggregated!</p> <p>Use cases for <code>map</code> are for instance passing the <code>Series</code> in an expression to a third party library. Below we show how we could use <code>map</code> to pass an expression column to a neural network model.</p>  Python Rust <p> <code>map</code> <pre><code>df.with_columns([\n    pl.col(\"features\").map(lambda s: MyNeuralNetwork.forward(s.to_numpy())).alias(\"activations\")\n])\n</code></pre></p> <pre><code>df.with_columns([\ncol(\"features\").map(|s| Ok(my_nn.forward(s))).alias(\"activations\")\n])\n</code></pre> <p>Use cases for <code>map</code> in the <code>groupby</code> context are slim. They are only used for performance reasons, but can quite easily lead to incorrect results. Let me explain why.</p>  Python Rust <p> <code>map</code> <pre><code>df = pl.DataFrame(\n    {\n        \"keys\": [\"a\", \"a\", \"b\"],\n        \"values\": [10, 7, 1],\n    }\n)\n\nout = df.groupby(\"keys\", maintain_order=True).agg(\n    [\n        pl.col(\"values\").map(lambda s: s.shift()).alias(\"shift_map\"),\n        pl.col(\"values\").shift().alias(\"shift_expression\"),\n    ]\n)\nprint(df)\n</code></pre></p> <p> <code>map</code> <pre><code>let df = df!(\n\"keys\" =&gt; &amp;[\"a\", \"a\", \"b\"],\n\"values\" =&gt; &amp;[10, 7, 1],\n)?;\n\nlet out = df\n.lazy()\n.groupby([\"keys\"])\n.agg([\ncol(\"values\")\n.map(|s| Ok(s.shift(1)), GetOutput::default())\n.alias(\"shift_map\"),\ncol(\"values\").shift(1).alias(\"shift_expression\"),\n])\n.collect()?;\n\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 keys \u2506 values \u2502\n\u2502 ---  \u2506 ---    \u2502\n\u2502 str  \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a    \u2506 10     \u2502\n\u2502 a    \u2506 7      \u2502\n\u2502 b    \u2506 1      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In the snippet above we groupby the <code>\"keys\"</code> column. That means we have the following groups:</p> <pre><code>\"a\" -&gt; [10, 7]\n\"b\" -&gt; [1]\n</code></pre> <p>If we would then apply a <code>shift</code> operation to the right, we'd expect:</p> <pre><code>\"a\" -&gt; [null, 10]\n\"b\" -&gt; [null]\n</code></pre> <p>Now, let's print and see what we've got.</p> <pre><code>print(out)\n</code></pre> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 keys \u2506 shift_map  \u2506 shift_expression \u2502\n\u2502 ---  \u2506 ---        \u2506 ---              \u2502\n\u2502 str  \u2506 list[i64]  \u2506 list[i64]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a    \u2506 [null, 10] \u2506 [null, 10]       \u2502\n\u2502 b    \u2506 [7]        \u2506 [null]           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Ouch.. we clearly get the wrong results here. Group <code>\"b\"</code> even got a value from group <code>\"a\"</code> \ud83d\ude35.</p> <p>This went horribly wrong, because the <code>map</code> applies the function before we aggregate! So that means the whole column <code>[10, 7, 1</code>] got shifted to <code>[null, 10, 7]</code> and was then aggregated.</p> <p>So my advice is to never use <code>map</code> in the <code>groupby</code> context unless you know you need it and know what you are doing.</p>"},{"location":"user-guide/expressions/user-defined-functions/#to-apply","title":"To <code>apply</code>","text":"<p>Luckily we can fix previous example with <code>apply</code>. <code>apply</code> works on the smallest logical elements for that operation.</p> <p>That is:</p> <ul> <li><code>select context</code> -&gt; single elements</li> <li><code>groupby context</code> -&gt; single groups</li> </ul> <p>So with <code>apply</code> we should be able to fix our example:</p>  Python Rust <p> <code>apply</code> <pre><code>out = df.groupby(\"keys\", maintain_order=True).agg(\n    [\n        pl.col(\"values\").apply(lambda s: s.shift()).alias(\"shift_map\"),\n        pl.col(\"values\").shift().alias(\"shift_expression\"),\n    ]\n)\nprint(out)\n</code></pre></p> <p> <code>apply</code> <pre><code>let out = df\n.clone()\n.lazy()\n.groupby([col(\"keys\")])\n.agg([\ncol(\"values\")\n.apply(|s| Ok(s.shift(1)), GetOutput::default())\n.alias(\"shift_map\"),\ncol(\"values\").shift(1).alias(\"shift_expression\"),\n])\n.collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 keys \u2506 shift_map  \u2506 shift_expression \u2502\n\u2502 ---  \u2506 ---        \u2506 ---              \u2502\n\u2502 str  \u2506 list[i64]  \u2506 list[i64]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a    \u2506 [null, 10] \u2506 [null, 10]       \u2502\n\u2502 b    \u2506 [null]     \u2506 [null]           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>And observe, a valid result! \ud83c\udf89</p>"},{"location":"user-guide/expressions/user-defined-functions/#apply-in-the-select-context","title":"<code>apply</code> in the <code>select</code> context","text":"<p>In the <code>select</code> context, the <code>apply</code> expression passes elements of the column to the python function.</p> <p>Note that you are now running python, this will be slow.</p> <p>Let's go through some examples to see what to expect. We will continue with the <code>DataFrame</code> we defined at the start of this section and show an example with the <code>apply</code> function and a counter example where we use the expression API to achieve the same goals.</p>"},{"location":"user-guide/expressions/user-defined-functions/#adding-a-counter","title":"Adding a counter","text":"<p>In this example we create a global <code>counter</code> and then add the integer <code>1</code> to the global state at every element processed. Every iteration the result of the increment will be added to the element value.</p> <p>Note, this example isn't provided in Rust.  The reason is that the global <code>counter</code> value would lead to data races when this apply is evaluated in parallel.  It would be possible to wrap it in a <code>Mutex</code> to protect the variable, but that would be obscuring the point of the example.  This is a case where the Python Global Interpreter Lock's performance tradeoff provides some safety guarantees.</p>  Python Rust <p> <code>apply</code> <pre><code>counter = 0\n\n\ndef add_counter(val: int) -&gt; int:\n    global counter\n    counter += 1\n    return counter + val\n\n\nout = df.select(\n    [\n        pl.col(\"values\").apply(add_counter).alias(\"solution_apply\"),\n        (pl.col(\"values\") + pl.arange(1, pl.count() + 1)).alias(\"solution_expr\"),\n    ]\n)\nprint(out)\n</code></pre></p> <p> <code>apply</code> <pre><code>\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 solution_apply \u2506 solution_expr \u2502\n\u2502 ---            \u2506 ---           \u2502\n\u2502 i64            \u2506 i64           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 11             \u2506 11            \u2502\n\u2502 9              \u2506 9             \u2502\n\u2502 4              \u2506 4             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/user-defined-functions/#combining-multiple-column-values","title":"Combining multiple column values","text":"<p>If we want to have access to values of different columns in a single <code>apply</code> function call, we can create <code>struct</code> data type. This data type collects those columns as fields in the <code>struct</code>. So if we'd create a struct from the columns <code>\"keys\"</code> and <code>\"values\"</code>, we would get the following struct elements:</p> <pre><code>[\n    {\"keys\": \"a\", \"values\": 10},\n    {\"keys\": \"a\", \"values\": 7},\n    {\"keys\": \"b\", \"values\": 1},\n]\n</code></pre> <p>In Python, those would be passed as <code>dict</code> to the calling python function and can thus be indexed by <code>field: str</code>.  In rust, you'll get a <code>Series</code> with the <code>Struct</code> type. The fields of the struct can then be indexed and downcast.</p>  Python Rust <p> <code>apply</code> \u00b7 <code>struct</code> <pre><code>out = df.select(\n    [\n        pl.struct([\"keys\", \"values\"])\n        .apply(lambda x: len(x[\"keys\"]) + x[\"values\"])\n        .alias(\"solution_apply\"),\n        (pl.col(\"keys\").str.lengths() + pl.col(\"values\")).alias(\"solution_expr\"),\n    ]\n)\nprint(out)\n</code></pre></p> <p> <code>apply</code> \u00b7 <code>Struct</code> \u00b7  Available on feature dtype-struct <pre><code>let out = df\n.lazy()\n.select([\n// pack to struct to get access to multiple fields in a custom `apply/map`\nas_struct(&amp;[col(\"keys\"), col(\"values\")])\n// we will compute the len(a) + b\n.apply(\n|s| {\n// downcast to struct\nlet ca = s.struct_()?;\n\n// get the fields as Series\nlet s_a = &amp;ca.fields()[0];\nlet s_b = &amp;ca.fields()[1];\n\n// downcast the `Series` to their known type\nlet ca_a = s_a.utf8()?;\nlet ca_b = s_b.i32()?;\n\n// iterate both `ChunkedArrays`\nlet out: Int32Chunked = ca_a\n.into_iter()\n.zip(ca_b)\n.map(|(opt_a, opt_b)| match (opt_a, opt_b) {\n(Some(a), Some(b)) =&gt; Some(a.len() as i32 + b),\n_ =&gt; None,\n})\n.collect();\n\nOk(out.into_series())\n},\nGetOutput::from_type(DataType::Int32),\n)\n.alias(\"solution_apply\"),\n(col(\"keys\").str().count_match(\".\") + col(\"values\")).alias(\"solution_expr\"),\n])\n.collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 solution_apply \u2506 solution_expr \u2502\n\u2502 ---            \u2506 ---           \u2502\n\u2502 i64            \u2506 i64           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 11             \u2506 11            \u2502\n\u2502 8              \u2506 8             \u2502\n\u2502 2              \u2506 2             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/user-defined-functions/#return-types","title":"Return types?","text":"<p>Custom python functions are black boxes for polars. We really don't know what kind of black arts you are doing, so we have to infer and try our best to understand what you meant.</p> <p>As a user it helps to understand what we do to better utilize custom functions.</p> <p>The data type is automatically inferred. We do that by waiting for the first non-null value. That value will then be used to determine the type of the <code>Series</code>.</p> <p>The mapping of python types to polars data types is as follows:</p> <ul> <li><code>int</code> -&gt; <code>Int64</code></li> <li><code>float</code> -&gt; <code>Float64</code></li> <li><code>bool</code> -&gt; <code>Boolean</code></li> <li><code>str</code> -&gt; <code>Utf8</code></li> <li><code>list[tp]</code> -&gt; <code>List[tp]</code> (where the inner type is inferred with the same rules)</li> <li><code>dict[str, [tp]]</code> -&gt; <code>struct</code></li> <li><code>Any</code> -&gt; <code>object</code> (Prevent this at all times)</li> </ul> <p>Rust types map as follows:</p> <ul> <li><code>i32</code> or <code>i64</code> -&gt; <code>Int64</code></li> <li><code>f32</code> or <code>f64</code> -&gt; <code>Float64</code></li> <li><code>bool</code> -&gt; <code>Boolean</code></li> <li><code>String</code> or <code>str</code> -&gt; <code>Utf8</code></li> <li><code>Vec&lt;tp&gt;</code> -&gt; <code>List[tp]</code> (where the inner type is inferred with the same rules)</li> </ul>"},{"location":"user-guide/expressions/window/","title":"Window functions","text":"<p>Window functions are expressions with superpowers. They allow you to perform aggregations on groups in the <code>select</code> context. Let's get a feel of what that means. First we create a dataset. The dataset loaded in the snippet below contains information about pokemon:</p>  Python Rust <p> <code>read_csv</code> <pre><code>import polars as pl\n\n# then let's load some csv data with information about pokemon\ndf = pl.read_csv(\n    \"https://gist.githubusercontent.com/ritchie46/cac6b337ea52281aa23c049250a4ff03/raw/89a957ff3919d90e6ef2d34235e6bf22304f3366/pokemon.csv\"\n)\nprint(df.head())\n</code></pre></p> <p> <code>CsvReader</code> \u00b7  Available on feature csv <pre><code>use polars::prelude::*;\nuse reqwest::blocking::Client;\n\nlet data: Vec&lt;u8&gt; = Client::new()\n.get(\"https://gist.githubusercontent.com/ritchie46/cac6b337ea52281aa23c049250a4ff03/raw/89a957ff3919d90e6ef2d34235e6bf22304f3366/pokemon.csv\")\n.send()?\n.text()?\n.bytes()\n.collect();\n\nlet df = CsvReader::new(std::io::Cursor::new(data))\n.has_header(true)\n.finish()?;\n\nprintln!(\"{}\", df);\n</code></pre></p> <pre><code>shape: (5, 13)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 #   \u2506 Name                  \u2506 Type 1 \u2506 Type 2 \u2506 \u2026 \u2506 Sp. Def \u2506 Speed \u2506 Generation \u2506 Legendary \u2502\n\u2502 --- \u2506 ---                   \u2506 ---    \u2506 ---    \u2506   \u2506 ---     \u2506 ---   \u2506 ---        \u2506 ---       \u2502\n\u2502 i64 \u2506 str                   \u2506 str    \u2506 str    \u2506   \u2506 i64     \u2506 i64   \u2506 i64        \u2506 bool      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 Bulbasaur             \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 65      \u2506 45    \u2506 1          \u2506 false     \u2502\n\u2502 2   \u2506 Ivysaur               \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 80      \u2506 60    \u2506 1          \u2506 false     \u2502\n\u2502 3   \u2506 Venusaur              \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 100     \u2506 80    \u2506 1          \u2506 false     \u2502\n\u2502 3   \u2506 VenusaurMega Venusaur \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 120     \u2506 80    \u2506 1          \u2506 false     \u2502\n\u2502 4   \u2506 Charmander            \u2506 Fire   \u2506 null   \u2506 \u2026 \u2506 50      \u2506 65    \u2506 1          \u2506 false     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/window/#groupby-aggregations-in-selection","title":"Groupby Aggregations in selection","text":"<p>Below we show how to use window functions to group over different columns and perform an aggregation on them. Doing so allows us to use multiple groupby operations in parallel, using a single query. The results of the aggregation are projected back to the original rows. Therefore, a window function will always lead to a <code>DataFrame</code> with the same size as the original.</p> <p>Note how we call <code>.over(\"Type 1\")</code> and <code>.over([\"Type 1\", \"Type 2\"])</code>. Using window functions we can aggregate over different groups in a single <code>select</code> call!  Note that, in Rust, the type of the argument to <code>over()</code> must be a collection, so even when you're only using one column, you must provided it in an array.</p> <p>The best part is, this won't cost you anything. The computed groups are cached and shared between different <code>window</code> expressions.</p>  Python Rust <p> <code>over</code> <pre><code>out = df.select(\n    [\n        \"Type 1\",\n        \"Type 2\",\n        pl.col(\"Attack\").mean().over(\"Type 1\").alias(\"avg_attack_by_type\"),\n        pl.col(\"Defense\")\n        .mean()\n        .over([\"Type 1\", \"Type 2\"])\n        .alias(\"avg_defense_by_type_combination\"),\n        pl.col(\"Attack\").mean().alias(\"avg_attack\"),\n    ]\n)\nprint(out)\n</code></pre></p> <p> <code>over</code> <pre><code>let out = df\n.clone()\n.lazy()\n.select([\ncol(\"Type 1\"),\ncol(\"Type 2\"),\ncol(\"Attack\")\n.mean()\n.over([\"Type 1\"])\n.alias(\"avg_attack_by_type\"),\ncol(\"Defense\")\n.mean()\n.over([\"Type 1\", \"Type 2\"])\n.alias(\"avg_defense_by_type_combination\"),\ncol(\"Attack\").mean().alias(\"avg_attack\"),\n])\n.collect()?;\n\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (163, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Type 1  \u2506 Type 2 \u2506 avg_attack_by_type \u2506 avg_defense_by_type_combination \u2506 avg_attack \u2502\n\u2502 ---     \u2506 ---    \u2506 ---                \u2506 ---                             \u2506 ---        \u2502\n\u2502 str     \u2506 str    \u2506 f64                \u2506 f64                             \u2506 f64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Grass   \u2506 Poison \u2506 72.923077          \u2506 67.8                            \u2506 75.349693  \u2502\n\u2502 Grass   \u2506 Poison \u2506 72.923077          \u2506 67.8                            \u2506 75.349693  \u2502\n\u2502 Grass   \u2506 Poison \u2506 72.923077          \u2506 67.8                            \u2506 75.349693  \u2502\n\u2502 Grass   \u2506 Poison \u2506 72.923077          \u2506 67.8                            \u2506 75.349693  \u2502\n\u2502 \u2026       \u2506 \u2026      \u2506 \u2026                  \u2506 \u2026                               \u2506 \u2026          \u2502\n\u2502 Dragon  \u2506 null   \u2506 94.0               \u2506 55.0                            \u2506 75.349693  \u2502\n\u2502 Dragon  \u2506 null   \u2506 94.0               \u2506 55.0                            \u2506 75.349693  \u2502\n\u2502 Dragon  \u2506 Flying \u2506 94.0               \u2506 95.0                            \u2506 75.349693  \u2502\n\u2502 Psychic \u2506 null   \u2506 53.875             \u2506 51.428571                       \u2506 75.349693  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/window/#operations-per-group","title":"Operations per group","text":"<p>Window functions can do more than aggregation. They can also be viewed as an operation within a group. If, for instance, you want to <code>sort</code> the values within a <code>group</code>, you can write <code>col(\"value\").sort().over(\"group\")</code> and voil\u00e0! We sorted by group!</p> <p>Let's filter out some rows to make this more clear.</p>  Python Rust <p> <code>filter</code> <pre><code>filtered = df.filter(pl.col(\"Type 2\") == \"Psychic\").select(\n    [\n        \"Name\",\n        \"Type 1\",\n        \"Speed\",\n    ]\n)\nprint(filtered)\n</code></pre></p> <p> <code>filter</code> <pre><code>let filtered = df\n.clone()\n.lazy()\n.filter(col(\"Type 2\").eq(lit(\"Psychic\")))\n.select([col(\"Name\"), col(\"Type 1\"), col(\"Speed\")])\n.collect()?;\n\nprintln!(\"{}\", filtered);\n</code></pre></p> <pre><code>shape: (7, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name                \u2506 Type 1 \u2506 Speed \u2502\n\u2502 ---                 \u2506 ---    \u2506 ---   \u2502\n\u2502 str                 \u2506 str    \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Slowpoke            \u2506 Water  \u2506 15    \u2502\n\u2502 Slowbro             \u2506 Water  \u2506 30    \u2502\n\u2502 SlowbroMega Slowbro \u2506 Water  \u2506 30    \u2502\n\u2502 Exeggcute           \u2506 Grass  \u2506 40    \u2502\n\u2502 Exeggutor           \u2506 Grass  \u2506 55    \u2502\n\u2502 Starmie             \u2506 Water  \u2506 115   \u2502\n\u2502 Jynx                \u2506 Ice    \u2506 95    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Observe that the group <code>Water</code> of column <code>Type 1</code> is not contiguous. There are two rows of <code>Grass</code> in between. Also note that each pokemon within a group are sorted by <code>Speed</code> in <code>ascending</code> order. Unfortunately, for this example we want them sorted in <code>descending</code> speed order. Luckily with window functions this is easy to accomplish.</p>  Python Rust <p> <code>over</code> <pre><code>out = filtered.with_columns(\n    [\n        pl.col([\"Name\", \"Speed\"]).sort_by(\"Speed\", descending=True).over(\"Type 1\"),\n    ]\n)\nprint(out)\n</code></pre></p> <p> <code>over</code> <pre><code>let out = filtered\n.lazy()\n.with_columns([cols([\"Name\", \"Speed\"]).sort_by([\"Speed\"],[true]).over([\"Type 1\"])])\n.collect()?;\nprintln!(\"{}\", out);\n</code></pre></p> <pre><code>shape: (7, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Name                \u2506 Type 1 \u2506 Speed \u2502\n\u2502 ---                 \u2506 ---    \u2506 ---   \u2502\n\u2502 str                 \u2506 str    \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Starmie             \u2506 Water  \u2506 115   \u2502\n\u2502 Slowbro             \u2506 Water  \u2506 30    \u2502\n\u2502 SlowbroMega Slowbro \u2506 Water  \u2506 30    \u2502\n\u2502 Exeggutor           \u2506 Grass  \u2506 55    \u2502\n\u2502 Exeggcute           \u2506 Grass  \u2506 40    \u2502\n\u2502 Slowpoke            \u2506 Water  \u2506 15    \u2502\n\u2502 Jynx                \u2506 Ice    \u2506 95    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p><code>Polars</code> keeps track of each group's location and maps the expressions to the proper row locations. This will also work over different groups in a single <code>select</code>.</p> <p>The power of window expressions is that you often don't need a <code>groupby -&gt; explode</code> combination, but you can put the logic in a single expression. It also makes the API cleaner. If properly used a:</p> <ul> <li><code>groupby</code> -&gt; marks that groups are aggregated and we expect a <code>DataFrame</code> of size <code>n_groups</code></li> <li><code>over</code> -&gt; marks that we want to compute something within a group, but doesn't modify the original size of the <code>DataFrame</code></li> </ul>"},{"location":"user-guide/expressions/window/#window-expression-rules","title":"Window expression rules","text":"<p>The evaluations of window expressions are as follows (assuming we apply it to a <code>pl.Int32</code> column):</p>  Python Rust <p> <code>over</code> \u00b7 <code>implode</code> <pre><code># aggregate and broadcast within a group\n# output type: -&gt; Int32\npl.sum(\"foo\").over(\"groups\")\n\n# sum within a group and multiply with group elements\n# output type: -&gt; Int32\n(pl.col(\"x\").sum() * pl.col(\"y\")).over(\"groups\")\n\n# sum within a group and multiply with group elements\n# and aggregate the group to a implode\n# output type: -&gt; List(Int32)\n(pl.col(\"x\").sum() * pl.col(\"y\")).implode().over(\"groups\")\n\n# note that it will require an explicit `implode()` call\n# sum within a group and multiply with group elements\n# and aggregate the group to a list\n# the flatten call explodes that list\n\n# This is the fastest method to do things over groups when the groups are sorted\n(pl.col(\"x\").sum() * pl.col(\"y\")).implode().over(\"groups\").flatten()\n</code></pre></p> <p> <code>over</code> \u00b7 <code>implode</code> <pre><code>// aggregate and broadcast within a group\n// output type: -&gt; i32\nsum(\"foo\").over([col(\"groups\")])\n// sum within a group and multiply with group elements\n// output type: -&gt; i32\n(col(\"x\").sum() * col(\"y\"))\n.over([col(\"groups\")])\n.alias(\"x1\")\n// sum within a group and multiply with group elements\n// and aggregate the group to a list\n// output type: -&gt; ChunkedArray&lt;i32&gt;\n(col(\"x\").sum() * col(\"y\"))\n.list()\n.over([col(\"groups\")])\n.alias(\"x2\")\n// note that it will require an explicit `list()` call\n// sum within a group and multiply with group elements\n// and aggregate the group to a list\n// the flatten call explodes that list\n\n// This is the fastest method to do things over groups when the groups are sorted\n(col(\"x\").sum() * col(\"y\"))\n.list()\n.over([col(\"groups\")])\n.flatten()\n.alias(\"x3\");\n</code></pre></p>"},{"location":"user-guide/expressions/window/#more-examples","title":"More examples","text":"<p>For more exercise, below are some window functions for us to compute:</p> <ul> <li>sort all pokemon by type</li> <li>select the first <code>3</code> pokemon per type as <code>\"Type 1\"</code></li> <li>sort the pokemon within a type by speed and select the first <code>3</code> as <code>\"fastest/group\"</code></li> <li>sort the pokemon within a type by attack and select the first <code>3</code> as <code>\"strongest/group\"</code></li> <li>sort the pokemon by name within a type and select the first <code>3</code> as <code>\"sorted_by_alphabet\"</code></li> </ul>  Python Rust <p> <code>over</code> \u00b7 <code>implode</code> <pre><code>out = df.sort(\"Type 1\").select(\n    [\n        pl.col(\"Type 1\").head(3).implode().over(\"Type 1\").flatten(),\n        pl.col(\"Name\")\n        .sort_by(pl.col(\"Speed\"))\n        .head(3)\n        .implode()\n        .over(\"Type 1\")\n        .flatten()\n        .alias(\"fastest/group\"),\n        pl.col(\"Name\")\n        .sort_by(pl.col(\"Attack\"))\n        .head(3)\n        .implode()\n        .over(\"Type 1\")\n        .flatten()\n        .alias(\"strongest/group\"),\n        pl.col(\"Name\")\n        .sort()\n        .head(3)\n        .implode()\n        .over(\"Type 1\")\n        .flatten()\n        .alias(\"sorted_by_alphabet\"),\n    ]\n)\nprint(out)\n</code></pre></p> <p> <code>over</code> \u00b7 <code>implode</code> <pre><code>let out = df\n.clone()\n.lazy()\n.select([\ncol(\"Type 1\")\n.head(Some(3))\n.list()\n.over([\"Type 1\"])\n.flatten(),\ncol(\"Name\")\n.sort_by([\"Speed\"], [false])\n.head(Some(3))\n.list()\n.over([\"Type 1\"])\n.flatten()\n.alias(\"fastest/group\"),\ncol(\"Name\")\n.sort_by([\"Attack\"], [false])\n.head(Some(3))\n.list()\n.over([\"Type 1\"])\n.flatten()\n.alias(\"strongest/group\"),\ncol(\"Name\")\n.sort(false)\n.head(Some(3))\n.list()\n.over([\"Type 1\"])\n.flatten()\n.alias(\"sorted_by_alphabet\"),\n])\n.collect()?;\nprintln!(\"{:?}\", out);\n</code></pre></p> <pre><code>shape: (43, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Type 1 \u2506 fastest/group       \u2506 strongest/group \u2506 sorted_by_alphabet      \u2502\n\u2502 ---    \u2506 ---                 \u2506 ---             \u2506 ---                     \u2502\n\u2502 str    \u2506 str                 \u2506 str             \u2506 str                     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Bug    \u2506 Paras               \u2506 Metapod         \u2506 Beedrill                \u2502\n\u2502 Bug    \u2506 Metapod             \u2506 Kakuna          \u2506 BeedrillMega Beedrill   \u2502\n\u2502 Bug    \u2506 Parasect            \u2506 Caterpie        \u2506 Butterfree              \u2502\n\u2502 Dragon \u2506 Dratini             \u2506 Dratini         \u2506 Dragonair               \u2502\n\u2502 \u2026      \u2506 \u2026                   \u2506 \u2026               \u2506 \u2026                       \u2502\n\u2502 Rock   \u2506 Omanyte             \u2506 Omastar         \u2506 Geodude                 \u2502\n\u2502 Water  \u2506 Slowpoke            \u2506 Magikarp        \u2506 Blastoise               \u2502\n\u2502 Water  \u2506 Slowbro             \u2506 Tentacool       \u2506 BlastoiseMega Blastoise \u2502\n\u2502 Water  \u2506 SlowbroMega Slowbro \u2506 Horsea          \u2506 Cloyster                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/expressions/window/#flattened-window-function","title":"Flattened window function","text":"<p>If we have a window function that aggregates to a <code>list</code> like the example above with the following Python expression:</p> <p><code>pl.col(\"Name\").sort_by(pl.col(\"Speed\")).head(3).implode().over(\"Type 1\")</code></p> <p>and in Rust:</p> <p><code>col(\"Name\").sort_by([\"Speed\"], [false]).head(Some(3)).implode().over([\"Type 1\"])</code></p> <p>This still works, but that would give us a column type <code>List</code> which might not be what we want (this would significantly increase our memory usage!).</p> <p>Instead we could <code>flatten</code>. This just turns our 2D list into a 1D array and projects that array/column back to our <code>DataFrame</code>. This is very fast because the reshape is often free, and adding the column back the the original <code>DataFrame</code> is also a lot cheaper (since we don't require a join like in a normal window function).</p> <p>However, for this operation to make sense, it is important that the columns used in <code>over([..])</code> are sorted!</p>"},{"location":"user-guide/io/aws/","title":"AWS","text":"<p> Under Construction  </p> <p>This section is still under development. Want to help out? Consider contributing and making a pull request to our repository. Please read our Contribution Guidelines on how to proceed.</p> <p>To read from or write to an AWS bucket, additional dependencies are needed in Rust:</p>  Rust <pre><code>$ cargo add aws_sdk_s3 aws_config tokio --features tokio/full\n</code></pre> <p>In the next few snippets we'll demonstrate interacting with a <code>Parquet</code> file located on an AWS bucket.</p>"},{"location":"user-guide/io/aws/#read","title":"Read","text":"<p>Load a <code>.parquet</code> file using:</p>  Python Rust <p> <code>from_arrow</code> \u00b7  Available on feature pyarrow \u00b7  Available on feature fsspec <pre><code>import polars as pl\nimport pyarrow.parquet as pq\nimport s3fs\n\nfs = s3fs.S3FileSystem()\nbucket = \"&lt;YOUR_BUCKET&gt;\"\npath = \"&lt;YOUR_PATH&gt;\"\n\ndataset = pq.ParquetDataset(f\"s3://{bucket}/{path}\", filesystem=fs)\ndf = pl.from_arrow(dataset.read())\n</code></pre></p> <pre><code>use aws_sdk_s3::Region;\n\nuse aws_config::meta::region::RegionProviderChain;\nuse aws_sdk_s3::Client;\nuse std::borrow::Cow;\n\nuse polars::prelude::*;\n\n#[tokio::main]\nasync fn main() {\nlet bucket = \"&lt;YOUR_BUCKET&gt;\";\nlet path = \"&lt;YOUR_PATH&gt;\";\n\nlet config = aws_config::from_env().load().await;\nlet client = Client::new(&amp;config);\n\nlet req = client.get_object().bucket(bucket).key(path);\n\nlet res = req.clone().send().await.unwrap();\nlet bytes = res.body.collect().await.unwrap();\nlet bytes = bytes.into_bytes();\n\nlet cursor = std::io::Cursor::new(bytes);\n\nlet df = CsvReader::new(cursor).finish().unwrap();\n\nprintln!(\"{:?}\", df);\n}\n</code></pre>"},{"location":"user-guide/io/bigquery/","title":"Google BigQuery","text":"<p>To read or write from GBQ, additional dependencies are needed:</p>  Python <pre><code>$ pip install google-cloud-bigquery\n</code></pre>"},{"location":"user-guide/io/bigquery/#read","title":"Read","text":"<p>We can load a query into a <code>DataFrame</code> like this:</p>  Python <p> <code>from_arrow</code> \u00b7  Available on feature pyarrow \u00b7  Available on feature fsspec <pre><code>import polars as pl\nfrom google.cloud import bigquery\n\nclient = bigquery.Client()\n\n# Perform a query.\nQUERY = (\n    'SELECT name FROM `bigquery-public-data.usa_names.usa_1910_2013` '\n    'WHERE state = \"TX\" '\n    'LIMIT 100')\nquery_job = client.query(QUERY)  # API request\nrows = query_job.result()  # Waits for query to finish\n\ndf = pl.from_arrow(rows.to_arrow())\n</code></pre></p>"},{"location":"user-guide/io/bigquery/#write","title":"Write","text":"<p> Under Construction  </p> <p>This section is still under development. Want to help out? Consider contributing and making a pull request to our repository. Please read our Contribution Guidelines on how to proceed.</p>"},{"location":"user-guide/io/csv/","title":"CSV","text":""},{"location":"user-guide/io/csv/#read-write","title":"Read &amp; Write","text":"<p>Reading a CSV file should look familiar:</p>  Python Rust NodeJS <p> <code>read_csv</code> <pre><code>df = pl.read_csv(\"path.csv\")\n</code></pre></p> <p> <code>CsvReader</code> \u00b7  Available on feature csv <pre><code>use polars::prelude::*;\n\nlet df = CsvReader::from_path(\"path.csv\").unwrap().finish().unwrap();\n</code></pre></p> <p> <code>readCSV</code> <pre><code>df = pl.readCSV(\"path.csv\")\n</code></pre></p> <p>Writing a CSV file is similar with the <code>write_csv</code> function:</p>  Python Rust NodeJS <p> <code>write_csv</code> <pre><code>df = pl.DataFrame({\"foo\": [1, 2, 3], \"bar\": [None, \"bak\", \"baz\"]})\ndf.write_csv(\"path.csv\")\n</code></pre></p> <p> <code>CsvWriter</code> \u00b7  Available on feature csv <pre><code>let mut df = df!(\n\"foo\" =&gt; &amp;[1, 2, 3],\n\"bar\" =&gt; &amp;[None, Some(\"bak\"), Some(\"baz\")],\n)\n.unwrap();\n\nlet mut file = std::fs::File::create(\"path.csv\").unwrap();\nCsvWriter::new(&amp;mut file).finish(&amp;mut df).unwrap();\n</code></pre></p> <p> <code>writeCSV</code> <pre><code>df = pl.DataFrame({ foo: [1, 2, 3], bar: [null, \"bak\", \"baz\"] });\ndf.writeCSV(\"path.csv\");\n</code></pre></p>"},{"location":"user-guide/io/csv/#scan","title":"Scan","text":"<p><code>Polars</code> allows you to scan a CSV input. Scanning delays the actual parsing of the file and instead returns a lazy computation holder called a <code>LazyFrame</code>.</p>  Python Rust NodeJS <p> <code>scan_csv</code> <pre><code>df = pl.scan_csv(\"path.csv\")\n</code></pre></p> <p> <code>LazyCsvReader</code> \u00b7  Available on feature csv <pre><code>let df = LazyCsvReader::new(\"./test.csv\").finish().unwrap();\n</code></pre></p> <p> <code>scanCSV</code> <pre><code>df = pl.scanCSV(\"path.csv\");\n</code></pre></p> <p>If you want to know why this is desirable, you can read more about those <code>Polars</code> optimizations here.</p>"},{"location":"user-guide/io/database/","title":"Databases","text":""},{"location":"user-guide/io/database/#read-from-a-database","title":"Read from a database","text":"<p>We can read from a database with Polars using the <code>pl.read_database</code> function. To use this function you need an SQL query string and a connection string called a <code>connection_uri</code>.</p> <p>For example, the following snippet shows the general patterns for reading all columns from the <code>foo</code> table in a Postgres database:</p>  Python <p> <code>read_database</code> \u00b7  Available on feature connectorx <pre><code>import polars as pl\n\nconnection_uri = \"postgres://username:password@server:port/database\"\nquery = \"SELECT * FROM foo\"\n\npl.read_database(query=query, connection_uri=connection_uri)\n</code></pre></p>"},{"location":"user-guide/io/database/#engines","title":"Engines","text":"<p>Polars doesn't manage connections and data transfer from databases by itself. Instead external libraries (known as engines) handle this. At present Polars can use two engines to read from databases: </p> <ul> <li>ConnectorX and </li> <li>ADBC</li> </ul>"},{"location":"user-guide/io/database/#connectorx","title":"ConnectorX","text":"<p>ConnectorX is the default engine and supports numerous databases including Postgres, Mysql, SQL Server and Redshift. ConnectorX is written in Rust and stores data in Arrow format to allow for zero-copy to Polars.</p> <p>To read from one of the supported databases with <code>ConnectorX</code> you need to activate the additional dependancy <code>ConnectorX</code> when installing Polars or install it manually with</p> <pre><code>$  pip install connectorx\n</code></pre>"},{"location":"user-guide/io/database/#adbc","title":"ADBC","text":"<p>ADBC (Arrow Database Connectivity) is an engine supported by the Apache Arrow project. ADBC aims to be both an API standard for connecting to databases and libraries implementing this standard in a range of languages.</p> <p>It is still early days for ADBC so support for different databases is still limited. At present drivers for ADBC are only available for Postgres and SQLite. To install ADBC you need to install the driver for your database. For example to install the driver for SQLite you run</p> <pre><code>$  pip install adbc-driver-sqlite\n</code></pre> <p>As ADBC is not the default engine you must specify the engine as an argument to <code>pl.read_database</code></p>  Python <p> <code>read_database</code> <pre><code>connection_uri = \"postgres://username:password@server:port/database\"\nquery = \"SELECT * FROM foo\"\n\npl.read_database(query=query, connection_uri=connection_uri, engine=\"adbc\")\n</code></pre></p>"},{"location":"user-guide/io/database/#write-to-a-database","title":"Write to a database","text":"<p>We can write to a database with Polars using the <code>pl.write_database</code> function. </p>"},{"location":"user-guide/io/database/#engines_1","title":"Engines","text":"<p>As with reading from a database above Polars uses an engine to write to a database. The currently supported engines are:</p> <ul> <li>SQLAlchemy and</li> <li>Arrow Database Connectivity (ADBC)</li> </ul>"},{"location":"user-guide/io/database/#sqlalchemy","title":"SQLAlchemy","text":"<p>With the default engine SQLAlchemy you can write to any database supported by SQLAlchemy. To use this engine you need to install SQLAlchemy and Pandas <pre><code>$  pip install SQLAlchemy pandas\n</code></pre> In this example, we write the <code>DataFrame</code> to a table called <code>records</code> in the database</p>  Python <p> <code>write_database</code> <pre><code>connection_uri = \"postgres://username:password@server:port/database\"\ndf = pl.DataFrame({\"foo\": [1, 2, 3]})\n\ndf.write_database(table_name=\"records\", connection_connection_uri=connection_uri)\n</code></pre></p> <p>In the SQLAlchemy approach Polars converts the <code>DataFrame</code> to a Pandas <code>DataFrame</code> backed by PyArrow and then uses SQLAlchemy methods on a Pandas <code>DataFrame</code> to write to the database. </p>"},{"location":"user-guide/io/database/#adbc_1","title":"ADBC","text":"<p>As with reading from a database you can also use ADBC to write to a SQLite or Posgres database. As shown above you need to install the appropriate ADBC driver for your database.</p>  Python <p> <code>write_database</code> <pre><code>connection_uri = \"postgres://username:password@server:port/database\"\ndf = pl.DataFrame({\"foo\": [1, 2, 3]})\n\ndf.write_database(table_name=\"records\", connection_uri=connection_uri, engine=\"adbc\")\n</code></pre></p>"},{"location":"user-guide/io/json_file/","title":"JSON files","text":""},{"location":"user-guide/io/json_file/#read-write","title":"Read &amp; Write","text":""},{"location":"user-guide/io/json_file/#json","title":"JSON","text":"<p>Reading a JSON file should look familiar:</p>  Python Rust <p> <code>read_json</code> <pre><code>df = pl.read_json(\"path.json\")\n</code></pre></p> <p> <code>JsonReader</code> \u00b7  Available on feature json <pre><code>use polars::prelude::*;\n\nlet mut file = std::fs::File::open(\"path.json\").unwrap();\nlet df = JsonReader::new(&amp;mut file).finish().unwrap();\n</code></pre></p>"},{"location":"user-guide/io/json_file/#newline-delimited-json","title":"Newline Delimited JSON","text":"<p>JSON objects that are delimited by newlines can be read into polars in a much more performant way than standard json.</p>  Python Rust <p> <code>read_ndjson</code> <pre><code>df = pl.read_ndjson(\"path.json\")\n</code></pre></p> <p> <code>JsonLineReader</code> \u00b7  Available on feature json <pre><code>let mut file = std::fs::File::open(\"path.json\").unwrap();\nlet df = JsonLineReader::new(&amp;mut file).finish().unwrap();\n</code></pre></p>"},{"location":"user-guide/io/json_file/#write","title":"Write","text":"Python Rust <p> <code>write_json</code> \u00b7 <code>write_ndjson</code> <pre><code>df = pl.DataFrame({\"foo\": [1, 2, 3], \"bar\": [None, \"bak\", \"baz\"]})\n# json\ndf.write_json(\"path.json\")\n# ndjson\ndf.write_ndjson(\"path.json\")\n</code></pre></p> <p> <code>JsonWriter</code> \u00b7 <code>JsonWriter</code> \u00b7  Available on feature json <pre><code>let mut df = df!(\n\"foo\" =&gt; &amp;[1, 2, 3],\n\"bar\" =&gt; &amp;[None, Some(\"bak\"), Some(\"baz\")],\n)\n.unwrap();\n\nlet mut file = std::fs::File::create(\"path.json\").unwrap();\n\n// json\nJsonWriter::new(&amp;mut file)\n.with_json_format(JsonFormat::Json)\n.finish(&amp;mut df)\n.unwrap();\n\n// ndjson\nJsonWriter::new(&amp;mut file)\n.with_json_format(JsonFormat::JsonLines)\n.finish(&amp;mut df)\n.unwrap();\n</code></pre></p>"},{"location":"user-guide/io/json_file/#scan","title":"Scan","text":"<p><code>Polars</code> allows you to scan a JSON input only for newline delimited json. Scanning delays the actual parsing of the file and instead returns a lazy computation holder called a <code>LazyFrame</code>.</p>  Python Rust <p> <code>scan_ndjson</code> <pre><code>df = pl.scan_ndjson(\"path.json\")\n</code></pre></p> <p> <code>LazyJsonLineReader</code> \u00b7  Available on feature json <pre><code>let df = LazyJsonLineReader::new(\"path.json\".to_string()).finish().unwrap();\n</code></pre></p>"},{"location":"user-guide/io/multiple/","title":"Multiple","text":""},{"location":"user-guide/io/multiple/#dealing-with-multiple-files","title":"Dealing with multiple files.","text":"<p><code>Polars</code> can deal with multiple files differently depending on your needs and memory strain.</p> <p>Let's create some files to give use some context:</p>  Python <p> <code>write_csv</code> <pre><code>import polars as pl\n\ndf = pl.DataFrame({\"foo\": [1, 2, 3], \"bar\": [None, \"ham\", \"spam\"]})\n\nfor i in range(5):\n    df.write_csv(f\"my_many_files_{i}.csv\")\n</code></pre></p>"},{"location":"user-guide/io/multiple/#reading-into-a-single-dataframe","title":"Reading into a single <code>DataFrame</code>","text":"<p>To read multiple files into a single <code>DataFrame</code>, we can use globbing patterns:</p>  Python <p> <code>read_csv</code> <pre><code>df = pl.read_csv(\"my_many_files_*.csv\")\nprint(df)\n</code></pre></p> <pre><code>shape: (15, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 bar  \u2502\n\u2502 --- \u2506 ---  \u2502\n\u2502 i64 \u2506 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 null \u2502\n\u2502 2   \u2506 ham  \u2502\n\u2502 3   \u2506 spam \u2502\n\u2502 1   \u2506 null \u2502\n\u2502 \u2026   \u2506 \u2026    \u2502\n\u2502 3   \u2506 spam \u2502\n\u2502 1   \u2506 null \u2502\n\u2502 2   \u2506 ham  \u2502\n\u2502 3   \u2506 spam \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To see how this works we can take a look at the query plan. Below we see that all files are read separately and concatenated into a single <code>DataFrame</code>. <code>Polars</code> will try to parallelize the reading.</p>  Python <p> <code>show_graph</code> <pre><code>pl.scan_csv(\"my_many_files_*.csv\").show_graph()\n</code></pre></p> <p></p>"},{"location":"user-guide/io/multiple/#reading-and-processing-in-parallel","title":"Reading and processing in parallel","text":"<p>If your files don't have to be in a single table you can also build a query plan for each file and execute them in parallel on the <code>Polars</code> thread pool.</p> <p>All query plan execution is embarrassingly parallel and doesn't require any communication.</p>  Python <p> <code>scan_csv</code> <pre><code>import polars as pl\nimport glob\n\nqueries = []\nfor file in glob.glob(\"my_many_files_*.csv\"):\n    q = pl.scan_csv(file).groupby(\"bar\").agg([pl.count(), pl.sum(\"foo\")])\n    queries.append(q)\n\ndataframes = pl.collect_all(queries)\nprint(dataframes)\n</code></pre></p> <pre><code>[shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bar  \u2506 count \u2506 foo \u2502\n\u2502 ---  \u2506 ---   \u2506 --- \u2502\n\u2502 str  \u2506 u32   \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 spam \u2506 1     \u2506 3   \u2502\n\u2502 ham  \u2506 1     \u2506 2   \u2502\n\u2502 null \u2506 1     \u2506 1   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518, shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bar  \u2506 count \u2506 foo \u2502\n\u2502 ---  \u2506 ---   \u2506 --- \u2502\n\u2502 str  \u2506 u32   \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 null \u2506 1     \u2506 1   \u2502\n\u2502 spam \u2506 1     \u2506 3   \u2502\n\u2502 ham  \u2506 1     \u2506 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518, shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bar  \u2506 count \u2506 foo \u2502\n\u2502 ---  \u2506 ---   \u2506 --- \u2502\n\u2502 str  \u2506 u32   \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 null \u2506 1     \u2506 1   \u2502\n\u2502 ham  \u2506 1     \u2506 2   \u2502\n\u2502 spam \u2506 1     \u2506 3   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518, shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bar  \u2506 count \u2506 foo \u2502\n\u2502 ---  \u2506 ---   \u2506 --- \u2502\n\u2502 str  \u2506 u32   \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 null \u2506 1     \u2506 1   \u2502\n\u2502 spam \u2506 1     \u2506 3   \u2502\n\u2502 ham  \u2506 1     \u2506 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518, shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 bar  \u2506 count \u2506 foo \u2502\n\u2502 ---  \u2506 ---   \u2506 --- \u2502\n\u2502 str  \u2506 u32   \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 spam \u2506 1     \u2506 3   \u2502\n\u2502 null \u2506 1     \u2506 1   \u2502\n\u2502 ham  \u2506 1     \u2506 2   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518]\n</code></pre>"},{"location":"user-guide/io/parquet/","title":"Parquet","text":"<p>Loading or writing <code>Parquet</code> files is lightning fast. <code>Pandas</code> uses <code>PyArrow</code> -<code>Python</code> bindings exposed by <code>Arrow</code>- to load <code>Parquet</code> files into memory, but it has to copy that data into <code>Pandas</code> memory. With <code>Polars</code> there is no extra cost due to copying as we read <code>Parquet</code> directly into <code>Arrow</code> memory and keep it there.</p>"},{"location":"user-guide/io/parquet/#read","title":"Read","text":"Python Rust NodeJS <p> <code>read_parquet</code> <pre><code>df = pl.read_parquet(\"path.parquet\")\n</code></pre></p> <p> <code>ParquetReader</code> \u00b7  Available on feature parquet <pre><code>let mut file = std::fs::File::open(\"path.parquet\").unwrap();\n\nlet df = ParquetReader::new(&amp;mut file).finish().unwrap();\n</code></pre></p> <p> <code>readParquet</code> <pre><code>df = pl.readParquet(\"path.parquet\")\n</code></pre></p>"},{"location":"user-guide/io/parquet/#write","title":"Write","text":"Python Rust NodeJS <p> <code>write_parquet</code> <pre><code>df = pl.DataFrame({\"foo\": [1, 2, 3], \"bar\": [None, \"bak\", \"baz\"]})\ndf.write_parquet(\"path.parquet\")\n</code></pre></p> <p> <code>ParquetWriter</code> \u00b7  Available on feature parquet <pre><code>let mut df = df!(\n\"foo\" =&gt; &amp;[1, 2, 3],\n\"bar\" =&gt; &amp;[None, Some(\"bak\"), Some(\"baz\")],\n)\n.unwrap();\n\nlet mut file = std::fs::File::create(\"path.parquet\").unwrap();\nParquetWriter::new(&amp;mut file).finish(&amp;mut df).unwrap();\n</code></pre></p> <p> <code>writeParquet</code> <pre><code>df = pl.DataFrame({ foo: [1, 2, 3], bar: [null, \"bak\", \"baz\"] });\ndf.writeParquet(\"path.parquet\");\n</code></pre></p>"},{"location":"user-guide/io/parquet/#scan","title":"Scan","text":"<p><code>Polars</code> allows you to scan a <code>Parquet</code> input. Scanning delays the actual parsing of the file and instead returns a lazy computation holder called a <code>LazyFrame</code>.</p>  Python Rust NodeJS <p> <code>scan_parquet</code> <pre><code>df = pl.scan_parquet(\"path.parquet\")\n</code></pre></p> <p> <code>scan_parquet</code> \u00b7  Available on feature parquet <pre><code>let args = ScanArgsParquet::default();\nlet df = LazyFrame::scan_parquet(\"./file.parquet\",args).unwrap();\n</code></pre></p> <pre><code>df = pl.scanParquet(\"path.parquet\");\n</code></pre> <p>If you want to know why this is desirable, you can read more about those <code>Polars</code> optimizations here.</p>"},{"location":"user-guide/lazy/execution/","title":"Query execution","text":"<p>Our example query on the Reddit dataset is:</p>  Python <p> <code>scan_csv</code> <pre><code>q1 = (\n    pl.scan_csv(\"docs/src/data/reddit.csv\")\n    .with_columns(pl.col(\"name\").str.to_uppercase())\n    .filter(pl.col(\"comment_karma\") &gt; 0)\n)\n</code></pre></p> <p>If we were to run the code above on the Reddit CSV the query would not be evaluated. Instead Polars takes each line of code, adds it to the internal query graph and optimizes the query graph.</p> <p>When we execute the code Polars executes the optimized query graph by default.</p>"},{"location":"user-guide/lazy/execution/#execution-on-the-full-dataset","title":"Execution on the full dataset","text":"<p>We can execute our query on the full dataset by calling the <code>.collect</code> method on the query.</p>  Python <p> <code>scan_csv</code> \u00b7 <code>collect</code> <pre><code>q4 = (\n    pl.scan_csv(f\"docs/data/reddit.csv\")\n    .with_columns(pl.col(\"name\").str.to_uppercase())\n    .filter(pl.col(\"comment_karma\") &gt; 0)\n    .collect()\n)\n</code></pre></p> <pre><code>shape: (14_029, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id      \u2506 name                      \u2506 created_utc \u2506 updated_on \u2506 comment_karma \u2506 link_karma \u2502\n\u2502 ---     \u2506 ---                       \u2506 ---         \u2506 ---        \u2506 ---           \u2506 ---        \u2502\n\u2502 i64     \u2506 str                       \u2506 i64         \u2506 i64        \u2506 i64           \u2506 i64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6       \u2506 TAOJIANLONG_JASONBROKEN   \u2506 1397113510  \u2506 1536527864 \u2506 4             \u2506 0          \u2502\n\u2502 17      \u2506 SSAIG_JASONBROKEN         \u2506 1397113544  \u2506 1536527864 \u2506 1             \u2506 0          \u2502\n\u2502 19      \u2506 FDBVFDSSDGFDS_JASONBROKEN \u2506 1397113552  \u2506 1536527864 \u2506 3             \u2506 0          \u2502\n\u2502 37      \u2506 IHATEWHOWEARE_JASONBROKEN \u2506 1397113636  \u2506 1536527864 \u2506 61            \u2506 0          \u2502\n\u2502 \u2026       \u2506 \u2026                         \u2506 \u2026           \u2506 \u2026          \u2506 \u2026             \u2506 \u2026          \u2502\n\u2502 1229384 \u2506 DSFOX                     \u2506 1163177415  \u2506 1536497412 \u2506 44411         \u2506 7917       \u2502\n\u2502 1229459 \u2506 NEOCARTY                  \u2506 1163177859  \u2506 1536533090 \u2506 40            \u2506 0          \u2502\n\u2502 1229587 \u2506 TEHSMA                    \u2506 1163178847  \u2506 1536497412 \u2506 14794         \u2506 5707       \u2502\n\u2502 1229621 \u2506 JEREMYLOW                 \u2506 1163179075  \u2506 1536497412 \u2506 411           \u2506 1063       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Above we see that from the 10 Million rows there 14,029 rows match our predicate.</p> <p>With the default <code>collect</code> method Polars processes all of your data as one batch. This means that all the data has to fit into your available memory at the point of peak memory usage in your query.</p>"},{"location":"user-guide/lazy/execution/#execution-on-larger-than-memory-data","title":"Execution on larger-than-memory data","text":"<p>If your data requires more memory than you have available Polars may be able to process the data in batches using streaming mode. To use streaming mode you simply pass the <code>streaming=True</code> argument to <code>collect</code></p>  Python <p> <code>scan_csv</code> \u00b7 <code>collect</code> <pre><code>q5 = (\n    pl.scan_csv(f\"docs/data/reddit.csv\")\n    .with_columns(pl.col(\"name\").str.to_uppercase())\n    .filter(pl.col(\"comment_karma\") &gt; 0)\n    .collect(streaming=True)\n)\n</code></pre></p> <p>We look at streaming in more detail here.</p>"},{"location":"user-guide/lazy/execution/#execution-on-a-partial-dataset","title":"Execution on a partial dataset","text":"<p>While you're writing, optimizing or checking your query on a large dataset, querying all available data may lead to a slow development process.</p> <p>You can instead execute the query with the <code>.fetch</code> method. The <code>.fetch</code> method takes a parameter <code>n_rows</code> and tries to 'fetch' that number of rows at the data source. The number of rows cannot be guaranteed, however, as the lazy API does not count how many rows there are at each stage of the query.</p> <p>Here we \"fetch\" 100 rows from the source file and apply the predicates.</p>  Python <p> <code>scan_csv</code> \u00b7 <code>collect</code> \u00b7 <code>fetch</code> <pre><code>q9 = (\n    pl.scan_csv(f\"docs/data/reddit.csv\")\n    .with_columns(pl.col(\"name\").str.to_uppercase())\n    .filter(pl.col(\"comment_karma\") &gt; 0)\n    .fetch(n_rows=int(100))\n)\n</code></pre></p> <pre><code>shape: (27, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id    \u2506 name                      \u2506 created_utc \u2506 updated_on \u2506 comment_karma \u2506 link_karma \u2502\n\u2502 ---   \u2506 ---                       \u2506 ---         \u2506 ---        \u2506 ---           \u2506 ---        \u2502\n\u2502 i64   \u2506 str                       \u2506 i64         \u2506 i64        \u2506 i64           \u2506 i64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6     \u2506 TAOJIANLONG_JASONBROKEN   \u2506 1397113510  \u2506 1536527864 \u2506 4             \u2506 0          \u2502\n\u2502 17    \u2506 SSAIG_JASONBROKEN         \u2506 1397113544  \u2506 1536527864 \u2506 1             \u2506 0          \u2502\n\u2502 19    \u2506 FDBVFDSSDGFDS_JASONBROKEN \u2506 1397113552  \u2506 1536527864 \u2506 3             \u2506 0          \u2502\n\u2502 37    \u2506 IHATEWHOWEARE_JASONBROKEN \u2506 1397113636  \u2506 1536527864 \u2506 61            \u2506 0          \u2502\n\u2502 \u2026     \u2506 \u2026                         \u2506 \u2026           \u2506 \u2026          \u2506 \u2026             \u2506 \u2026          \u2502\n\u2502 77763 \u2506 LUNCHY                    \u2506 1137599510  \u2506 1536528275 \u2506 65            \u2506 0          \u2502\n\u2502 77765 \u2506 COMPOSTELLAS              \u2506 1137474000  \u2506 1536528276 \u2506 6             \u2506 0          \u2502\n\u2502 77766 \u2506 GENERICBOB                \u2506 1137474000  \u2506 1536528276 \u2506 291           \u2506 14         \u2502\n\u2502 77768 \u2506 TINHEADNED                \u2506 1139665457  \u2506 1536497404 \u2506 4434          \u2506 103        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/lazy/optimizations/","title":"Optimizations","text":"<p>If you use <code>Polars</code>' lazy API, <code>Polars</code> will run several optimizations on your query. Some of them are executed up front, others are determined just in time as the materialized data comes in.</p> <p>Here is a non-complete overview of optimizations done by polars, what they do and how often they run.</p> Optimization Explanation runs Predicate pushdown Applies filters as early as possible/ at scan level. 1 time Projection pushdown Select only the columns that are needed at the scan level. 1 time Slice pushdown Only load the required slice from the scan level.  Don't materialize sliced outputs (e.g. join.head(10)). 1 time Common subplan elimination Cache subtrees/file scans that are used by multiple subtrees in the query plan. 1 time Simplify expressions Various optimizations, such as constant folding and replacing expensive operations with faster alternatives. until fixed point Join ordering Estimates the branches of joins that should be executed first in order to reduce memory pressure. 1 time Type coercion Coerce types such that operations succeed and run on minimal required memory. until fixed point Cardinality estimation Estimates cardinality in order to determine optimal groupby strategy. 0/n times; dependent on query"},{"location":"user-guide/lazy/query_plan/","title":"Query Plan","text":"<p>For any lazy query <code>Polars</code> has both:</p> <ul> <li>a non-optimized plan with the set of steps code as we provided it and</li> <li>an optimized plan with changes made by the query optimizer</li> </ul> <p>We can understand both the non-optimized and optimized query plans with visualization and by printing them as text.</p>"},{"location":"user-guide/lazy/query_plan/#non-optimized-query-plan","title":"Non-optimized query plan","text":""},{"location":"user-guide/lazy/query_plan/#graphviz-visualization","title":"Graphviz visualization","text":"<p>First we visualise the non-optimized plan by setting <code>optimized=False</code>.</p>  Python <p> <code>show_graph</code> <pre><code>q1 = (\n    pl.scan_csv(f\"docs/src/data/reddit.csv\")\n    .with_columns(pl.col(\"name\").str.to_uppercase())\n    .filter(pl.col(\"comment_karma\") &gt; 0)\n)\n</code></pre></p> <pre><code>\n</code></pre> <p></p> <p>The query plan visualisation should be read from bottom to top. In the visualisation:</p> <ul> <li>each box corresponds to a stage in the query plan</li> <li>the <code>sigma</code> stands for <code>SELECTION</code> and indicates any filter conditions</li> <li>the <code>pi</code> stands for <code>PROJECTION</code> and indicates choosing a subset of columns</li> </ul>"},{"location":"user-guide/lazy/query_plan/#printed-query-plan","title":"Printed query plan","text":"<p>We can also print the non-optimized plan with <code>explain(optimized=False)</code></p>  Python <p> <code>explain</code> <pre><code>q1 = (\n    pl.scan_csv(f\"docs/src/data/reddit.csv\")\n    .with_columns(pl.col(\"name\").str.to_uppercase())\n    .filter(pl.col(\"comment_karma\") &gt; 0)\n)\n</code></pre></p> <pre><code>FILTER [(col(\"comment_karma\")) &gt; (0)] FROM WITH_COLUMNS:\n [col(\"name\").str.uppercase()]\n\n    CSV SCAN data/reddit.csv\n    PROJECT */6 COLUMNS\n</code></pre> <p>The printed plan should also be read from bottom to top. This non-optimized plan is roughly to:</p> <ul> <li>read from the <code>data/reddit.csv</code> file</li> <li>read all 6 columns (where the * wildcard in PROJECT */6 COLUMNS means take all columns)</li> <li>transform the <code>name</code> column to uppercase</li> <li>apply a filter on the <code>comment_karma</code> column</li> </ul>"},{"location":"user-guide/lazy/query_plan/#optimized-query-plan","title":"Optimized query plan","text":"<p>Now we visualise the optimized plan with <code>show_graph</code>.</p>  Python <p> <code>show_graph</code> <pre><code>q1.show_graph()\n</code></pre></p> <p></p> <p>We can also print the optimized plan with <code>explain</code></p>  Python <p> <code>explain</code> <pre><code>q1.explain()\n</code></pre></p> <pre><code> WITH_COLUMNS:\n [col(\"name\").str.uppercase()]\n\n    CSV SCAN data/reddit.csv\n    PROJECT */6 COLUMNS\n    SELECTION: [(col(\"comment_karma\")) &gt; (0)]\n</code></pre> <p>The optimized plan is to:</p> <ul> <li>read the data from the Reddit CSV</li> <li>apply the filter on the <code>comment_karma</code> column while the CSV is being read line-by-line</li> <li>transform the <code>name</code> column to uppercase</li> </ul> <p>In this case the query optimizer has identified that the <code>filter</code> can be applied while the CSV is read from disk rather than writing the whole file to disk and then applying it. This optimization is called Predicate Pushdown.</p>"},{"location":"user-guide/lazy/schemas/","title":"Schema","text":"<p>The schema of a Polars <code>DataFrame</code> or <code>LazyFrame</code> sets out the names of the columns and their datatypes. You can see the schema with the <code>.schema</code> method on a <code>DataFrame</code> or <code>LazyFrame</code></p>  Python <p> <code>DataFrame</code> \u00b7 <code>lazy</code> <pre><code>q3 = pl.DataFrame({\"foo\": [\"a\", \"b\", \"c\"], \"bar\": [0, 1, 2]}).lazy()\n\nprint(q3.schema)\n</code></pre></p> <pre><code>{'foo': Utf8, 'bar': Int64}\n</code></pre> <p>The schema plays an important role in the lazy API.</p>"},{"location":"user-guide/lazy/schemas/#type-checking-in-the-lazy-api","title":"Type checking in the lazy API","text":"<p>One advantage of the lazy API is that Polars will check the schema before any data is processed. This check happens when you execute your lazy query.</p> <p>We see how this works in the following simple example where we call the <code>.round</code> expression on the integer <code>bar</code> column.</p>  Python <p> <code>lazy</code> \u00b7 <code>with_columns</code> <pre><code>pl.DataFrame({\"foo\": [\"a\", \"b\", \"c\"], \"bar\": [0, 1, 2]}).lazy().with_columns(\n    pl.col(\"bar\").round(0)\n)\n</code></pre></p> <p>The <code>.round</code> expression is only valid for columns with a floating point dtype. Calling <code>.round</code> on an integer column means the operation will raise a <code>SchemaError</code>.</p> <p>If we executed this query in eager mode the error would only be found once the data had been processed in all earlier steps.</p> <p>When we execute a lazy query Polars checks for any potential <code>SchemaError</code> before the time-consuming step of actually processing the data in the pipeline.</p>"},{"location":"user-guide/lazy/schemas/#the-lazy-api-must-know-the-schema","title":"The lazy API must know the schema","text":"<p>In the lazy API the Polars query optimizer must be able to infer the schema at every step of a query plan. This means that operations where the schema is not knowable in advance cannot be used with the lazy API.</p> <p>The classic example of an operation where the schema is not knowable in advance is a <code>.pivot</code> operation. In a <code>.pivot</code> the new column names come from data in one of the columns. As these column names cannot be known in advance a <code>.pivot</code> is not available in the lazy API.</p>"},{"location":"user-guide/lazy/schemas/#dealing-with-operations-not-available-in-the-lazy-api","title":"Dealing with operations not available in the lazy API","text":"<p>If your pipeline includes an operation that is not available in the lazy API it is normally best to:</p> <ul> <li>run the pipeline in lazy mode up until that point</li> <li>execute the pipeline with <code>.collect</code> to materialize a <code>DataFrame</code></li> <li>do the non-lazy operation on the <code>DataFrame</code></li> <li>convert the output back to a <code>LazyFrame</code> with <code>.lazy</code> and continue in lazy mode</li> </ul> <p>We show how to deal with a non-lazy operation in this example where we:</p> <ul> <li>create a simple <code>DataFrame</code></li> <li>convert it to a <code>LazyFrame</code> with <code>.lazy</code></li> <li>do a transformation using <code>.with_columns</code></li> <li>execute the query before the pivot with <code>.collect</code> to get a <code>DataFrame</code></li> <li>do the <code>.pivot</code> on the <code>DataFrame</code></li> <li>convert back in lazy mode</li> <li>do a <code>.filter</code></li> <li>finish by executing the query with <code>.collect</code> to get a <code>DataFrame</code></li> </ul>  Python <p> <code>collect</code> \u00b7 <code>pivot</code> \u00b7 <code>filter</code> <pre><code>lazy_eager_query = (\n    pl.DataFrame(\n        {\n            \"id\": [\"a\", \"b\", \"c\"],\n            \"month\": [\"jan\", \"feb\", \"mar\"],\n            \"values\": [0, 1, 2],\n        }\n    )\n    .lazy()\n    .with_columns((2 * pl.col(\"values\")).alias(\"double_values\"))\n    .collect()\n    .pivot(\n        index=\"id\", columns=\"month\", values=\"double_values\", aggregate_function=\"first\"\n    )\n    .lazy()\n    .filter(pl.col(\"mar\").is_null())\n    .collect()\n)\nprint(lazy_eager_query)\n</code></pre></p> <pre><code>shape: (2, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 jan  \u2506 feb  \u2506 mar  \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2506 ---  \u2502\n\u2502 str \u2506 i64  \u2506 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 0    \u2506 null \u2506 null \u2502\n\u2502 b   \u2506 null \u2506 2    \u2506 null \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/lazy/streaming/","title":"Streaming","text":"<p> Under Construction  </p> <p>This section is still under development. Want to help out? Consider contributing and making a pull request to our repository. Please read our Contribution Guidelines on how to proceed.</p>"},{"location":"user-guide/lazy/using/","title":"Usage","text":"<p>With the lazy API Polars doesn't run each query line-by-line but instead processes the full query end-to-end. To get the most out of Polars it is important that you use the lazy API because:</p> <ul> <li>the lazy API allows Polars to apply automatic query optimization with the query optimizer</li> <li>the lazy API allows you to work with larger than memory datasets using streaming</li> <li>the lazy API can catch schema errors before processing the data</li> </ul> <p>Here we see how to use the lazy API starting from either a file or an existing <code>DataFrame</code>.</p>"},{"location":"user-guide/lazy/using/#using-the-lazy-api-from-a-file","title":"Using the lazy API from a file","text":"<p>In the ideal case we use the lazy API right from a file as the query optimizer may help us to reduce the amount of data we read from the file.</p> <p>We create a lazy query from the Reddit CSV data and apply some transformations.</p> <p>By starting the query with <code>pl.scan_csv</code> we are using the lazy API.</p>  Python <p> <code>scan_csv</code> \u00b7 <code>with_columns</code> \u00b7 <code>filter</code> \u00b7 <code>col</code> <pre><code>q1 = (\n    pl.scan_csv(f\"docs/src/data/reddit.csv\")\n    .with_columns(pl.col(\"name\").str.to_uppercase())\n    .filter(pl.col(\"comment_karma\") &gt; 0)\n)\n</code></pre></p> <p>A <code>pl.scan_</code> function is available for a number of file types including CSV, IPC, Parquet and JSON.</p> <p>In this query we tell Polars that we want to:</p> <ul> <li>load data from the Reddit CSV file</li> <li>convert the <code>name</code> column to uppercase</li> <li>apply a filter to the <code>comment_karma</code> column</li> </ul> <p>The lazy query will not be executed at this point. See this page on executing lazy queries for more on running lazy queries.</p>"},{"location":"user-guide/lazy/using/#using-the-lazy-api-from-a-dataframe","title":"Using the lazy API from a <code>DataFrame</code>","text":"<p>An alternative way to access the lazy API is to call <code>.lazy</code> on a <code>DataFrame</code> that has already been created in memory.</p>  Python <p> <code>lazy</code> <pre><code>q3 = pl.DataFrame({\"foo\": [\"a\", \"b\", \"c\"], \"bar\": [0, 1, 2]}).lazy()\n</code></pre></p> <p>By calling <code>.lazy</code> we convert the <code>DataFrame</code> to a <code>LazyFrame</code>.</p>"},{"location":"user-guide/migration/pandas/","title":"Coming from Pandas","text":"<p>Here we set out the key points that anyone who has experience with <code>Pandas</code> and wants to try <code>Polars</code> should know. We include both differences in the concepts the libraries are built on and differences in how you should write <code>Polars</code> code compared to <code>Pandas</code> code.</p>"},{"location":"user-guide/migration/pandas/#differences-in-concepts-between-polars-and-pandas","title":"Differences in concepts between <code>Polars</code> and <code>Pandas</code>","text":""},{"location":"user-guide/migration/pandas/#polars-does-not-have-a-multi-indexindex","title":"<code>Polars</code> does not have a multi-index/index","text":"<p><code>Pandas</code> gives a label to each row with an index. <code>Polars</code> does not use an index and each row is indexed by its integer position in the table.</p> <p>Polars aims to have predictable results and readable queries, as such we think an index does not help us reach that objective. We believe the semantics of a query should not change by the state of an index or a <code>reset_index</code> call.</p> <p>In Polars a DataFrame will always be a 2D table with heterogeneous data-types. The data-types may have nesting, but the table itself will not. Operations like resampling will be done by specialized functions or methods that act like 'verbs' on a table explicitly stating columns that 'verb' operates on. As such, it is our conviction that not having indices make things simpler, more explicit, more readable and less error-prone.</p> <p>Note that an 'index' data structure as known in databases will be used by polars as an optimization technique.</p>"},{"location":"user-guide/migration/pandas/#polars-uses-apache-arrow-arrays-to-represent-data-in-memory-while-pandas-uses-numpy-arrays","title":"<code>Polars</code> uses Apache Arrow arrays to represent data in memory while <code>Pandas</code> uses <code>Numpy</code> arrays","text":"<p><code>Polars</code> represents data in memory with Arrow arrays while <code>Pandas</code> represents data in memory in <code>Numpy</code> arrays. Apache Arrow is an emerging standard for in-memory columnar analytics that can accelerate data load times, reduce memory usage and accelerate calculations.</p> <p><code>Polars</code> can convert data to <code>Numpy</code> format with the <code>to_numpy</code> method.</p>"},{"location":"user-guide/migration/pandas/#polars-has-more-support-for-parallel-operations-than-pandas","title":"<code>Polars</code> has more support for parallel operations than <code>Pandas</code>","text":"<p><code>Polars</code> exploits the strong support for concurrency in Rust to run many operations in parallel. While some operations in <code>Pandas</code> are multi-threaded the core of the library is single-threaded and an additional library such as <code>Dask</code> must be used to parallelise operations.</p>"},{"location":"user-guide/migration/pandas/#polars-can-lazily-evaluate-queries-and-apply-query-optimization","title":"<code>Polars</code> can lazily evaluate queries and apply query optimization","text":"<p>Eager evaluation is where code is evaluated as soon as you run the code. Lazy evaluation is where running a line of code means that the underlying logic is added to a query plan rather than being evaluated.</p> <p><code>Polars</code> supports eager evaluation and lazy evaluation whereas <code>Pandas</code> only supports eager evaluation. The lazy evaluation mode is powerful because <code>Polars</code> carries out automatic query optimization where it examines the query plan and looks for ways to accelerate the query or reduce memory usage.</p> <p><code>Dask</code> also supports lazy evaluation where it generates a query plan. However, <code>Dask</code> does not carry out query optimization on the query plan.</p>"},{"location":"user-guide/migration/pandas/#key-syntax-differences","title":"Key syntax differences","text":"<p>Users coming from <code>Pandas</code> generally need to know one thing...</p> <pre><code>polars != pandas\n</code></pre> <p>If your <code>Polars</code> code looks like it could be <code>Pandas</code> code, it might run, but it likely runs slower than it should.</p> <p>Let's go through some typical <code>Pandas</code> code and see how we might write that in <code>Polars</code>.</p>"},{"location":"user-guide/migration/pandas/#selecting-data","title":"Selecting data","text":"<p>As there is no index in <code>Polars</code> there is no <code>.loc</code> or <code>iloc</code> method in <code>Polars</code> - and there is also no <code>SettingWithCopyWarning</code> in <code>Polars</code>.</p> <p>However, the best way to select data in <code>Polars</code> is to use the expression API. For example, if you want to select a column in <code>Pandas</code> you can do one of the following:</p> <pre><code>df['a']\ndf.loc[:,'a']\n</code></pre> <p>but in <code>Polars</code> you would use the <code>.select</code> method:</p> <pre><code>df.select(['a'])\n</code></pre> <p>If you want to select rows based on the values then in <code>Polars</code> you use the <code>.filter</code> method:</p> <pre><code>df.filter(pl.col('a') &lt; 10)\n</code></pre> <p>As noted in the section on expressions below, <code>Polars</code> can run operations in <code>.select</code> and <code>filter</code> in parallel and <code>Polars</code> can carry out query optimization on the full set of data selection criteria.</p>"},{"location":"user-guide/migration/pandas/#be-lazy","title":"Be lazy","text":"<p>Working in lazy evaluation mode is straightforward and should be your default in <code>Polars</code> as the lazy mode allows <code>Polars</code> to do query optimization.</p> <p>We can run in lazy mode by either using an implicitly lazy function (such as <code>scan_csv</code>) or explicitly using the <code>lazy</code> method.</p> <p>Take the following simple example where we read a CSV file from disk and do a groupby. The CSV file has numerous columns but we just want to do a groupby on one of the id columns (<code>id1</code>) and then sum by a value column (<code>v1</code>). In <code>Pandas</code> this would be:</p> <pre><code>    df = pd.read_csv(csv_file, usecols=['id1','v1'])\n    grouped_df = df.loc[:,['id1','v1']].groupby('id1').sum('v1')\n</code></pre> <p>In <code>Polars</code> you can build this query in lazy mode with query optimization and evaluate it by replacing the eager <code>Pandas</code> function <code>read_csv</code> with the implicitly lazy <code>Polars</code> function <code>scan_csv</code>:</p> <pre><code>    df = pl.scan_csv(csv_file)\n    grouped_df = df.groupby('id1').agg(pl.col('v1').sum()).collect()\n</code></pre> <p><code>Polars</code> optimizes this query by identifying that only the <code>id1</code> and <code>v1</code> columns are relevant and so will only read these columns from the CSV. By calling the <code>.collect</code> method at the end of the second line we instruct <code>Polars</code> to eagerly evaluate the query.</p> <p>If you do want to run this query in eager mode you can just replace <code>scan_csv</code> with <code>read_csv</code> in the <code>Polars</code> code.</p> <p>Read more about working with lazy evaluation in the lazy API section.</p>"},{"location":"user-guide/migration/pandas/#express-yourself","title":"Express yourself","text":"<p>A typical <code>Pandas</code> script consists of multiple data transformations that are executed sequentially. However, in <code>Polars</code> these transformations can be executed in parallel using expressions.</p>"},{"location":"user-guide/migration/pandas/#column-assignment","title":"Column assignment","text":"<p>We have a dataframe <code>df</code> with a column called <code>value</code>. We want to add two new columns, a column called <code>tenXValue</code> where the <code>value</code> column is multiplied by 10 and a column called <code>hundredXValue</code> where the <code>value</code> column is multiplied by 100.</p> <p>In <code>Pandas</code> this would be:</p> <pre><code>df[\"tenXValue\"] = df[\"value\"] * 10\ndf[\"hundredXValue\"] = df[\"value\"] * 100\n</code></pre> <p>These column assignments are executed sequentially.</p> <p>In <code>Polars</code> we add columns to <code>df</code> using the <code>.with_columns</code> method and name them with the <code>.alias</code> method:</p> <pre><code>df.with_columns([\n    (pl.col(\"value\") * 10).alias(\"tenXValue\"),\n    (pl.col(\"value\") * 100).alias(\"hundredXValue\"),\n])\n</code></pre> <p>These column assignments are executed in parallel.</p>"},{"location":"user-guide/migration/pandas/#column-assignment-based-on-predicate","title":"Column assignment based on predicate","text":"<p>In this case we have a dataframe <code>df</code> with columns <code>a</code>,<code>b</code> and <code>c</code>. We want to re-assign the values in column <code>a</code> based on a condition. When the value in column <code>c</code> is equal to 2 then we replace the value in <code>a</code> with the value in <code>b</code>.</p> <p>In <code>Pandas</code> this would be:</p> <pre><code>df.loc[df[\"c\"] == 2, \"a\"] = df.loc[df[\"c\"] == 2, \"b\"]\n</code></pre> <p>while in <code>Polars</code> this would be:</p> <pre><code>df.with_columns(\n    pl.when(pl.col(\"c\") == 2)\n    .then(pl.col(\"b\"))\n    .otherwise(pl.col(\"a\")).alias(\"a\")\n)\n</code></pre> <p>The <code>Polars</code> way is pure in that the original <code>DataFrame</code> is not modified. The <code>mask</code> is also not computed twice as in <code>Pandas</code> (you could prevent this in <code>Pandas</code>, but that would require setting a temporary variable).</p> <p>Additionally <code>Polars</code> can compute every branch of an <code>if -&gt; then -&gt; otherwise</code> in parallel. This is valuable, when the branches get more expensive to compute.</p>"},{"location":"user-guide/migration/pandas/#filtering","title":"Filtering","text":"<p>We want to filter the dataframe <code>df</code> with housing data based on some criteria.</p> <p>In <code>Pandas</code> you filter the dataframe by passing Boolean expressions to the <code>loc</code> method:</p> <pre><code>df.loc[(df['sqft_living'] &gt; 2500) &amp; (df['price'] &lt; 300000)]\n</code></pre> <p>while in <code>Polars</code> you call the <code>filter</code> method:</p> <pre><code>df.filter(\n    (pl.col(\"m2_living\") &gt; 2500) &amp; (pl.col(\"price\") &lt; 300000)\n)\n</code></pre> <p>The query optimizer in <code>Polars</code> can also detect if you write multiple filters separately and combine them into a single filter in the optimized plan.</p>"},{"location":"user-guide/migration/pandas/#pandas-transform","title":"<code>Pandas</code> transform","text":"<p>The <code>Pandas</code> documentation demonstrates an operation on a groupby called <code>transform</code>. In this case we have a dataframe <code>df</code> and we want a new column showing the number of rows in each group.</p> <p>In <code>Pandas</code> we have:</p> <pre><code>df = pd.DataFrame({\n    \"type\": [\"m\", \"n\", \"o\", \"m\", \"m\", \"n\", \"n\"],\n    \"c\": [1, 1, 1, 2, 2, 2, 2],\n})\n\ndf[\"size\"] = df.groupby(\"c\")[\"type\"].transform(len)\n</code></pre> <p>Here <code>Pandas</code> does a groupby on <code>\"c\"</code>, takes column <code>\"type\"</code>, computes the group length and then joins the result back to the original <code>DataFrame</code> producing:</p> <pre><code>   c type size\n0  1    m    3\n1  1    n    3\n2  1    o    3\n3  2    m    4\n4  2    m    4\n5  2    n    4\n6  2    n    4\n</code></pre> <p>In <code>Polars</code> the same can be achieved with <code>window</code> functions:</p> <pre><code>df.select([\n    pl.all(),\n    pl.col(\"type\").count().over(\"c\").alias(\"size\")\n])\n</code></pre> <pre><code>shape: (7, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 c   \u2506 type \u2506 size \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2502\n\u2502 i64 \u2506 str  \u2506 u32  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 m    \u2506 3    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 n    \u2506 3    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 o    \u2506 3    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 m    \u2506 4    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 m    \u2506 4    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 n    \u2506 4    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 n    \u2506 4    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Because we can store the whole operation in a single expression, we can combine several <code>window</code> functions and even combine different groups!</p> <p><code>Polars</code> will cache window expressions that are applied over the same group, so storing them in a single <code>select</code> is both convenient and optimal. In the following example we look at a case where we are calculating group statistics over <code>\"c\"</code> twice:</p> <pre><code>df.select([\n    pl.all(),\n    pl.col(\"c\").count().over(\"c\").alias(\"size\"),\n    pl.col(\"c\").sum().over(\"type\").alias(\"sum\"),\n    pl.col(\"c\").reverse().over(\"c\").flatten().alias(\"reverse_type\")\n])\n</code></pre> <pre><code>shape: (7, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 c   \u2506 type \u2506 size \u2506 sum \u2506 reverse_type \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2506 --- \u2506 ---          \u2502\n\u2502 i64 \u2506 str  \u2506 u32  \u2506 i64 \u2506 i64          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 m    \u2506 3    \u2506 5   \u2506 2            \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 n    \u2506 3    \u2506 5   \u2506 2            \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 1   \u2506 o    \u2506 3    \u2506 1   \u2506 2            \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 m    \u2506 4    \u2506 5   \u2506 2            \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 m    \u2506 4    \u2506 5   \u2506 1            \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 n    \u2506 4    \u2506 5   \u2506 1            \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2   \u2506 n    \u2506 4    \u2506 5   \u2506 1            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/migration/pandas/#missing-data","title":"Missing data","text":"<p><code>Pandas</code> uses <code>NaN</code> and/or <code>None</code> values to indicate missing values depending on the dtype of the column. In addition the behaviour in <code>Pandas</code> varies depending on whether the default dtypes or optional nullable arrays are used. In <code>Polars</code> missing data corresponds to a <code>null</code> value for all data types.</p> <p>For float columns <code>Polars</code> permits the use of <code>NaN</code> values. These <code>NaN</code> values are not considered to be missing data but instead a special floating point value.</p> <p>In <code>Pandas</code> an integer column with missing values is cast to be a float column with <code>NaN</code> values for the missing values (unless using optional nullable integer dtypes). In <code>Polars</code> any missing values in an integer column are simply <code>null</code> values and the column remains an integer column.</p> <p>See the missing data section for more details.</p>"},{"location":"user-guide/migration/spark/","title":"Coming from Apache Spark","text":""},{"location":"user-guide/migration/spark/#column-based-api-vs-row-based-api","title":"Column-based API vs. Row-based API","text":"<p>Whereas the <code>Spark</code> <code>DataFrame</code> is analogous to a collection of rows, a <code>Polars</code> <code>DataFrame</code> is closer to a collection of columns. This means that you can combine columns in <code>Polars</code> in ways that are not possible in <code>Spark</code>, because <code>Spark</code> preserves the relationship of the data in each row.</p> <p>Consider this sample dataset:</p> <pre><code>import polars as pl\n\ndf = pl.DataFrame({\n    \"foo\": [\"a\", \"b\", \"c\", \"d\", \"d\"],\n    \"bar\": [1, 2, 3, 4, 5],\n})\n\ndfs = spark.createDataFrame(\n    [\n        (\"a\", 1),\n        (\"b\", 2),\n        (\"c\", 3),\n        (\"d\", 4),\n        (\"d\", 5),\n    ],\n    schema=[\"foo\", \"bar\"],\n)\n</code></pre>"},{"location":"user-guide/migration/spark/#example-1-combining-head-and-sum","title":"Example 1: Combining <code>head</code> and <code>sum</code>","text":"<p>In <code>Polars</code> you can write something like this:</p> <pre><code>df.select([\n    pl.col(\"foo\").sort().head(2),\n    pl.col(\"bar\").filter(pl.col(\"foo\") == \"d\").sum()\n])\n</code></pre> <p>Output:</p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 bar \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 str \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 9   \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 b   \u2506 9   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The expressions on columns <code>foo</code> and <code>bar</code> are completely independent. Since the expression on <code>bar</code> returns a single value, that value is repeated for each value output by the expression on <code>foo</code>. But <code>a</code> and <code>b</code> have no relation to the data that produced the sum of <code>9</code>.</p> <p>To do something similar in <code>Spark</code>, you'd need to compute the sum separately and provide it as a literal:</p> <pre><code>from pyspark.sql.functions import col, sum, lit\n\nbar_sum = (\n    dfs\n    .where(col(\"foo\") == \"d\")\n    .groupBy()\n    .agg(sum(col(\"bar\")))\n    .take(1)[0][0]\n)\n\n(\n    dfs\n    .orderBy(\"foo\")\n    .limit(2)\n    .withColumn(\"bar\", lit(bar_sum))\n    .show()\n)\n</code></pre> <p>Output:</p> <pre><code>+---+---+\n|foo|bar|\n+---+---+\n|  a|  9|\n|  b|  9|\n+---+---+\n</code></pre>"},{"location":"user-guide/migration/spark/#example-2-combining-two-heads","title":"Example 2: Combining Two <code>head</code>s","text":"<p>In <code>Polars</code> you can combine two different <code>head</code> expressions on the same DataFrame, provided that they return the same number of values.</p> <pre><code>df.select([\n    pl.col(\"foo\").sort().head(2),\n    pl.col(\"bar\").sort(descending=True).head(2),\n])\n</code></pre> <p>Output:</p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 bar \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 str \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 5   \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 b   \u2506 4   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Again, the two <code>head</code> expressions here are completely independent, and the pairing of <code>a</code> to <code>5</code> and <code>b</code> to <code>4</code> results purely from the juxtaposition of the two columns output by the expressions.</p> <p>To accomplish something similar in <code>Spark</code>, you would need to generate an artificial key that enables you to join the values in this way.</p> <pre><code>from pyspark.sql import Window\nfrom pyspark.sql.functions import row_number\n\nfoo_dfs = (\n    dfs\n    .withColumn(\n        \"rownum\",\n        row_number().over(Window.orderBy(\"foo\"))\n    )\n)\n\nbar_dfs = (\n    dfs\n    .withColumn(\n        \"rownum\",\n        row_number().over(Window.orderBy(col(\"bar\").desc()))\n    )\n)\n\n(\n    foo_dfs.alias(\"foo\")\n    .join(bar_dfs.alias(\"bar\"), on=\"rownum\")\n    .select(\"foo.foo\", \"bar.bar\")\n    .limit(2)\n    .show()\n)\n</code></pre> <p>Output:</p> <pre><code>+---+---+\n|foo|bar|\n+---+---+\n|  a|  5|\n|  b|  4|\n+---+---+\n</code></pre>"},{"location":"user-guide/misc/alternatives/","title":"Alternatives","text":"<p>These are some tools that share similar functionality to what polars does.</p> <ul> <li> <p>Pandas</p> <p>A very versatile tool for small data. Read 10 things I hate about pandas written by the author himself. Polars has solved all those 10 things. Polars is a versatile tool for small and large data with a more predictable API, less ambiguous and stricter API.</p> </li> <li> <p>Pandas the API</p> <p>The API of pandas was designed for in memory data. This makes it a poor fit for performant analysis on large data (read anything that does not fit into RAM). Any tool that tries to distribute that API will likely have a suboptimal query plan compared to plans that follow from a declarative API like SQL or polars' API.</p> </li> <li> <p>Dask</p> <p>Parallelizes existing single-threaded libraries like <code>NumPy</code> and <code>Pandas</code>. As a consumer of those libraries Dask therefore has less control over low level performance and semantics. Those libraries are treated like a black box. On a single machine the parallelization effort can also be seriously stalled by pandas strings. Pandas strings, by default, are stored as python objects in numpy arrays meaning that any operation on them is GIL bound and therefore single threaded. This can be circumvented by multi-processing but has a non-trivial cost.</p> </li> <li> <p>Modin</p> <p>Similar to Dask</p> </li> <li> <p>Vaex</p> <p>Vaexs method of out-of-core analysis is memory mapping files. This works until it doesn't. For instance parquet or csv files first need to be read and converted to a file format that can be memory mapped. Another downside is that the OS determines when pages will be swapped. Operations that need a full data shuffle, such as sorts, have terrible performance on memory mapped data. Polars' out of core processing is not based on memory mapping, but on streaming data in batches (and spilling to disk if needed), we control which data must be hold in memory, not the OS, meaning that we don't have unexpected IO stalls.</p> </li> <li> <p>DuckDB</p> <p>Polars and DuckDB have many similarities. DuckDB is focused on providing an in-process OLAP Sqlite alternative, polars is focused on providing a scalable <code>DataFrame</code> interface to many languages. Those different front-ends lead to different optimization strategies and different algorithm prioritization. The interop between both is zero-copy. See more: https://duckdb.org/docs/guides/python/polars</p> </li> <li> <p>Spark</p> <p>Spark is designed for distributed workloads and uses the JVM. The setup for spark is complicated and the startup-time is slow. On a single machine Polars has much better performance characteristics. If you need to process TB's of data spark is a better choice.</p> </li> <li> <p>CuDF</p> <p>GPU's and CuDF are fast! However, GPU's are not readily available and expensive in production. The amount of memory available on GPU often is a fraction of available RAM. This (and out-of-core) processing means that polars can handle much larger data-sets. Next to that Polars can be close in performance to CuDF. CuDF doesn't optimize your query, so is not uncommon that on ETL jobs polars will be faster because it can elide unneeded work and materialization's.</p> </li> <li> <p>Any</p> <p>Polars is written in Rust. This gives it strong safety, performance and concurrency guarantees. Polars is written in a modular manner. Parts of polars can be used in other query program and can be added as a library.</p> </li> </ul>"},{"location":"user-guide/misc/contributing/","title":"Contributing","text":"<p>See the <code>CONTRIBUTING.md</code> if you would like to contribute to the <code>Polars</code> project.</p> <p>If you're new to this we recommend starting out with contributing examples to the Python API documentation. The Python API docs are generated from the docstrings of the Python wrapper located in <code>polars/py-polars</code>.</p> <p>Here is an example commit that adds a docstring.</p> <p>If you spot any gaps in this User Guide you can submit fixes to the <code>pola-rs/polars-book</code> repo.</p> <p>Happy hunting!</p>"},{"location":"user-guide/misc/reference-guides/","title":"Reference Guides","text":"<p>The api documentations with details on function / object signatures can be found here:</p> <ul> <li>NodeJS</li> <li>Python</li> <li>Rust</li> </ul>"},{"location":"user-guide/sql/create/","title":"CREATE","text":"<p>In Polars, the <code>SQLContext</code> provides a way to execute SQL statements against <code>LazyFrames</code> and <code>DataFrames</code> using SQL syntax. One of the SQL statements that can be executed using <code>SQLContext</code> is the <code>CREATE TABLE</code> statement, which is used to create a new table.</p> <p>The syntax for the <code>CREATE TABLE</code> statement in Polars is as follows:</p> <pre><code>CREATE TABLE table_name\nAS\nSELECT ...\n</code></pre> <p>In this syntax, <code>table_name</code> is the name of the new table that will be created, and <code>SELECT ...</code> is a SELECT statement that defines the data that will be inserted into the table.</p> <p>Here's an example of how to use the <code>CREATE TABLE</code> statement in Polars:</p>  Python <p> <code>register</code> \u00b7 <code>execute</code> <pre><code>data = {\"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"], \"age\": [25, 30, 35, 40]}\ndf = pl.LazyFrame(data)\n\nctx = pl.SQLContext(my_table=df, eager_execution=True)\n\nresult = ctx.execute(\n\"\"\"\n    CREATE TABLE older_people\n    AS\n    SELECT * FROM my_table WHERE age &gt; 30\n\"\"\"\n)\n\nprint(ctx.execute(\"SELECT * FROM older_people\"))\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name    \u2506 age \u2502\n\u2502 ---     \u2506 --- \u2502\n\u2502 str     \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Charlie \u2506 35  \u2502\n\u2502 David   \u2506 40  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In this example, we use the <code>execute()</code> method of the <code>SQLContext</code> to execute a <code>CREATE TABLE</code> statement that creates a new table called <code>older_people</code> based on a SELECT statement that selects all rows from the <code>my_table</code> DataFrame where the <code>age</code> column is greater than 30.</p> <p>Note</p> <p>Note that the result of a <code>CREATE TABLE</code> statement is not the table itself. The table is registered in the <code>SQLContext</code>. In case you want to turn the table back to a <code>DataFrame</code> you can use a <code>SELECT * FROM ...</code> statement</p>"},{"location":"user-guide/sql/cte/","title":"Common Table Expressions","text":"<p>Common Table Expressions (CTEs) are a feature of SQL that allow you to define a temporary named result set that can be referenced within a SQL statement. CTEs provide a way to break down complex SQL queries into smaller, more manageable pieces, making them easier to read, write, and maintain.</p> <p>A CTE is defined using the <code>WITH</code> keyword followed by a comma-separated list of subqueries, each of which defines a named result set that can be used in subsequent queries. The syntax for a CTE is as follows:</p> <pre><code>WITH cte_name AS (\n    subquery\n)\nSELECT ...\n</code></pre> <p>In this syntax, <code>cte_name</code> is the name of the CTE, and <code>subquery</code> is the subquery that defines the result set. The CTE can then be referenced in subsequent queries as if it were a table or view.</p> <p>CTEs are particularly useful when working with complex queries that involve multiple levels of subqueries, as they allow you to break down the query into smaller, more manageable pieces that are easier to understand and debug. Additionally, CTEs can help improve query performance by allowing the database to optimize and cache the results of subqueries, reducing the number of times they need to be executed.</p> <p>Polars supports Common Table Expressions (CTEs) using the WITH clause in SQL syntax. Below is an example </p>  Python <p> <code>register</code> \u00b7 <code>execute</code> <pre><code>ctx = pl.SQLContext()\ndf = pl.LazyFrame(\n    {\"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"], \"age\": [25, 30, 35, 40]}\n)\nctx.register(\"my_table\", df)\n\nresult = ctx.execute(\n\"\"\"\n    WITH older_people AS (\n        SELECT * FROM my_table WHERE age &gt; 30\n    )\n    SELECT * FROM older_people WHERE STARTS_WITH(name,'C')\n\"\"\",\n    eager=True,\n)\n\nprint(result)\n</code></pre></p> <pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name    \u2506 age \u2502\n\u2502 ---     \u2506 --- \u2502\n\u2502 str     \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Charlie \u2506 35  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In this example, we use the <code>execute()</code> method of the <code>SQLContext</code> to execute a SQL query that includes a CTE. The CTE selects all rows from the <code>my_table</code> LazyFrame where the <code>age</code> column is greater than 30 and gives it the alias <code>older_people</code>. We then execute a second SQL query that selects all rows from the <code>older_people</code> CTE where the <code>name</code> column starts with the letter 'C'.</p>"},{"location":"user-guide/sql/intro/","title":"Introduction","text":"<p>While Polars does support writing queries in SQL, it's recommended that users familiarize themselves with the expression syntax for more readable and expressive code. As a primarily DataFrame library, new features will typically be added to the expression API first. However, if you already have an existing SQL codebase or prefer to use SQL, Polars also offers support for SQL queries.</p> <p>Note</p> <p>In Polars, there is no separate SQL engine because Polars translates SQL queries into expressions, which are then executed using its built-in execution engine. This approach ensures that Polars maintains its performance and scalability advantages as a native DataFrame library while still providing users with the ability to work with SQL queries.</p>"},{"location":"user-guide/sql/intro/#context","title":"Context","text":"<p>Polars uses the <code>SQLContext</code> to manage SQL queries . The context contains a dictionary mapping <code>DataFrames</code> and <code>LazyFrames</code> names to their corresponding datasets1. The example below starts a <code>SQLContext</code>:</p>  Python <p> <code>SQLContext</code> <pre><code>ctx = pl.SQLContext()\n</code></pre></p> <p></p>"},{"location":"user-guide/sql/intro/#register-dataframes","title":"Register Dataframes","text":"<p>There are 2 ways to register DataFrames in the <code>SQLContext</code>:</p> <ul> <li>register all <code>LazyFrames</code> and <code>DataFrames</code> in the global namespace</li> <li>register them one by one</li> </ul>  Python <p> <code>SQLContext</code> <pre><code>df = pl.DataFrame({\"a\": [1, 2, 3]})\nlf = pl.LazyFrame({\"b\": [4, 5, 6]})\n\n# Register all dataframes in the global namespace: registers both df and lf\nctx = pl.SQLContext(register_globals=True)\n\n# Other option: register dataframe df as \"df\" and lazyframe lf as \"lf\"\nctx = pl.SQLContext(df=df, lf=lf)\n</code></pre></p> <p></p> <p>We can also register Pandas DataFrames by converting them to Polars first.</p>  Python <p> <code>SQLContext</code> <pre><code>import pandas as pd\n\ndf_pandas = pd.DataFrame({\"c\": [7, 8, 9]})\nctx = pl.SQLContext(df_pandas=pl.from_pandas(df_pandas))\n</code></pre></p> <p></p> <p>Note</p> <p>Converting a Pandas DataFrame backed by Numpy to Polars triggers a conversion to the Arrow format. This conversion has a computation cost. Converting a Pandas DataFrame backed by Arrow on the other hand will be free or almost free.</p> <p>Once the <code>SQLContext</code> is initialized, we can register additional Dataframes or unregister existing Dataframes with:</p> <ul> <li><code>register</code></li> <li><code>register_globals</code></li> <li><code>register_many</code></li> <li><code>unregister</code></li> </ul>"},{"location":"user-guide/sql/intro/#execute-queries-and-collect-results","title":"Execute queries and collect results","text":"<p>SQL queries are always executed in lazy mode to benefit from lazy optimizations, so we have 2 options to collect the result:</p> <ul> <li>Set the parameter <code>eager_execution</code> to True in <code>SQLContext</code>. With this parameter, Polars will automatically collect SQL results</li> <li>Set the parameter <code>eager</code> to True when executing a query with <code>execute</code>, or collect the result with <code>collect</code>.</li> </ul> <p>We execute SQL queries by calling <code>execute</code> on a <code>SQLContext</code>.</p>  Python <p> <code>register</code> \u00b7 <code>execute</code> <pre><code># For local files use scan_csv instead\npokemon = pl.read_csv(\n    \"https://gist.githubusercontent.com/ritchie46/cac6b337ea52281aa23c049250a4ff03/raw/89a957ff3919d90e6ef2d34235e6bf22304f3366/pokemon.csv\"\n)\nctx = pl.SQLContext(register_globals=True, eager_execution=True)\ndf_small = ctx.execute(\"SELECT * from pokemon LIMIT 5\")\nprint(df_small)\n</code></pre></p> <pre><code>shape: (5, 13)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 #   \u2506 Name                  \u2506 Type 1 \u2506 Type 2 \u2506 \u2026 \u2506 Sp. Def \u2506 Speed \u2506 Generation \u2506 Legendary \u2502\n\u2502 --- \u2506 ---                   \u2506 ---    \u2506 ---    \u2506   \u2506 ---     \u2506 ---   \u2506 ---        \u2506 ---       \u2502\n\u2502 i64 \u2506 str                   \u2506 str    \u2506 str    \u2506   \u2506 i64     \u2506 i64   \u2506 i64        \u2506 bool      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 Bulbasaur             \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 65      \u2506 45    \u2506 1          \u2506 false     \u2502\n\u2502 2   \u2506 Ivysaur               \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 80      \u2506 60    \u2506 1          \u2506 false     \u2502\n\u2502 3   \u2506 Venusaur              \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 100     \u2506 80    \u2506 1          \u2506 false     \u2502\n\u2502 3   \u2506 VenusaurMega Venusaur \u2506 Grass  \u2506 Poison \u2506 \u2026 \u2506 120     \u2506 80    \u2506 1          \u2506 false     \u2502\n\u2502 4   \u2506 Charmander            \u2506 Fire   \u2506 null   \u2506 \u2026 \u2506 50      \u2506 65    \u2506 1          \u2506 false     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/intro/#compatibility","title":"Compatibility","text":"<p>Polars does not support the full SQL language, in Polars you are allowed to:</p> <ul> <li>Write a <code>CREATE</code> statements <code>CREATE TABLE xxx AS ...</code></li> <li>Write a <code>SELECT</code> statements with all generic elements (<code>GROUP BY</code>, <code>WHERE</code>,<code>ORDER</code>,<code>LIMIT</code>,<code>JOIN</code>, ...)</li> <li>Write Common Table Expressions (CTE's) (<code>WITH tablename AS</code>)</li> <li>Show an overview of all tables <code>SHOW TABLES</code></li> </ul> <p>The following is not yet supported:</p> <ul> <li><code>INSERT</code>, <code>UPDATE</code> or <code>DELETE</code> statements</li> <li>Table aliasing (e.g. <code>SELECT p.Name from pokemon AS p</code>)</li> <li>Meta queries such as <code>ANALYZE</code>, <code>EXPLAIN</code></li> </ul> <p>In the upcoming sections we will cover each of the statements in more details.</p> <ol> <li> <p>Additionally it also tracks the common table expressions as well.\u00a0\u21a9</p> </li> </ol>"},{"location":"user-guide/sql/select/","title":"SELECT","text":"<p>In Polars SQL, the <code>SELECT</code> statement is used to retrieve data from a table into a <code>DataFrame</code>. The basic syntax of a <code>SELECT</code> statement in Polars SQL is as follows:</p> <pre><code>SELECT column1, column2, ...\nFROM table_name;\n</code></pre> <p>Here, <code>column1</code>, <code>column2</code>, etc. are the columns that you want to select from the table. You can also use the wildcard <code>*</code> to select all columns. <code>table_name</code> is the name of the table or that you want to retrieve data from. In the sections below we will cover some of the more common SELECT variants</p>  Python <p> <code>register</code> \u00b7 <code>execute</code> <pre><code>df = pl.DataFrame(\n    {\n        \"city\": [\n            \"New York\",\n            \"Los Angeles\",\n            \"Chicago\",\n            \"Houston\",\n            \"Phoenix\",\n            \"Amsterdam\",\n        ],\n        \"country\": [\"USA\", \"USA\", \"USA\", \"USA\", \"USA\", \"Netherlands\"],\n        \"population\": [8399000, 3997000, 2705000, 2320000, 1680000, 900000],\n    }\n)\n\nctx = pl.SQLContext(population=df, eager_execution=True)\n\nprint(ctx.execute(\"SELECT * FROM population\"))\n</code></pre></p> <pre><code>shape: (6, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 city        \u2506 country     \u2506 population \u2502\n\u2502 ---         \u2506 ---         \u2506 ---        \u2502\n\u2502 str         \u2506 str         \u2506 i64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 New York    \u2506 USA         \u2506 8399000    \u2502\n\u2502 Los Angeles \u2506 USA         \u2506 3997000    \u2502\n\u2502 Chicago     \u2506 USA         \u2506 2705000    \u2502\n\u2502 Houston     \u2506 USA         \u2506 2320000    \u2502\n\u2502 Phoenix     \u2506 USA         \u2506 1680000    \u2502\n\u2502 Amsterdam   \u2506 Netherlands \u2506 900000     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/select/#group-by","title":"GROUP BY","text":"<p>The <code>GROUP BY</code> statement is used to group rows in a table by one or more columns and compute aggregate functions on each group.</p>  Python <p> <code>execute</code> <pre><code>result = ctx.execute(\n\"\"\"\n        SELECT country, AVG(population) as avg_population\n        FROM population\n        GROUP BY country \n    \"\"\"\n)\nprint(result)\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 country     \u2506 avg_population \u2502\n\u2502 ---         \u2506 ---            \u2502\n\u2502 str         \u2506 f64            \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Netherlands \u2506 900000.0       \u2502\n\u2502 USA         \u2506 3.8202e6       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/select/#order-by","title":"ORDER BY","text":"<p>The <code>ORDER BY</code> statement is used to sort the result set of a query by one or more columns in ascending or descending order. </p>  Python <p> <code>execute</code> <pre><code>result = ctx.execute(\n\"\"\"\n        SELECT city, population\n        FROM population\n        ORDER BY population \n    \"\"\"\n)\nprint(result)\n</code></pre></p> <pre><code>shape: (6, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 city        \u2506 population \u2502\n\u2502 ---         \u2506 ---        \u2502\n\u2502 str         \u2506 i64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Amsterdam   \u2506 900000     \u2502\n\u2502 Phoenix     \u2506 1680000    \u2502\n\u2502 Houston     \u2506 2320000    \u2502\n\u2502 Chicago     \u2506 2705000    \u2502\n\u2502 Los Angeles \u2506 3997000    \u2502\n\u2502 New York    \u2506 8399000    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/select/#join","title":"JOIN","text":"Python <p> <code>register_many</code> \u00b7 <code>execute</code> <pre><code>income = pl.DataFrame(\n    {\n        \"city\": [\n            \"New York\",\n            \"Los Angeles\",\n            \"Chicago\",\n            \"Houston\",\n            \"Amsterdam\",\n            \"Rotterdam\",\n            \"Utrecht\",\n        ],\n        \"country\": [\n            \"USA\",\n            \"USA\",\n            \"USA\",\n            \"USA\",\n            \"Netherlands\",\n            \"Netherlands\",\n            \"Netherlands\",\n        ],\n        \"income\": [55000, 62000, 48000, 52000, 42000, 38000, 41000],\n    }\n)\nctx.register_many(income=income)\nresult = ctx.execute(\n\"\"\"\n        SELECT country, city, income, population\n        FROM population\n        LEFT JOIN income on population.city = income.city\n    \"\"\"\n)\nprint(result)\n</code></pre></p> <pre><code>shape: (6, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 country     \u2506 city        \u2506 income \u2506 population \u2502\n\u2502 ---         \u2506 ---         \u2506 ---    \u2506 ---        \u2502\n\u2502 str         \u2506 str         \u2506 i64    \u2506 i64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 USA         \u2506 New York    \u2506 55000  \u2506 8399000    \u2502\n\u2502 USA         \u2506 Los Angeles \u2506 62000  \u2506 3997000    \u2502\n\u2502 USA         \u2506 Chicago     \u2506 48000  \u2506 2705000    \u2502\n\u2502 USA         \u2506 Houston     \u2506 52000  \u2506 2320000    \u2502\n\u2502 USA         \u2506 Phoenix     \u2506 null   \u2506 1680000    \u2502\n\u2502 Netherlands \u2506 Amsterdam   \u2506 42000  \u2506 900000     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/select/#functions","title":"Functions","text":"<p>Polars provides a wide range of SQL functions, including:</p> <ul> <li>Mathematical functions: <code>ABS</code>, <code>EXP</code>, <code>LOG</code>, <code>ASIN</code>, <code>ACOS</code>, <code>ATAN</code>, etc.</li> <li>String functions: <code>LOWER</code>, <code>UPPER</code>, <code>LTRIM</code>, <code>RTRIM</code>, <code>STARTS_WITH</code>,<code>ENDS_WITH</code>.</li> <li>Aggregation functions: <code>SUM</code>, <code>AVG</code>, <code>MIN</code>, <code>MAX</code>, <code>COUNT</code>, <code>STDDEV</code>, <code>FIRST</code> etc.</li> <li>Array functions: <code>EXPLODE</code>, <code>UNNEST</code>,<code>ARRAY_SUM</code>,<code>ARRAY_REVERSE</code>, etc.</li> </ul> <p>For a full list of supported functions go the API documentation. The example below demonstrates how to use a function in a query</p>  Python <p> <code>query</code> <pre><code>result = ctx.execute(\n\"\"\"\n        SELECT city, population\n        FROM population\n        WHERE STARTS_WITH(country,'U')\n    \"\"\"\n)\nprint(result)\n</code></pre></p> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 city        \u2506 population \u2502\n\u2502 ---         \u2506 ---        \u2502\n\u2502 str         \u2506 i64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 New York    \u2506 8399000    \u2502\n\u2502 Los Angeles \u2506 3997000    \u2502\n\u2502 Chicago     \u2506 2705000    \u2502\n\u2502 Houston     \u2506 2320000    \u2502\n\u2502 Phoenix     \u2506 1680000    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/select/#table-functions","title":"Table Functions","text":"<p>In the examples earlier we first generated a DataFrame which we registered in the <code>SQLContext</code>. Polars also support directly reading from CSV, Parquet, JSON and IPC in your SQL query using table functions <code>read_xxx</code>.</p>  Python <p> <code>execute</code> <pre><code>result = ctx.execute(\n\"\"\"\n        SELECT *\n        FROM read_csv('docs/src/data/iris.csv')\n    \"\"\"\n)\nprint(result)\n</code></pre></p> <pre><code>shape: (150, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 sepal_length \u2506 sepal_width \u2506 petal_length \u2506 petal_width \u2506 species   \u2502\n\u2502 ---          \u2506 ---         \u2506 ---          \u2506 ---         \u2506 ---       \u2502\n\u2502 f64          \u2506 f64         \u2506 f64          \u2506 f64         \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 5.1          \u2506 3.5         \u2506 1.4          \u2506 0.2         \u2506 Setosa    \u2502\n\u2502 4.9          \u2506 3.0         \u2506 1.4          \u2506 0.2         \u2506 Setosa    \u2502\n\u2502 4.7          \u2506 3.2         \u2506 1.3          \u2506 0.2         \u2506 Setosa    \u2502\n\u2502 4.6          \u2506 3.1         \u2506 1.5          \u2506 0.2         \u2506 Setosa    \u2502\n\u2502 \u2026            \u2506 \u2026           \u2506 \u2026            \u2506 \u2026           \u2506 \u2026         \u2502\n\u2502 6.3          \u2506 2.5         \u2506 5.0          \u2506 1.9         \u2506 Virginica \u2502\n\u2502 6.5          \u2506 3.0         \u2506 5.2          \u2506 2.0         \u2506 Virginica \u2502\n\u2502 6.2          \u2506 3.4         \u2506 5.4          \u2506 2.3         \u2506 Virginica \u2502\n\u2502 5.9          \u2506 3.0         \u2506 5.1          \u2506 1.8         \u2506 Virginica \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/sql/show/","title":"SHOW TABLES","text":"<p>In Polars, the <code>SHOW TABLES</code> statement is used to list all the tables that have been registered in the current <code>SQLContext</code>. When you register a DataFrame with the <code>SQLContext</code>, you give it a name that can be used to refer to the DataFrame in subsequent SQL statements. The <code>SHOW TABLES</code> statement allows you to see a list of all the registered tables, along with their names.</p> <p>The syntax for the <code>SHOW TABLES</code> statement in Polars is as follows:</p> <pre><code>SHOW TABLES\n</code></pre> <p>Here's an example of how to use the <code>SHOW TABLES</code> statement in Polars:</p>  Python <p> <code>register</code> \u00b7 <code>execute</code> <pre><code># Create some DataFrames and register them with the SQLContext\ndf1 = pl.LazyFrame(\n    {\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n        \"age\": [25, 30, 35, 40],\n    }\n)\ndf2 = pl.LazyFrame(\n    {\n        \"name\": [\"Ellen\", \"Frank\", \"Gina\", \"Henry\"],\n        \"age\": [45, 50, 55, 60],\n    }\n)\nctx = pl.SQLContext(mytable1=df1, mytable2=df2)\n\ntables = ctx.execute(\"SHOW TABLES\", eager=True)\n\nprint(tables)\n</code></pre></p> <pre><code>shape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name     \u2502\n\u2502 ---      \u2502\n\u2502 str      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 mytable1 \u2502\n\u2502 mytable2 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In this example, we create two DataFrames and register them with the <code>SQLContext</code> using different names. We then execute a <code>SHOW TABLES</code> statement using the <code>execute()</code> method of the <code>SQLContext</code> object, which returns a DataFrame containing a list of all the registered tables and their names. The resulting DataFrame is then printed using the <code>print()</code> function.</p> <p>Note that the <code>SHOW TABLES</code> statement only lists tables that have been registered with the current <code>SQLContext</code>. If you register a DataFrame with a different <code>SQLContext</code> or in a different Python session, it will not appear in the list of tables returned by <code>SHOW TABLES</code>.</p>"},{"location":"user-guide/transformations/concatenation/","title":"Concatenation","text":"<p>There are a number of ways to concatenate data from separate DataFrames:</p> <ul> <li>two dataframes with the same columns can be vertically concatenated to make a longer dataframe</li> <li>two dataframes with the same number of rows and non-overlapping columns can be horizontally concatenated to make a wider dataframe</li> <li>two dataframes with different numbers of rows and columns can be diagonally concatenated to make a dataframe which might be longer and/ or wider. Where column names overlap values will be vertically concatenated. Where column names do not overlap new rows and columns will be added. Missing values will be set as <code>null</code></li> </ul>"},{"location":"user-guide/transformations/concatenation/#vertical-concatenation-getting-longer","title":"Vertical concatenation - getting longer","text":"<p>In a vertical concatenation you combine all of the rows from a list of <code>DataFrames</code> into a single longer <code>DataFrame</code>.</p>  Python <p> <code>concat</code> <pre><code>df_v1 = pl.DataFrame(\n    {\n        \"a\": [1],\n        \"b\": [3],\n    }\n)\ndf_v2 = pl.DataFrame(\n    {\n        \"a\": [2],\n        \"b\": [4],\n    }\n)\ndf_vertical_concat = pl.concat(\n    [\n        df_v1,\n        df_v2,\n    ],\n    how=\"vertical\",\n)\nprint(df_vertical_concat)\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b   \u2502\n\u2502 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 3   \u2502\n\u2502 2   \u2506 4   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Vertical concatenation fails when the dataframes do not have the same column names.</p>"},{"location":"user-guide/transformations/concatenation/#horizontal-concatenation-getting-wider","title":"Horizontal concatenation - getting wider","text":"<p>In a horizontal concatenation you combine all of the columns from a list of <code>DataFrames</code> into a single wider <code>DataFrame</code>.</p>  Python <p> <code>concat</code> <pre><code>df_h1 = pl.DataFrame(\n    {\n        \"l1\": [1, 2],\n        \"l2\": [3, 4],\n    }\n)\ndf_h2 = pl.DataFrame(\n    {\n        \"r1\": [5, 6],\n        \"r2\": [7, 8],\n        \"r3\": [9, 10],\n    }\n)\ndf_horizontal_concat = pl.concat(\n    [\n        df_h1,\n        df_h2,\n    ],\n    how=\"horizontal\",\n)\nprint(df_horizontal_concat)\n</code></pre></p> <pre><code>shape: (2, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 l1  \u2506 l2  \u2506 r1  \u2506 r2  \u2506 r3  \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2506 --- \u2506 --- \u2502\n\u2502 i64 \u2506 i64 \u2506 i64 \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 3   \u2506 5   \u2506 7   \u2506 9   \u2502\n\u2502 2   \u2506 4   \u2506 6   \u2506 8   \u2506 10  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Horizontal concatenation fails when dataframes have overlapping columns or a different number of rows.</p>"},{"location":"user-guide/transformations/concatenation/#diagonal-concatenation-getting-longer-wider-and-nullier","title":"Diagonal concatenation - getting longer, wider and <code>null</code>ier","text":"<p>In a diagonal concatenation you combine all of the row and columns from a list of <code>DataFrames</code> into a single longer and/or wider <code>DataFrame</code>.</p>  Python <p> <code>concat</code> <pre><code>df_d1 = pl.DataFrame(\n    {\n        \"a\": [1],\n        \"b\": [3],\n    }\n)\ndf_d2 = pl.DataFrame(\n    {\n        \"a\": [2],\n        \"d\": [4],\n    }\n)\n\ndf_diagonal_concat = pl.concat(\n    [\n        df_d1,\n        df_d2,\n    ],\n    how=\"diagonal\",\n)\nprint(df_diagonal_concat)\n</code></pre></p> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a   \u2506 b    \u2506 d    \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2502\n\u2502 i64 \u2506 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 3    \u2506 null \u2502\n\u2502 2   \u2506 null \u2506 4    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Diagonal concatenation generates nulls when the column names do not overlap.</p> <p>When the dataframe shapes do not match and we have an overlapping semantic key then we can join the dataframes instead of concatenating them.</p>"},{"location":"user-guide/transformations/concatenation/#rechunking","title":"Rechunking","text":"<p>Before a concatenation we have two dataframes <code>df1</code> and <code>df2</code>. Each column in <code>df1</code> and <code>df2</code> is in one or more chunks in memory. By default, during concatenation the chunks in each column are copied to a single new chunk - this is known as rechunking. Rechunking is an expensive operation, but is often worth it because future operations will be faster. If you do not want Polars to rechunk the concatenated <code>DataFrame</code> you specify <code>rechunk = False</code> when doing the concatenation.</p>"},{"location":"user-guide/transformations/joins/","title":"Joins","text":""},{"location":"user-guide/transformations/joins/#join-strategies","title":"Join strategies","text":"<p><code>Polars</code> supports the following join strategies by specifying the <code>strategy</code> argument:</p> <ul> <li><code>inner</code></li> <li><code>left</code></li> <li><code>outer</code></li> <li><code>cross</code></li> <li><code>asof</code></li> <li><code>semi</code></li> <li><code>anti</code></li> </ul>"},{"location":"user-guide/transformations/joins/#inner-join","title":"Inner join","text":"<p>An <code>inner</code> join produces a <code>DataFrame</code> that contains only the rows where the join key exists in both <code>DataFrames</code>. Let's take for example the following two <code>DataFrames</code>:</p>  Python <p> <code>DataFrame</code> <pre><code>df_customers = pl.DataFrame(\n    {\n        \"customer_id\": [1, 2, 3],\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    }\n)\nprint(df_customers)\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 customer_id \u2506 name    \u2502\n\u2502 ---         \u2506 ---     \u2502\n\u2502 i64         \u2506 str     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1           \u2506 Alice   \u2502\n\u2502 2           \u2506 Bob     \u2502\n\u2502 3           \u2506 Charlie \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p></p>  Python <p> <code>DataFrame</code> <pre><code>df_orders = pl.DataFrame(\n    {\n        \"order_id\": [\"a\", \"b\", \"c\"],\n        \"customer_id\": [1, 2, 2],\n        \"amount\": [100, 200, 300],\n    }\n)\nprint(df_orders)\n</code></pre></p> <pre><code>shape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 order_id \u2506 customer_id \u2506 amount \u2502\n\u2502 ---      \u2506 ---         \u2506 ---    \u2502\n\u2502 str      \u2506 i64         \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a        \u2506 1           \u2506 100    \u2502\n\u2502 b        \u2506 2           \u2506 200    \u2502\n\u2502 c        \u2506 2           \u2506 300    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To get a <code>DataFrame</code> with the orders and their associated customer we can do an <code>inner</code> join on the <code>customer_id</code> column:</p>  Python <p> <code>join</code> <pre><code>df_inner_customer_join = df_customers.join(df_orders, on=\"customer_id\", how=\"inner\")\nprint(df_inner_customer_join)\n</code></pre></p> <pre><code>shape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 customer_id \u2506 name  \u2506 order_id \u2506 amount \u2502\n\u2502 ---         \u2506 ---   \u2506 ---      \u2506 ---    \u2502\n\u2502 i64         \u2506 str   \u2506 str      \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1           \u2506 Alice \u2506 a        \u2506 100    \u2502\n\u2502 2           \u2506 Bob   \u2506 b        \u2506 200    \u2502\n\u2502 2           \u2506 Bob   \u2506 c        \u2506 300    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/joins/#left-join","title":"Left join","text":"<p>The <code>left</code> join produces a <code>DataFrame</code> that contains all the rows from the left <code>DataFrame</code> and only the rows from the right <code>DataFrame</code> where the join key exists in the left <code>DataFrame</code>. If we now take the example from above and want to have a <code>DataFrame</code> with all the customers and their associated orders (regardless of whether they have placed an order or not) we can do a <code>left</code> join:</p>  Python <p> <code>join</code> <pre><code>df_left_join = df_customers.join(df_orders, on=\"customer_id\", how=\"left\")\nprint(df_left_join)\n</code></pre></p> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 customer_id \u2506 name    \u2506 order_id \u2506 amount \u2502\n\u2502 ---         \u2506 ---     \u2506 ---      \u2506 ---    \u2502\n\u2502 i64         \u2506 str     \u2506 str      \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1           \u2506 Alice   \u2506 a        \u2506 100    \u2502\n\u2502 2           \u2506 Bob     \u2506 b        \u2506 200    \u2502\n\u2502 2           \u2506 Bob     \u2506 c        \u2506 300    \u2502\n\u2502 3           \u2506 Charlie \u2506 null     \u2506 null   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Notice, that the fields for the customer with the <code>customer_id</code> of <code>3</code> are null, as there are no orders for this customer.</p>"},{"location":"user-guide/transformations/joins/#outer-join","title":"Outer join","text":"<p>The <code>outer</code> join produces a <code>DataFrame</code> that contains all the rows from both <code>DataFrames</code>. Columns are null, if the join key does not exist in the source <code>DataFrame</code>. Doing an <code>outer</code> join on the two <code>DataFrames</code> from above produces a similar <code>DataFrame</code> to the <code>left</code> join:</p>  Python <p> <code>join</code> <pre><code>df_outer_join = df_customers.join(df_orders, on=\"customer_id\", how=\"outer\")\nprint(df_outer_join)\n</code></pre></p> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 customer_id \u2506 name    \u2506 order_id \u2506 amount \u2502\n\u2502 ---         \u2506 ---     \u2506 ---      \u2506 ---    \u2502\n\u2502 i64         \u2506 str     \u2506 str      \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1           \u2506 Alice   \u2506 a        \u2506 100    \u2502\n\u2502 2           \u2506 Bob     \u2506 b        \u2506 200    \u2502\n\u2502 2           \u2506 Bob     \u2506 c        \u2506 300    \u2502\n\u2502 3           \u2506 Charlie \u2506 null     \u2506 null   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/joins/#cross-join","title":"Cross join","text":"<p>A <code>cross</code> join is a cartesian product of the two <code>DataFrames</code>. This means that every row in the left <code>DataFrame</code> is joined with every row in the right <code>DataFrame</code>. The <code>cross</code> join is useful for creating a <code>DataFrame</code> with all possible combinations of the columns in two <code>DataFrames</code>. Let's take for example the following two <code>DataFrames</code>.</p>  Python <p> <code>DataFrame</code> <pre><code>df_colors = pl.DataFrame(\n    {\n        \"color\": [\"red\", \"blue\", \"green\"],\n    }\n)\nprint(df_colors)\n</code></pre></p> <pre><code>shape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 color \u2502\n\u2502 ---   \u2502\n\u2502 str   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 red   \u2502\n\u2502 blue  \u2502\n\u2502 green \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p></p>  Python <p> <code>DataFrame</code> <pre><code>df_sizes = pl.DataFrame(\n    {\n        \"size\": [\"S\", \"M\", \"L\"],\n    }\n)\nprint(df_sizes)\n</code></pre></p> <pre><code>shape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 size \u2502\n\u2502 ---  \u2502\n\u2502 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 S    \u2502\n\u2502 M    \u2502\n\u2502 L    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>We can now create a <code>DataFrame</code> containing all possible combinations of the colors and sizes with a <code>cross</code> join:</p>  Python <p> <code>join</code> <pre><code>df_cross_join = df_colors.join(df_sizes, how=\"cross\")\nprint(df_cross_join)\n</code></pre></p> <pre><code>shape: (9, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 color \u2506 size \u2502\n\u2502 ---   \u2506 ---  \u2502\n\u2502 str   \u2506 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 red   \u2506 S    \u2502\n\u2502 red   \u2506 M    \u2502\n\u2502 red   \u2506 L    \u2502\n\u2502 blue  \u2506 S    \u2502\n\u2502 blue  \u2506 M    \u2502\n\u2502 blue  \u2506 L    \u2502\n\u2502 green \u2506 S    \u2502\n\u2502 green \u2506 M    \u2502\n\u2502 green \u2506 L    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p></p> <p>The <code>inner</code>, <code>left</code>, <code>outer</code> and <code>cross</code> join strategies are standard amongst dataframe libraries. We provide more details on the less familiar <code>semi</code>, <code>anti</code> and <code>asof</code> join strategies below.</p>"},{"location":"user-guide/transformations/joins/#semi-join","title":"Semi join","text":"<p>Consider the following scenario: a car rental company has a <code>DataFrame</code> showing the cars that it owns with each car having a unique <code>id</code>.</p>  Python <p> <code>DataFrame</code> <pre><code>df_cars = pl.DataFrame(\n    {\n        \"id\": [\"a\", \"b\", \"c\"],\n        \"make\": [\"ford\", \"toyota\", \"bmw\"],\n    }\n)\nprint(df_cars)\n</code></pre></p> <pre><code>shape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 make   \u2502\n\u2502 --- \u2506 ---    \u2502\n\u2502 str \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 ford   \u2502\n\u2502 b   \u2506 toyota \u2502\n\u2502 c   \u2506 bmw    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The company has another <code>DataFrame</code> showing each repair job carried out on a vehicle.</p>  Python <p> <code>DataFrame</code> <pre><code>df_repairs = pl.DataFrame(\n    {\n        \"id\": [\"c\", \"c\"],\n        \"cost\": [100, 200],\n    }\n)\nprint(df_repairs)\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 cost \u2502\n\u2502 --- \u2506 ---  \u2502\n\u2502 str \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 c   \u2506 100  \u2502\n\u2502 c   \u2506 200  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You want to answer this question: which of the cars have had repairs carried out?</p> <p>An inner join does not answer this question directly as it produces a <code>DataFrame</code> with multiple rows for each car that has had multiple repair jobs:</p>  Python <p> <code>join</code> <pre><code>df_inner_join = df_cars.join(df_repairs, on=\"id\", how=\"inner\")\nprint(df_inner_join)\n</code></pre></p> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 make \u2506 cost \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2502\n\u2502 str \u2506 str  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 c   \u2506 bmw  \u2506 100  \u2502\n\u2502 c   \u2506 bmw  \u2506 200  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>However, a semi join produces a single row for each car that has had a repair job carried out.</p>  Python <p> <code>join</code> <pre><code>df_semi_join = df_cars.join(df_repairs, on=\"id\", how=\"semi\")\nprint(df_semi_join)\n</code></pre></p> <pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 make \u2502\n\u2502 --- \u2506 ---  \u2502\n\u2502 str \u2506 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 c   \u2506 bmw  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/joins/#anti-join","title":"Anti join","text":"<p>Continuing this example, an alternative question might be: which of the cars have not had a repair job carried out? An anti join produces a <code>DataFrame</code> showing all the cars from <code>df_cars</code> where the <code>id</code> is not present in the <code>df_repairs</code> <code>DataFrame</code>.</p>  Python <p> <code>join</code> <pre><code>df_anti_join = df_cars.join(df_repairs, on=\"id\", how=\"anti\")\nprint(df_anti_join)\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 make   \u2502\n\u2502 --- \u2506 ---    \u2502\n\u2502 str \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 ford   \u2502\n\u2502 b   \u2506 toyota \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/joins/#asof-join","title":"Asof join","text":"<p>An <code>asof</code> join is like a left join except that we match on nearest key rather than equal keys. In <code>Polars</code> we can do an asof join with the <code>join</code> method and specifying <code>strategy=\"asof\"</code>. However, for more flexibility we can use the <code>join_asof</code> method.</p> <p>Consider the following scenario: a stock market broker has a <code>DataFrame</code> called <code>df_trades</code> showing transactions it has made for different stocks.</p>  Python <p> <code>DataFrame</code> <pre><code>df_trades = pl.DataFrame(\n    {\n        \"time\": [\n            datetime(2020, 1, 1, 9, 1, 0),\n            datetime(2020, 1, 1, 9, 1, 0),\n            datetime(2020, 1, 1, 9, 3, 0),\n            datetime(2020, 1, 1, 9, 6, 0),\n        ],\n        \"stock\": [\"A\", \"B\", \"B\", \"C\"],\n        \"trade\": [101, 299, 301, 500],\n    }\n)\nprint(df_trades)\n</code></pre></p> <pre><code>shape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 stock \u2506 trade \u2502\n\u2502 ---                 \u2506 ---   \u2506 ---   \u2502\n\u2502 datetime[\u03bcs]        \u2506 str   \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2020-01-01 09:01:00 \u2506 A     \u2506 101   \u2502\n\u2502 2020-01-01 09:01:00 \u2506 B     \u2506 299   \u2502\n\u2502 2020-01-01 09:03:00 \u2506 B     \u2506 301   \u2502\n\u2502 2020-01-01 09:06:00 \u2506 C     \u2506 500   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The broker has another <code>DataFrame</code> called <code>df_quotes</code> showing prices it has quoted for these stocks.</p>  Python <p> <code>DataFrame</code> <pre><code>df_quotes = pl.DataFrame(\n    {\n        \"time\": [\n            datetime(2020, 1, 1, 9, 0, 0),\n            datetime(2020, 1, 1, 9, 2, 0),\n            datetime(2020, 1, 1, 9, 4, 0),\n            datetime(2020, 1, 1, 9, 6, 0),\n        ],\n        \"stock\": [\"A\", \"B\", \"C\", \"A\"],\n        \"quote\": [100, 300, 501, 102],\n    }\n)\n\nprint(df_quotes)\n</code></pre></p> <pre><code>shape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 stock \u2506 quote \u2502\n\u2502 ---                 \u2506 ---   \u2506 ---   \u2502\n\u2502 datetime[\u03bcs]        \u2506 str   \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2020-01-01 09:00:00 \u2506 A     \u2506 100   \u2502\n\u2502 2020-01-01 09:02:00 \u2506 B     \u2506 300   \u2502\n\u2502 2020-01-01 09:04:00 \u2506 C     \u2506 501   \u2502\n\u2502 2020-01-01 09:06:00 \u2506 A     \u2506 102   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You want to produce a <code>DataFrame</code> showing for each trade the most recent quote provided before the trade. You do this with <code>join_asof</code> (using the default <code>strategy = \"backward\"</code>). To avoid joining between trades on one stock with a quote on another you must specify an exact preliminary join on the stock column with <code>by=\"stock\"</code>.</p>  Python <p> <code>join_asof</code> <pre><code>df_asof_join = df_trades.join_asof(df_quotes, on=\"time\", by=\"stock\")\nprint(df_asof_join)\n</code></pre></p> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 stock \u2506 trade \u2506 quote \u2502\n\u2502 ---                 \u2506 ---   \u2506 ---   \u2506 ---   \u2502\n\u2502 datetime[\u03bcs]        \u2506 str   \u2506 i64   \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2020-01-01 09:01:00 \u2506 A     \u2506 101   \u2506 100   \u2502\n\u2502 2020-01-01 09:01:00 \u2506 B     \u2506 299   \u2506 null  \u2502\n\u2502 2020-01-01 09:03:00 \u2506 B     \u2506 301   \u2506 300   \u2502\n\u2502 2020-01-01 09:06:00 \u2506 C     \u2506 500   \u2506 501   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>If you want to make sure that only quotes within a certain time range are joined to the trades you can specify the <code>tolerance</code> argument. In this case we want to make sure that the last preceding quote is within 1 minute of the trade so we set <code>tolerance = \"1m\"</code>.</p>  Python <pre><code>df_asof_tolerance_join = df_trades.join_asof(df_quotes, on=\"time\", by=\"stock\")\nprint(df_asof_tolerance_join)\n</code></pre> <pre><code>shape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 stock \u2506 trade \u2506 quote \u2502\n\u2502 ---                 \u2506 ---   \u2506 ---   \u2506 ---   \u2502\n\u2502 datetime[\u03bcs]        \u2506 str   \u2506 i64   \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2020-01-01 09:01:00 \u2506 A     \u2506 101   \u2506 100   \u2502\n\u2502 2020-01-01 09:01:00 \u2506 B     \u2506 299   \u2506 null  \u2502\n\u2502 2020-01-01 09:03:00 \u2506 B     \u2506 301   \u2506 300   \u2502\n\u2502 2020-01-01 09:06:00 \u2506 C     \u2506 500   \u2506 501   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/melt/","title":"Melts","text":"<p>Melt operations unpivot a DataFrame from wide format to long format</p>"},{"location":"user-guide/transformations/melt/#dataset","title":"Dataset","text":"Python <p> <code>DataFrame</code> <pre><code>import polars as pl\n\ndf = pl.DataFrame(\n    {\n        \"A\": [\"a\", \"b\", \"a\"],\n        \"B\": [1, 3, 5],\n        \"C\": [10, 11, 12],\n        \"D\": [2, 4, 6],\n    }\n)\nprint(df)\n</code></pre></p> <pre><code>shape: (3, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 A   \u2506 B   \u2506 C   \u2506 D   \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2506 --- \u2502\n\u2502 str \u2506 i64 \u2506 i64 \u2506 i64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 1   \u2506 10  \u2506 2   \u2502\n\u2502 b   \u2506 3   \u2506 11  \u2506 4   \u2502\n\u2502 a   \u2506 5   \u2506 12  \u2506 6   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/melt/#eager-lazy","title":"Eager + Lazy","text":"<p><code>Eager</code> and <code>lazy</code> have the same API.</p>  Python <p> <code>melt</code> <pre><code>out = df.melt(id_vars=[\"A\", \"B\"], value_vars=[\"C\", \"D\"])\nprint(out)\n</code></pre></p> <pre><code>shape: (6, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 A   \u2506 B   \u2506 variable \u2506 value \u2502\n\u2502 --- \u2506 --- \u2506 ---      \u2506 ---   \u2502\n\u2502 str \u2506 i64 \u2506 str      \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a   \u2506 1   \u2506 C        \u2506 10    \u2502\n\u2502 b   \u2506 3   \u2506 C        \u2506 11    \u2502\n\u2502 a   \u2506 5   \u2506 C        \u2506 12    \u2502\n\u2502 a   \u2506 1   \u2506 D        \u2506 2     \u2502\n\u2502 b   \u2506 3   \u2506 D        \u2506 4     \u2502\n\u2502 a   \u2506 5   \u2506 D        \u2506 6     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/pivot/","title":"Pivots","text":"<p>Pivot a column in a <code>DataFrame</code> and perform one of the following aggregations:</p> <ul> <li>first</li> <li>sum</li> <li>min</li> <li>max</li> <li>mean</li> <li>median</li> </ul> <p>The pivot operation consists of a group by one, or multiple columns (these will be the new y-axis), column that will be pivoted (this will be the new x-axis) and an aggregation.</p>"},{"location":"user-guide/transformations/pivot/#dataset","title":"Dataset","text":"Python <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"foo\": [\"A\", \"A\", \"B\", \"B\", \"C\"],\n        \"N\": [1, 2, 2, 4, 2],\n        \"bar\": [\"k\", \"l\", \"m\", \"n\", \"o\"],\n    }\n)\nprint(df)\n</code></pre></p> <pre><code>shape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 N   \u2506 bar \u2502\n\u2502 --- \u2506 --- \u2506 --- \u2502\n\u2502 str \u2506 i64 \u2506 str \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 A   \u2506 1   \u2506 k   \u2502\n\u2502 A   \u2506 2   \u2506 l   \u2502\n\u2502 B   \u2506 2   \u2506 m   \u2502\n\u2502 B   \u2506 4   \u2506 n   \u2502\n\u2502 C   \u2506 2   \u2506 o   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/pivot/#eager","title":"Eager","text":"Python <p> <code>pivot</code> <pre><code>out = df.pivot(index=\"foo\", columns=\"bar\", values=\"N\", aggregate_function=\"first\")\nprint(out)\n</code></pre></p> <pre><code>shape: (3, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 k    \u2506 l    \u2506 m    \u2506 n    \u2506 o    \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2506 ---  \u2506 ---  \u2506 ---  \u2502\n\u2502 str \u2506 i64  \u2506 i64  \u2506 i64  \u2506 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 A   \u2506 1    \u2506 2    \u2506 null \u2506 null \u2506 null \u2502\n\u2502 B   \u2506 null \u2506 null \u2506 2    \u2506 4    \u2506 null \u2502\n\u2502 C   \u2506 null \u2506 null \u2506 null \u2506 null \u2506 2    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/pivot/#lazy","title":"Lazy","text":"<p>A polars <code>LazyFrame</code> always need to no the schema of a computation statically (before collecting the query). As a pivot's output schema depends on the data, and it is therefore impossible to determine the schema without running the query.</p> <p>Polars could have abstracted this fact for you just like Spark does, but we don't want you to shoot yourself in the foot with a shotgun. The cost should be clear up front.</p>  Python <p> <code>pivot</code> <pre><code>q = (\n    df.lazy()\n    .collect()\n    .pivot(index=\"foo\", columns=\"bar\", values=\"N\", aggregate_function=\"first\")\n    .lazy()\n)\nout = q.collect()\nprint(out)\n</code></pre></p> <pre><code>shape: (3, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 foo \u2506 k    \u2506 l    \u2506 m    \u2506 n    \u2506 o    \u2502\n\u2502 --- \u2506 ---  \u2506 ---  \u2506 ---  \u2506 ---  \u2506 ---  \u2502\n\u2502 str \u2506 i64  \u2506 i64  \u2506 i64  \u2506 i64  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 A   \u2506 1    \u2506 2    \u2506 null \u2506 null \u2506 null \u2502\n\u2502 B   \u2506 null \u2506 null \u2506 2    \u2506 4    \u2506 null \u2502\n\u2502 C   \u2506 null \u2506 null \u2506 null \u2506 null \u2506 2    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/filter/","title":"Filtering","text":"<p>Filtering date columns works in the same way as with other types of columns using the <code>.filter</code> method.</p> <p>Polars uses Python's native <code>datetime</code>, <code>date</code> and <code>timedelta</code> for equality comparisons between the datatypes <code>pl.Datetime</code>, <code>pl.Date</code> and <code>pl.Duration</code>.</p> <p>In the following example we use a time series of Apple stock prices.</p>  Python <p> <code>read_csv</code> <pre><code>import polars as pl\nfrom datetime import datetime\n\ndf = pl.read_csv(\"docs/src/data/appleStock.csv\", try_parse_dates=True)\nprint(df)\n</code></pre></p> <pre><code>shape: (100, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close  \u2502\n\u2502 ---        \u2506 ---    \u2502\n\u2502 date       \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1981-02-23 \u2506 24.62  \u2502\n\u2502 1981-05-06 \u2506 27.38  \u2502\n\u2502 1981-05-18 \u2506 28.0   \u2502\n\u2502 1981-09-25 \u2506 14.25  \u2502\n\u2502 \u2026          \u2506 \u2026      \u2502\n\u2502 2012-12-04 \u2506 575.85 \u2502\n\u2502 2013-07-05 \u2506 417.42 \u2502\n\u2502 2013-11-07 \u2506 512.49 \u2502\n\u2502 2014-02-25 \u2506 522.06 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/filter/#filtering-by-single-dates","title":"Filtering by single dates","text":"<p>We can filter by a single date by casting the desired date string to a <code>Date</code> object in a filter expression:</p>  Python <p> <code>filter</code> <pre><code>filtered_df = df.filter(\n    pl.col(\"Date\") == datetime(1995, 10, 16),\n)\nprint(filtered_df)\n</code></pre></p> <pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close \u2502\n\u2502 ---        \u2506 ---   \u2502\n\u2502 date       \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1995-10-16 \u2506 36.13 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note we are using the lowercase <code>datetime</code> method rather than the uppercase <code>Datetime</code> data type.</p>"},{"location":"user-guide/transformations/time-series/filter/#filtering-by-a-date-range","title":"Filtering by a date range","text":"<p>We can filter by a range of dates using the <code>is_between</code> method in a filter expression with the start and end dates:</p>  Python <p> <code>filter</code> \u00b7 <code>is_between</code> <pre><code>filtered_range_df = df.filter(\n    pl.col(\"Date\").is_between(datetime(1995, 7, 1), datetime(1995, 11, 1)),\n)\nprint(filtered_range_df)\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close \u2502\n\u2502 ---        \u2506 ---   \u2502\n\u2502 date       \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1995-07-06 \u2506 47.0  \u2502\n\u2502 1995-10-16 \u2506 36.13 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/filter/#filtering-with-negative-dates","title":"Filtering with negative dates","text":"<p>Say you are working with an archeologist and are dealing in negative dates. Polars can parse and store them just fine, but the Python <code>datetime</code> library does not. So for filtering, you should use attributes in the <code>.dt</code> namespace:</p>  Python <p> <code>strptime</code> <pre><code>ts = pl.Series([\"-1300-05-23\", \"-1400-03-02\"]).str.strptime(pl.Date)\n\nnegative_dates_df = pl.DataFrame({\"ts\": ts, \"values\": [3, 4]})\n\nnegative_dates_filtered_df = negative_dates_df.filter(pl.col(\"ts\").dt.year() &lt; -1300)\nprint(negative_dates_filtered_df)\n</code></pre></p> <pre><code>shape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ts          \u2506 values \u2502\n\u2502 ---         \u2506 ---    \u2502\n\u2502 date        \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 -1400-03-02 \u2506 4      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/parsing/","title":"Parsing","text":"<p>Polars has native support for parsing time series data and doing more sophisticated operations such as temporal grouping and resampling.</p>"},{"location":"user-guide/transformations/time-series/parsing/#datatypes","title":"Datatypes","text":"<p><code>Polars</code> has the following datetime datatypes:</p> <ul> <li><code>Date</code>: Date representation e.g. 2014-07-08. It is internally represented as days since UNIX epoch encoded by a 32-bit signed integer.</li> <li><code>Datetime</code>: Datetime representation e.g. 2014-07-08 07:00:00. It is internally represented as a 64 bit integer since the Unix epoch and can have different units such as ns, us, ms.</li> <li><code>Duration</code>: A time delta type that is created when subtracting <code>Date/Datetime</code>. Similar to <code>timedelta</code> in python.</li> <li><code>Time</code>: Time representation, internally represented as nanoseconds since midnight.</li> </ul>"},{"location":"user-guide/transformations/time-series/parsing/#parsing-dates-from-a-file","title":"Parsing dates from a file","text":"<p>When loading from a CSV file <code>Polars</code> attempts to parse dates and times if the <code>try_parse_dates</code> flag is set to <code>True</code>:</p>  Python <p> <code>read_csv</code> <pre><code>df = pl.read_csv(\"docs/src/data/appleStock.csv\", try_parse_dates=True)\nprint(df)\n</code></pre></p> <pre><code>shape: (100, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close  \u2502\n\u2502 ---        \u2506 ---    \u2502\n\u2502 date       \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1981-02-23 \u2506 24.62  \u2502\n\u2502 1981-05-06 \u2506 27.38  \u2502\n\u2502 1981-05-18 \u2506 28.0   \u2502\n\u2502 1981-09-25 \u2506 14.25  \u2502\n\u2502 \u2026          \u2506 \u2026      \u2502\n\u2502 2012-12-04 \u2506 575.85 \u2502\n\u2502 2013-07-05 \u2506 417.42 \u2502\n\u2502 2013-11-07 \u2506 512.49 \u2502\n\u2502 2014-02-25 \u2506 522.06 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>On the other hand binary formats such as parquet have a schema that is respected by <code>Polars</code>.</p>"},{"location":"user-guide/transformations/time-series/parsing/#casting-strings-to-dates","title":"Casting strings to dates","text":"<p>You can also cast a column of datetimes encoded as strings to a datetime type. You do this by calling the string <code>str.strptime</code> method and passing the format of the date string:</p>  Python <p> <code>read_csv</code> \u00b7 <code>strptime</code> <pre><code>df = pl.read_csv(\"docs/src/data/appleStock.csv\", try_parse_dates=False)\n\ndf = df.with_columns(pl.col(\"Date\").str.strptime(pl.Date, format=\"%Y-%m-%d\"))\nprint(df)\n</code></pre></p> <pre><code>shape: (100, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close  \u2502\n\u2502 ---        \u2506 ---    \u2502\n\u2502 date       \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1981-02-23 \u2506 24.62  \u2502\n\u2502 1981-05-06 \u2506 27.38  \u2502\n\u2502 1981-05-18 \u2506 28.0   \u2502\n\u2502 1981-09-25 \u2506 14.25  \u2502\n\u2502 \u2026          \u2506 \u2026      \u2502\n\u2502 2012-12-04 \u2506 575.85 \u2502\n\u2502 2013-07-05 \u2506 417.42 \u2502\n\u2502 2013-11-07 \u2506 512.49 \u2502\n\u2502 2014-02-25 \u2506 522.06 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The strptime date formats can be found here..</p>"},{"location":"user-guide/transformations/time-series/parsing/#extracting-date-features-from-a-date-column","title":"Extracting date features from a date column","text":"<p>You can extract data features such as the year or day from a date column using the <code>.dt</code> namespace on a date column:</p>  Python <p> <code>year</code> <pre><code>df_with_year = df.with_columns(pl.col(\"Date\").dt.year().alias(\"year\"))\nprint(df_with_year)\n</code></pre></p> <pre><code>shape: (100, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close  \u2506 year \u2502\n\u2502 ---        \u2506 ---    \u2506 ---  \u2502\n\u2502 date       \u2506 f64    \u2506 i32  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1981-02-23 \u2506 24.62  \u2506 1981 \u2502\n\u2502 1981-05-06 \u2506 27.38  \u2506 1981 \u2502\n\u2502 1981-05-18 \u2506 28.0   \u2506 1981 \u2502\n\u2502 1981-09-25 \u2506 14.25  \u2506 1981 \u2502\n\u2502 \u2026          \u2506 \u2026      \u2506 \u2026    \u2502\n\u2502 2012-12-04 \u2506 575.85 \u2506 2012 \u2502\n\u2502 2013-07-05 \u2506 417.42 \u2506 2013 \u2502\n\u2502 2013-11-07 \u2506 512.49 \u2506 2013 \u2502\n\u2502 2014-02-25 \u2506 522.06 \u2506 2014 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/parsing/#mixed-offsets","title":"Mixed offsets","text":"<p>If you have mixed offsets (say, due to crossing daylight saving time), then you can use <code>utc=True</code> and then convert to your time zone:</p>  Python <p> <code>strptime</code> \u00b7 <code>convert_time_zone</code> \u00b7  Available on feature timezone <pre><code>data = [\n    \"2021-03-27T00:00:00+0100\",\n    \"2021-03-28T00:00:00+0100\",\n    \"2021-03-29T00:00:00+0200\",\n    \"2021-03-30T00:00:00+0200\",\n]\nmixed_parsed = (\n    pl.Series(data)\n    .str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S%z\", utc=True)\n    .dt.convert_time_zone(\"Europe/Brussels\")\n)\nprint(mixed_parsed)\n</code></pre></p> <pre><code>shape: (4,)\nSeries: '' [datetime[\u03bcs, Europe/Brussels]]\n[\n    2021-03-27 00:00:00 CET\n    2021-03-28 00:00:00 CET\n    2021-03-29 00:00:00 CEST\n    2021-03-30 00:00:00 CEST\n]\n</code></pre>"},{"location":"user-guide/transformations/time-series/resampling/","title":"Resampling","text":"<p>We can resample by either:</p> <ul> <li>upsampling (moving data to a higher frequency)</li> <li>downsampling (moving data to a lower frequency)</li> <li>combinations of these e.g. first upsample and then downsample</li> </ul>"},{"location":"user-guide/transformations/time-series/resampling/#downsampling-to-a-lower-frequency","title":"Downsampling to a lower frequency","text":"<p><code>Polars</code> views downsampling as a special case of the groupby operation and you can do this with <code>groupby_dynamic</code> and <code>groupby_rolling</code> - see the temporal groupby page for examples.</p>"},{"location":"user-guide/transformations/time-series/resampling/#upsampling-to-a-higher-frequency","title":"Upsampling to a higher frequency","text":"<p>Let's go through an example where we generate data at 30 minute intervals:</p>  Python <p> <code>DataFrame</code> \u00b7 <code>date_range</code> <pre><code>df = pl.DataFrame(\n    {\n        \"time\": pl.date_range(\n            start=datetime(2021, 12, 16),\n            end=datetime(2021, 12, 16, 3),\n            interval=\"30m\",\n            eager=True,\n        ),\n        \"groups\": [\"a\", \"a\", \"a\", \"b\", \"b\", \"a\", \"a\"],\n        \"values\": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0],\n    }\n)\nprint(df)\n</code></pre></p> <pre><code>shape: (7, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 groups \u2506 values \u2502\n\u2502 ---                 \u2506 ---    \u2506 ---    \u2502\n\u2502 datetime[\u03bcs]        \u2506 str    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-12-16 00:00:00 \u2506 a      \u2506 1.0    \u2502\n\u2502 2021-12-16 00:30:00 \u2506 a      \u2506 2.0    \u2502\n\u2502 2021-12-16 01:00:00 \u2506 a      \u2506 3.0    \u2502\n\u2502 2021-12-16 01:30:00 \u2506 b      \u2506 4.0    \u2502\n\u2502 2021-12-16 02:00:00 \u2506 b      \u2506 5.0    \u2502\n\u2502 2021-12-16 02:30:00 \u2506 a      \u2506 6.0    \u2502\n\u2502 2021-12-16 03:00:00 \u2506 a      \u2506 7.0    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Upsampling can be done by defining the new sampling interval. By upsampling we are adding in extra rows where we do not have data. As such upsampling by itself gives a DataFrame with nulls. These nulls can then be filled with a fill strategy or interpolation.</p>"},{"location":"user-guide/transformations/time-series/resampling/#upsampling-strategies","title":"Upsampling strategies","text":"<p>In this example we upsample from the original 30 minutes to 15 minutes and then use a <code>forward</code> strategy to replace the nulls with the previous non-null value:</p>  Python <p> <code>upsample</code> <pre><code>out1 = df.upsample(time_column=\"time\", every=\"15m\").fill_null(strategy=\"forward\")\nprint(out1)\n</code></pre></p> <pre><code>shape: (13, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 groups \u2506 values \u2502\n\u2502 ---                 \u2506 ---    \u2506 ---    \u2502\n\u2502 datetime[\u03bcs]        \u2506 str    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-12-16 00:00:00 \u2506 a      \u2506 1.0    \u2502\n\u2502 2021-12-16 00:15:00 \u2506 a      \u2506 1.0    \u2502\n\u2502 2021-12-16 00:30:00 \u2506 a      \u2506 2.0    \u2502\n\u2502 2021-12-16 00:45:00 \u2506 a      \u2506 2.0    \u2502\n\u2502 \u2026                   \u2506 \u2026      \u2506 \u2026      \u2502\n\u2502 2021-12-16 02:15:00 \u2506 b      \u2506 5.0    \u2502\n\u2502 2021-12-16 02:30:00 \u2506 a      \u2506 6.0    \u2502\n\u2502 2021-12-16 02:45:00 \u2506 a      \u2506 6.0    \u2502\n\u2502 2021-12-16 03:00:00 \u2506 a      \u2506 7.0    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>In this example we instead fill the nulls by linear interpolation:</p>  Python <p> <code>upsample</code> \u00b7 <code>interpolate</code> \u00b7 <code>fill_null</code> <pre><code>out2 = (\n    df.upsample(time_column=\"time\", every=\"15m\")\n    .interpolate()\n    .fill_null(strategy=\"forward\")\n)\nprint(out2)\n</code></pre></p> <pre><code>shape: (13, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 groups \u2506 values \u2502\n\u2502 ---                 \u2506 ---    \u2506 ---    \u2502\n\u2502 datetime[\u03bcs]        \u2506 str    \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-12-16 00:00:00 \u2506 a      \u2506 1.0    \u2502\n\u2502 2021-12-16 00:15:00 \u2506 a      \u2506 1.5    \u2502\n\u2502 2021-12-16 00:30:00 \u2506 a      \u2506 2.0    \u2502\n\u2502 2021-12-16 00:45:00 \u2506 a      \u2506 2.5    \u2502\n\u2502 \u2026                   \u2506 \u2026      \u2506 \u2026      \u2502\n\u2502 2021-12-16 02:15:00 \u2506 b      \u2506 5.5    \u2502\n\u2502 2021-12-16 02:30:00 \u2506 a      \u2506 6.0    \u2502\n\u2502 2021-12-16 02:45:00 \u2506 a      \u2506 6.5    \u2502\n\u2502 2021-12-16 03:00:00 \u2506 a      \u2506 7.0    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/rolling/","title":"Grouping","text":""},{"location":"user-guide/transformations/time-series/rolling/#grouping-by-fixed-windows","title":"Grouping by fixed windows","text":"<p>We can calculate temporal statistics using <code>groupby_dynamic</code> to group rows into days/months/years etc.</p>"},{"location":"user-guide/transformations/time-series/rolling/#annual-average-example","title":"Annual average example","text":"<p>In following simple example we calculate the annual average closing price of Apple stock prices. We first load the data from CSV:</p>  Python <p> <code>upsample</code> <pre><code>df = pl.read_csv(\"docs/src/data/appleStock.csv\", try_parse_dates=True)\nprint(df)\n</code></pre></p> <pre><code>shape: (100, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close  \u2502\n\u2502 ---        \u2506 ---    \u2502\n\u2502 date       \u2506 f64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1981-02-23 \u2506 24.62  \u2502\n\u2502 1981-05-06 \u2506 27.38  \u2502\n\u2502 1981-05-18 \u2506 28.0   \u2502\n\u2502 1981-09-25 \u2506 14.25  \u2502\n\u2502 \u2026          \u2506 \u2026      \u2502\n\u2502 2012-12-04 \u2506 575.85 \u2502\n\u2502 2013-07-05 \u2506 417.42 \u2502\n\u2502 2013-11-07 \u2506 512.49 \u2502\n\u2502 2014-02-25 \u2506 522.06 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Info</p> <p>The dates are sorted in ascending order - if they are not sorted in this way the <code>groupby_dynamic</code> output will not be correct!</p> <p>To get the annual average closing price we tell <code>groupby_dynamic</code> that we want to:</p> <ul> <li>group by the <code>Date</code> column on an annual (<code>1y</code>) basis</li> <li>take the mean values of the <code>Close</code> column for each year:</li> </ul>  Python <p> <code>groupby_dynamic</code> <pre><code>annual_average_df = df.groupby_dynamic(\"Date\", every=\"1y\").agg(pl.col(\"Close\").mean())\n\ndf_with_year = annual_average_df.with_columns(pl.col(\"Date\").dt.year().alias(\"year\"))\nprint(df_with_year)\n</code></pre></p> <p>The annual average closing price is then:</p> <pre><code>shape: (34, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Date       \u2506 Close     \u2506 year \u2502\n\u2502 ---        \u2506 ---       \u2506 ---  \u2502\n\u2502 date       \u2506 f64       \u2506 i32  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1981-01-01 \u2506 23.5625   \u2506 1981 \u2502\n\u2502 1982-01-01 \u2506 11.0      \u2506 1982 \u2502\n\u2502 1983-01-01 \u2506 30.543333 \u2506 1983 \u2502\n\u2502 1984-01-01 \u2506 27.583333 \u2506 1984 \u2502\n\u2502 \u2026          \u2506 \u2026         \u2506 \u2026    \u2502\n\u2502 2011-01-01 \u2506 368.225   \u2506 2011 \u2502\n\u2502 2012-01-01 \u2506 560.965   \u2506 2012 \u2502\n\u2502 2013-01-01 \u2506 464.955   \u2506 2013 \u2502\n\u2502 2014-01-01 \u2506 522.06    \u2506 2014 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/rolling/#parameters-for-groupby_dynamic","title":"Parameters for <code>groupby_dynamic</code>","text":"<p>A dynamic window is defined by a:</p> <ul> <li>every: indicates the interval of the window</li> <li>period: indicates the duration of the window</li> <li>offset: can be used to offset the start of the windows</li> </ul> <p>The value for <code>every</code> sets how often the groups start. The time period values are flexible - for example we could take:</p> <ul> <li>the average over 2 year intervals by replacing <code>1y</code> with <code>2y</code></li> <li>the average over 18 month periods by replacing <code>1y</code> with <code>1y6mo</code></li> </ul> <p>We can also use the <code>period</code> parameter to set how long the time period for each group is. For example, if we set the <code>every</code> parameter to be <code>1y</code> and the <code>period</code> parameter to be <code>2y</code> then we would get groups at one year intervals where each groups spanned two years.</p> <p>If the <code>period</code> parameter is not specified then it is set equal to the <code>every</code> parameter so that if the <code>every</code> parameter is set to be <code>1y</code> then each group spans <code>1y</code> as well.</p> <p>Because every does not have to be equal to period, we can create many groups in a very flexible way. They may overlap or leave boundaries between them.</p> <p>Let's see how the windows for some parameter combinations would look. Let's start out boring. \ud83e\udd71</p> <ul> <li>every: 1 day -&gt; <code>\"1d\"</code></li> <li>period: 1 day -&gt; <code>\"1d\"</code></li> </ul> <pre><code>this creates adjacent windows of the same size\n|--|\n   |--|\n      |--|\n</code></pre> <ul> <li>every: 1 day -&gt; <code>\"1d\"</code></li> <li>period: 2 days -&gt; <code>\"2d\"</code></li> </ul> <pre><code>these windows have an overlap of 1 day\n|----|\n   |----|\n      |----|\n</code></pre> <ul> <li>every: 2 days -&gt; <code>\"2d\"</code></li> <li>period: 1 day -&gt; <code>\"1d\"</code></li> </ul> <pre><code>this would leave gaps between the windows\ndata points that in these gaps will not be a member of any group\n|--|\n       |--|\n              |--|\n</code></pre>"},{"location":"user-guide/transformations/time-series/rolling/#truncate","title":"<code>truncate</code>","text":"<p>The <code>truncate</code> parameter is a Boolean variable that determines what datetime value is associated with each group in the output. In the example above the first data point is on 23rd February 1981. If <code>truncate = True</code> (the default) then the date for the first year in the annual average is 1st January 1981. However, if <code>truncate = False</code> then the date for the first year in the annual average is the date of the first data point on 23rd February 1981. Note that <code>truncate</code> only affects what's shown in the <code>Date</code> column and does not affect the window boundaries.</p>"},{"location":"user-guide/transformations/time-series/rolling/#using-expressions-in-groupby_dynamic","title":"Using expressions in <code>groupby_dynamic</code>","text":"<p>We aren't restricted to using simple aggregations like <code>mean</code> in a groupby operation - we can use the full range of expressions available in Polars.</p> <p>In the snippet below we create a <code>date range</code> with every day (<code>\"1d\"</code>) in 2021 and turn this into a <code>DataFrame</code>.</p> <p>Then in the <code>groupby_dynamic</code> we create dynamic windows that start every month (<code>\"1mo\"</code>) and have a window length of <code>1</code> month. The values that match these dynamic windows are then assigned to that group and can be aggregated with the powerful expression API.</p> <p>Below we show an example where we use groupby_dynamic to compute:</p> <ul> <li>the number of days until the end of the month</li> <li>the number of days in a month</li> </ul>  Python <p> <code>groupby_dynamic</code> \u00b7 <code>explode</code> \u00b7 <code>date_range</code> <pre><code>df = pl.date_range(\n    start=datetime(2021, 1, 1),\n    end=datetime(2021, 12, 31),\n    interval=\"1d\",\n    name=\"time\",\n    eager=True,\n).to_frame()\n\nout = (\n    df.groupby_dynamic(\"time\", every=\"1mo\", period=\"1mo\", closed=\"left\")\n    .agg(\n        [\n            pl.col(\"time\").cumcount().reverse().head(3).alias(\"day/eom\"),\n            ((pl.col(\"time\") - pl.col(\"time\").first()).last().dt.days() + 1).alias(\n                \"days_in_month\"\n            ),\n        ]\n    )\n    .explode(\"day/eom\")\n)\nprint(out)\n</code></pre></p> <pre><code>shape: (36, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 day/eom \u2506 days_in_month \u2502\n\u2502 ---                 \u2506 ---     \u2506 ---           \u2502\n\u2502 datetime[\u03bcs]        \u2506 u32     \u2506 i64           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-01-01 00:00:00 \u2506 30      \u2506 31            \u2502\n\u2502 2021-01-01 00:00:00 \u2506 29      \u2506 31            \u2502\n\u2502 2021-01-01 00:00:00 \u2506 28      \u2506 31            \u2502\n\u2502 2021-02-01 00:00:00 \u2506 27      \u2506 28            \u2502\n\u2502 \u2026                   \u2506 \u2026       \u2506 \u2026             \u2502\n\u2502 2021-11-01 00:00:00 \u2506 27      \u2506 30            \u2502\n\u2502 2021-12-01 00:00:00 \u2506 30      \u2506 31            \u2502\n\u2502 2021-12-01 00:00:00 \u2506 29      \u2506 31            \u2502\n\u2502 2021-12-01 00:00:00 \u2506 28      \u2506 31            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/rolling/#grouping-by-rolling-windows","title":"Grouping by rolling windows","text":"<p>The rolling groupby, <code>groupby_rolling</code>, is another entrance to the <code>groupby</code> context. But different from the <code>groupby_dynamic</code> the windows are not fixed by a parameter <code>every</code> and <code>period</code>. In a rolling groupby the windows are not fixed at all! They are determined by the values in the <code>index_column</code>.</p> <p>So imagine having a time column with the values <code>{2021-01-06, 20210-01-10}</code> and a <code>period=\"5d\"</code> this would create the following windows:</p> <pre><code>2021-01-01   2021-01-06\n    |----------|\n\n       2021-01-05   2021-01-10\n             |----------|\n</code></pre> <p>Because the windows of a rolling groupby are always determined by the values in the <code>DataFrame</code> column, the number of groups is always equal to the original <code>DataFrame</code>.</p>"},{"location":"user-guide/transformations/time-series/rolling/#combining-groupbys","title":"Combining Groupby's","text":"<p>Rolling and dynamic groupby's can be combined with normal groupby operations.</p> <p>Below is an example with a dynamic groupby.</p>  Python <p> <code>DataFrame</code> <pre><code>df = pl.DataFrame(\n    {\n        \"time\": pl.date_range(\n            start=datetime(2021, 12, 16),\n            end=datetime(2021, 12, 16, 3),\n            interval=\"30m\",\n            eager=True,\n        ),\n        \"groups\": [\"a\", \"a\", \"a\", \"b\", \"b\", \"a\", \"a\"],\n    }\n)\nprint(df)\n</code></pre></p> <pre><code>shape: (7, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 time                \u2506 groups \u2502\n\u2502 ---                 \u2506 ---    \u2502\n\u2502 datetime[\u03bcs]        \u2506 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-12-16 00:00:00 \u2506 a      \u2502\n\u2502 2021-12-16 00:30:00 \u2506 a      \u2502\n\u2502 2021-12-16 01:00:00 \u2506 a      \u2502\n\u2502 2021-12-16 01:30:00 \u2506 b      \u2502\n\u2502 2021-12-16 02:00:00 \u2506 b      \u2502\n\u2502 2021-12-16 02:30:00 \u2506 a      \u2502\n\u2502 2021-12-16 03:00:00 \u2506 a      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>  Python <p> <code>groupby_dynamic</code> <pre><code>out = df.groupby_dynamic(\n    \"time\",\n    every=\"1h\",\n    closed=\"both\",\n    by=\"groups\",\n    include_boundaries=True,\n).agg(\n    [\n        pl.count(),\n    ]\n)\nprint(out)\n</code></pre></p> <pre><code>shape: (7, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 groups \u2506 _lower_boundary     \u2506 _upper_boundary     \u2506 time                \u2506 count \u2502\n\u2502 ---    \u2506 ---                 \u2506 ---                 \u2506 ---                 \u2506 ---   \u2502\n\u2502 str    \u2506 datetime[\u03bcs]        \u2506 datetime[\u03bcs]        \u2506 datetime[\u03bcs]        \u2506 u32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a      \u2506 2021-12-15 23:00:00 \u2506 2021-12-16 00:00:00 \u2506 2021-12-15 23:00:00 \u2506 1     \u2502\n\u2502 a      \u2506 2021-12-16 00:00:00 \u2506 2021-12-16 01:00:00 \u2506 2021-12-16 00:00:00 \u2506 3     \u2502\n\u2502 a      \u2506 2021-12-16 01:00:00 \u2506 2021-12-16 02:00:00 \u2506 2021-12-16 01:00:00 \u2506 1     \u2502\n\u2502 a      \u2506 2021-12-16 02:00:00 \u2506 2021-12-16 03:00:00 \u2506 2021-12-16 02:00:00 \u2506 2     \u2502\n\u2502 a      \u2506 2021-12-16 03:00:00 \u2506 2021-12-16 04:00:00 \u2506 2021-12-16 03:00:00 \u2506 1     \u2502\n\u2502 b      \u2506 2021-12-16 01:00:00 \u2506 2021-12-16 02:00:00 \u2506 2021-12-16 01:00:00 \u2506 2     \u2502\n\u2502 b      \u2506 2021-12-16 02:00:00 \u2506 2021-12-16 03:00:00 \u2506 2021-12-16 02:00:00 \u2506 1     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/transformations/time-series/timezones/","title":"Time zones","text":"<p>Tom Scott</p> <p>You really should never, ever deal with time zones if you can help it</p> <p>The <code>Datetime</code> datatype can have a time zone associated with it. Examples of valid time zones are:</p> <ul> <li><code>None</code>: no time zone, also known as \"time zone naive\";</li> <li><code>UTC</code>: Coordinated Universal Time;</li> <li><code>Asia/Kathmandu</code>: time zone in \"area/location\" format.   See the list of tz database time zones   to see what's available;</li> <li><code>+01:00</code>: fixed offsets. May be useful when parsing, but you almost certainly want the \"Area/Location\"   format above instead as it will deal with irregularities such as DST (Daylight Saving Time) for you.</li> </ul> <p>Note that, because a <code>Datetime</code> can only have a single time zone, it is impossible to have a column with multiple time zones. If you are parsing data with multiple offsets, you may want to pass <code>utc=True</code> to convert them all to a common time zone (<code>UTC</code>), see parsing dates and times.</p> <p>The main methods for setting and converting between time zones are:</p> <ul> <li><code>dt.convert_time_zone</code>: convert from one time zone to another;</li> <li><code>dt.replace_time_zone</code>: set/unset/change time zone;</li> </ul> <p>Let's look at some examples of common operations:</p>  Python <p> <code>strptime</code> \u00b7 <code>replace_time_zone</code> \u00b7  Available on feature timezone <pre><code>ts = [\"2021-03-27 03:00\", \"2021-03-28 03:00\"]\ntz_naive = pl.Series(\"tz_naive\", ts).str.strptime(pl.Datetime)\ntz_aware = tz_naive.dt.replace_time_zone(\"UTC\").rename(\"tz_aware\")\ntime_zones_df = pl.DataFrame([tz_naive, tz_aware])\nprint(time_zones_df)\n</code></pre></p> <pre><code>shape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 tz_naive            \u2506 tz_aware                \u2502\n\u2502 ---                 \u2506 ---                     \u2502\n\u2502 datetime[\u03bcs]        \u2506 datetime[\u03bcs, UTC]       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-03-27 03:00:00 \u2506 2021-03-27 03:00:00 UTC \u2502\n\u2502 2021-03-28 03:00:00 \u2506 2021-03-28 03:00:00 UTC \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>  Python <p> <code>convert_time_zone</code> \u00b7 <code>replace_time_zone</code> \u00b7  Available on feature timezone <pre><code>time_zones_operations = time_zones_df.select(\n    [\n        pl.col(\"tz_aware\")\n        .dt.replace_time_zone(\"Europe/Brussels\")\n        .alias(\"replace time zone\"),\n        pl.col(\"tz_aware\")\n        .dt.convert_time_zone(\"Asia/Kathmandu\")\n        .alias(\"convert time zone\"),\n        pl.col(\"tz_aware\").dt.replace_time_zone(None).alias(\"unset time zone\"),\n    ]\n)\nprint(time_zones_operations)\n</code></pre></p> <pre><code>shape: (2, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 replace time zone             \u2506 convert time zone            \u2506 unset time zone     \u2502\n\u2502 ---                           \u2506 ---                          \u2506 ---                 \u2502\n\u2502 datetime[\u03bcs, Europe/Brussels] \u2506 datetime[\u03bcs, Asia/Kathmandu] \u2506 datetime[\u03bcs]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-03-27 03:00:00 CET       \u2506 2021-03-27 08:45:00 +0545    \u2506 2021-03-27 03:00:00 \u2502\n\u2502 2021-03-28 03:00:00 CEST      \u2506 2021-03-28 08:45:00 +0545    \u2506 2021-03-28 03:00:00 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"}]}